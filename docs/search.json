[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "E-mail: jhcho1016@naver.com"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Cho’s FinBlog!",
    "section": "",
    "text": "이 블로그는 빅데이터, AI 등을 활용한 금융 및 경제 분야를 공부하며 정리한 짧은 글이나 마저 끝내지 못한 연구 등을 올리는 개인 블로그입니다.\n\n\n모든 블로그 글 보기\n\n\n주요 관심주제: 금융 머신러닝(machine learning)/딥러닝(deep learning), 가격 모멘텀(momentum) 예측, 전략 강화 모형, 메타 라벨링(meta-labeling), 금융 시스템 리스크(systemic risk) 측정 등\n\n이 블로그의 모든 글은 오직 교육용 목적입니다.\nAbout Me"
  },
  {
    "objectID": "listing.html",
    "href": "listing.html",
    "title": "Cho's FinBlog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n머신러닝을 이용한 트레이딩: (0) 트레이딩 개요\n\n\n\ntrading\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n머신러닝을 이용한 트레이딩: (1) 시장 데이터 수집\n\n\n\ntrading\n\n\ndata\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n머신러닝을 이용한 트레이딩: (2) 가격 모멘텀 라벨링\n\n\n\ntrading\n\n\nlabeling\n\n\nmomentum\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n머신러닝을 이용한 트레이딩: (3) 피쳐 생성\n\n\n\ntrading\n\n\nfeature\n\n\nmomentum\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n머신러닝을 이용한 트레이딩: (4) 피쳐 선정\n\n\n\ntrading\n\n\nfeature selection\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n머신러닝을 이용한 트레이딩: (5) 매매 시그널 분류\n\n\n\ntrading\n\n\nsignals\n\n\nclassification\n\n\nmachine learning\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n머신러닝을 이용한 트레이딩: (6) 매매 규칙\n\n\n\ntrading\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n머신러닝을 이용한 트레이딩: (7) 매매 신뢰도 측정과 전략 강화\n\n\n\ntrading\n\n\nenhancing\n\n\nmachine learning\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n시스템 리스크 분석: (1) 시스템 리스크 개념\n\n\n\nsystemic risk\n\n\nrisk\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n시스템 리스크 분석: (2) 시스템 리스크 측정 방법 - 선행연구 요약\n\n\n\nsystemic risk\n\n\nmeasures\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n시스템 리스크 분석: (2-1) 거시경제지표를 활용한 경제전체 위험도 측정 방법\n\n\n\nsystemic risk\n\n\ncycle\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n시스템 리스크 분석: (2-2) 시장위험 측정 방법\n\n\n\nsystemic risk\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n시스템 리스크 분석: (2-3) 금융기관 단면적 위험 측정방법\n\n\n\nsystemic risk\n\n\nfinancial institution\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n시스템 리스크 분석: (2-4) 금융기관 부도확률 위험 측정방법\n\n\n\nsystemic risk\n\n\nfinancial institution\n\n\ndefault\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n시스템 리스크 분석: (2-5) 네트워크 측정방법\n\n\n\nsystemic risk\n\n\nfinancial institution\n\n\nnetwork\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/ml_trading/0_Introduction.html",
    "href": "posts/ml_trading/0_Introduction.html",
    "title": "머신러닝을 이용한 트레이딩: (0) 트레이딩 개요",
    "section": "",
    "text": "머신러닝을 이용한 트레이딩\n\n머신러닝을 이용한 모멘텀 예측과 전략 강화\n\n아래는 구체적인 트레이딩에 머신러닝을 활용하는 구조를 보여준다. 다만, 블로그에서는 이 중 일부만 다루기로 한다.\n\n1. 금융 데이터와 바(bar)\n\n틱 데이터로부터 시간 기준 혹은 금액 기준 바 형성\n\n\n\n2. 매수, 매도 시그널 포착\n\n기술적분석을 이용한 기존 모멘텀 전략 (MA crossover, RSI..)\n추가적인 ML 분류기를 이용한 모멘텀 감지\n\n\n\n3. 매매 규칙\n\n위의 매매 시그널로 진입 규칙 설정\n이익실현, 손절, 최대보유기간 등을 고려해 청산 규칙 설정\n과거 시나리오를 바탕으로 각 매매 결과 기록 (뒤의 강화 전략을 위해)\n\n\n\n4. 전략 강화 ML 모형\n\n4.1. 피쳐 생성 (\\(X\\))\n\n시장 데이터(가격, 거래량 등)와 기술적 지표\n시장 미시구조적(microstructure) 특징\n거시경제 변수\n자산 펀더멘털\nSNS/뉴스 센티멘트, 분석 컨센서스 등\n\n\n\n4.2. 머신러닝 모형 최적화\n\n피쳐 선정\nCross-validation (Purged k-fold)\n하이퍼파라미터(hyperparameter) 튜닝\nAutoML 스태킹 (혹은 각각의 Random forest, Adaboost, SVM, GBM, XGBoost, LSTM 등)\n성능 (accuracy, f1 score, roc-auc)\n\n\n\n4.3. 결과\n\n매매 신뢰도 (각각의 매매 시그널에 따라 실제 매매했을 때의 성공 예측 확률)\n\n\n\n\n5. 트레이딩 결정\n\n모멘텀 전략으로부터의 매매 시그널에 따라 베팅할지 혹은 패스할지를 결정\n위의 매매 신뢰도를 바탕으로 매매 금액/비중을 고려\n\n\n\n6. 백테스팅\n\nCumulative returns, Sharpe ratio, max drawdown, win ratio\n\n\n\n\n참고 문헌:\n\nAdvances in Financial Machine Learning, Lopez de Prado (2018)\n\n\n\n플로우차트\nSimple version \n\nDetailed version"
  },
  {
    "objectID": "posts/ml_trading/1_Get_Market_Data.html",
    "href": "posts/ml_trading/1_Get_Market_Data.html",
    "title": "머신러닝을 이용한 트레이딩: (1) 시장 데이터 수집",
    "section": "",
    "text": "기간: 2005년 7월 27일부터 2022년 1월 17일 까지\n삼성전자의 OHLCV(시가, 고가, 저가, 종가, 거래량)\n투자주체별 거래량: 개인, 외국인 기관\n\n파이썬 라이브러리인 FinanceDataReader와 yfinance을 이용한다. 순매수량 데이터는 대신증권 API로 받은 데이터를 이용한다.\n모든 시장 데이터는 일별 데이터이다.\n\n# lib\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns;sns.set()\n\nimport FinanceDataReader as fdr\nimport yfinance as yf\ndf_ohlc = fdr.DataReader('005930','2005-7-27','2022-1-17').iloc[:,0:4] #가격 수정되어 있음\nvolume_yf = yf.download('005930.KS','2005-7-27','2022-1-17',auto_adjust=True).Volume # 수정거래량\n\ndf_ohlcv = df_ohlc.join(volume_yf).dropna()\ndf = tautil.ohlcv(df_ohlcv)\n\nquantity_ = pd.read_csv('C:data/순매수량.csv')\nquantity_ = quantity_.iloc[:-1,1:5]\nquantity_.columns = ['Date','individual','foreign','institutional']\nquantity_.index = quantity_['Date'].apply(lambda x: pd.to_datetime(str(x), format='%Y%m%d'))\nquantity_.drop(columns='Date',inplace=True)\n\ndf = df.join(quantity_).dropna()\n\n얻은 데이터는 뒤에서 시각화하기로 한다."
  },
  {
    "objectID": "posts/ml_trading/2_Labeling.html",
    "href": "posts/ml_trading/2_Labeling.html",
    "title": "머신러닝을 이용한 트레이딩: (2) 가격 모멘텀 라벨링",
    "section": "",
    "text": "삼성전자 종가를 기준으로 가격의 트렌드와 트렌드 강도(모멘텀)을 측정하여 이를 라벨(label)로 사용한다.\n\nTrend scanning 기법\n\n향후 N 기간동안의 선형 회귀 시 t통계량의 절댓값의 최대값을 사용\n즉, 일마다 향후 주어진 기간 중 가장 강한 트렌드를 예측하고자 하는 그날의 정답 트렌드로 지정\n\n\n이 실험에서는 측정한 트렌드(\\(|\\hat{t}|\\))를 분위 수로 나누어 이산적인 값으로 지정해주었다. 이는 다음에 분류(classification) 작업을 하기 위함이다.\n아래 그래프에서 기간 N을 달리했을 때를 비교한 뒤, 투자자의 트레이딩 기간을 고려해 이를 정해준다.\n\n# lib\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns;sns.set()\n\n# homemade\nfrom features import tautil\nfrom labeling import labeling\n\n\nimport FinanceDataReader as fdr\ndf_ = fdr.DataReader('005930','2010-1-1','2021-6-1')\ndf = tautil.ohlcv(df_)\n\n\nclose =df.close\n\n\nwindows=[60]\n\n\ntrend_scanning_regime_q3 = []\nfor i in windows:\n    trend_scanning_regime_q3.append(labeling.trend_scanning_label(close,window=i,q=3)[0].abs())\n\n\ntrend_scanning_q3 = []\nfor i in windows:\n    trend_scanning_q3.append(labeling.trend_scanning_label(close,window=i,q=3)[0])\n\n\nfor j in range(len(windows)):\n    y = trend_scanning_q3[j][:'2020']\n    i = windows[j]\n    f, (a0, a1) = plt.subplots(2, gridspec_kw={'height_ratios': [5, 1]}, figsize=(15,5))\n    f.suptitle(\"Trend Scanning Labels {}span 3cut\".format(i))\n    a0.plot(close[:'2020'],alpha=0.4)\n    a0.scatter(close[:'2020'].index,close[:'2020'],c=y, cmap='vlag')\n    a1.plot(y.fillna(0))\n    f.show()\n\n\n\n\n\nfor j in range(len(windows)):\n    y = np.sign(trend_scanning_q3[j]-1)+1\n    i = windows[j]\n    f, (a0, a1) = plt.subplots(2, gridspec_kw={'height_ratios': [5, 1]}, figsize=(15,5))\n    f.suptitle(\"Trend Scanning (long position) Labels {}span 4cut\".format(i))\n    a0.plot(close,alpha=0.4)\n    a0.scatter(close.index,close,c=y, cmap='vlag')\n    a1.plot(y.fillna(0))\n    f.savefig(\"c:image/labeling/trend_scanning_long_pos_{}.png\".format(i))\n    f.show()"
  },
  {
    "objectID": "posts/ml_trading/3_Get_Features.html",
    "href": "posts/ml_trading/3_Get_Features.html",
    "title": "머신러닝을 이용한 트레이딩: (3) 피쳐 생성",
    "section": "",
    "text": "앞서 구한 시장 데이터를 이용하여 피쳐(feature)를 생성한다.\n피쳐의 종류는 아래에서 설명한다.\n\n# lib\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns;sns.set()\nplt.style.use('tableau-colorblind10')\n\n# homemade\nfrom features import tautil, sadf, trnd_scan, microstructure_features\n\n\n시장 데이터\n\nmtd_data = pd.read_excel('C:data/mtd_data.xlsx')\ndate = mtd_data.iloc[:,1].rename('Date')\nopen = mtd_data.iloc[:,2].rename('open')\nhigh = mtd_data.iloc[:,3].rename('high')\nlow = mtd_data.iloc[:,4].rename('low')\nclose = mtd_data.iloc[:,5].rename('close')\nvolume = mtd_data.iloc[:,6].rename('volume')\n\n\ndf = pd.DataFrame([open,high,low,close,volume]).T\ndf.index=date\n\n\nquantity_ = pd.read_csv('C:data/순매수량.csv')\nquantity_ = quantity_.iloc[:-1,1:5]\nquantity_.columns = ['Date','individual','foreign','institutional']\nquantity_.index = quantity_['Date'].apply(lambda x: pd.to_datetime(str(x), format='%Y%m%d'))\nquantity_.drop(columns='Date',inplace=True)\ndf = df.join(quantity_).dropna()\n\n\ndf.to_csv(\"C:data/market_samsung.csv\")\ndf\n\n\n\n\n\n  \n    \n      \n      open\n      high\n      low\n      close\n      volume\n      individual\n      foreign\n      institutional\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2005-07-27\n      11040\n      11180\n      10960\n      11020\n      18434300\n      -2543250.0\n      -1411300.0\n      -1210850.0\n    \n    \n      2005-07-28\n      11040\n      11320\n      11040\n      11200\n      23659800\n      -2067600.0\n      3772300.0\n      -1517250.0\n    \n    \n      2005-07-29\n      11320\n      11320\n      11200\n      11300\n      17875500\n      1583050.0\n      796450.0\n      -1843600.0\n    \n    \n      2005-08-01\n      11320\n      11380\n      11220\n      11380\n      16471100\n      -3111550.0\n      1520100.0\n      -2652100.0\n    \n    \n      2005-08-02\n      11400\n      11420\n      11240\n      11360\n      14254000\n      -1567950.0\n      -1895300.0\n      -1310950.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2021-10-08\n      72300\n      72400\n      71500\n      71500\n      14043287\n      2612855.0\n      319900.0\n      -2923031.0\n    \n    \n      2021-10-12\n      70700\n      70900\n      68700\n      69000\n      31001484\n      12155071.0\n      -11004329.0\n      -1421202.0\n    \n    \n      2021-10-13\n      68700\n      69600\n      68300\n      68800\n      24172015\n      2224620.0\n      -5271845.0\n      2725596.0\n    \n    \n      2021-10-14\n      69000\n      69800\n      68800\n      69400\n      19520641\n      2011163.0\n      -3787173.0\n      1696662.0\n    \n    \n      2021-10-15\n      70200\n      71000\n      70000\n      70100\n      18051612\n      822742.0\n      -1376104.0\n      456288.0\n    \n  \n\n3965 rows × 8 columns\n\n\n\n\nfor i in df.columns:\n    plt.figure(figsize=(10,1))\n    plt.title(i)\n    plt.plot(df[i])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n피쳐 변수 (Feature)\n\n14개의 기술적분석 지표 (괄호안 숫자는 각 지표를 구하기 위한 window)\n\nRSI (15)\nWillams’s R (15)\nADX (15)\n\nAROON Indicator (20)\n\nDPO (20)\nMACD Difference (25,10,9)\nMass Index (10, 25)\nTRIX (15)\nATR (10)\nUI (15)\nCMF (20)\nFI (15)\nMFI (15)\nEOM SMA (15)\nVPT\n\n5,10,20일 기간의 가격 수익률\n30일 기간의 가격 변동성(표준편차)\n개인, 기관, 외국인 별 순매수량의 5일, 20일\n트렌드-스캐닝 백워드 t value span (20,60)\n미시구조론 변수 각 20일 이동평균\n\nkyle_lambda\n\namihud_lambda\n\nhasbrouck_lambda\n\nbekker_parkinson_volatility\ncorwin_schultz_estimator\n\n\n\nwindows_TA = [1]\nTA = tautil.get_my_stationary_ta_windows(df_ohlcv,windows_TA).dropna()\n\n\nwindows_mom = [5,10,20]\nwindows_std = [30]\n\nmoms = tautil.mom_std(df,windows_mom, windows_std)\nmoms = moms.iloc[:,:len(windows_mom)+len(windows_std)]\n\n\nquantity_ = pd.read_csv('C:data/순매수량.csv')\nquantity_ = quantity_.iloc[:-1,1:5]\nquantity_.columns = ['Date','individual','foreign','institutional']\nquantity_.index = quantity_['Date'].apply(lambda x: pd.to_datetime(str(x), format='%Y%m%d'))\nquantity_.drop(columns='Date',inplace=True)\n\n\nwindows_ma=[5,20]\nfor i in quantity_.columns:\n    for j in windows_ma:\n        quantity_['{} sma_{}'.format(i,j)] = quantity_[i].rolling(j).mean()\n        quantity_.dropna(inplace=True)\nquantity = quantity_.iloc[:,3:]\n\n\nspans = [20,60]\ntrnd_back = pd.DataFrame()\nfor span in spans:\n    trnd_back['trend_back_scan_{}'.format(span)] = trnd_scan.trend_backward_scanning(df.close, span).t_value\n\n\ndollar_volume = df.volume*df.close\n\n\nclose=df.close\nkyle = microstructure_features.get_bar_based_kyle_lambda(close, df.volume, 20).rename('kyle_lambda')\namihud = microstructure_features.get_bar_based_amihud_lambda(close, dollar_volume, 20).rename('amihud_lambda')\nhasbrouk = microstructure_features.get_bar_based_hasbrouck_lambda(close, dollar_volume,20).rename('hasbrouck_lambda')\nbp_vol = microstructure_features.get_bekker_parkinson_vol(df.high,df.low,20).rename('bekker_parkinson_vol')\ncorsch = microstructure_features.get_corwin_schultz_estimator(df.high,df.low,20).rename('corwin_schultz_estimator')\n\n\nmicrostructure = pd.concat([kyle,amihud,hasbrouk,bp_vol,corsch],axis=1)\n\n\nfeatures = TA.join([moms,quantity,trnd_back,microstructure]).dropna()\nfeatures.to_csv('C:data/features_samsung.csv')\n\n\nfeatures = features['2010':'2020']\nf, axs = plt.subplots(len(features.T),figsize=(10,70),gridspec_kw={'hspace': 1})\nfor i in range(len(features.T)):\n    axs[i].title.set_text(features.columns[i])\n    axs[i].plot(features.iloc[:,i])\nf.savefig('C:image/features.png')"
  },
  {
    "objectID": "posts/ml_trading/4_Feature_Selection.html",
    "href": "posts/ml_trading/4_Feature_Selection.html",
    "title": "머신러닝을 이용한 트레이딩: (4) 피쳐 선정",
    "section": "",
    "text": "Input\n\nLabel: 트렌드 스캐닝 기준 up-trend vs. (down- or no-trend)\n기간 : 2005 - 2010\n피쳐: market data features\n\nModel: 랜덤포레스트\n\n5가지 피쳐 선정 기법 비교: original, mda-kmeans, mda-optics, mda-onc, rfecv, sbfs\n\nOutput\n\n최상의 방법으로 선정한 피쳐 사용"
  },
  {
    "objectID": "posts/ml_trading/4_Feature_Selection.html#클러스터-기반-방법",
    "href": "posts/ml_trading/4_Feature_Selection.html#클러스터-기반-방법",
    "title": "머신러닝을 이용한 트레이딩: (4) 피쳐 선정",
    "section": "클러스터 기반 방법",
    "text": "클러스터 기반 방법\n\nclustering\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_sc = sc.fit_transform(X)\nX_sc = pd.DataFrame(X_sc, index=X.index, columns=X.columns)\n\n\nX_sc=X_sc[:'2020']\n\n\nsilhouette_coefficients = []\nkmeans_kwargs = {\n    \"init\": \"random\",\n    \"n_init\": 10,\n    \"max_iter\": 300,\n    \"random_state\": 42,\n}\n\nfor k in range(2, 30):\n    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n    kmeans.fit(X.T)\n    score = silhouette_score(X.T, kmeans.labels_)\n    silhouette_coefficients.append(score)\n\n\nn_clusters=np.argmin(silhouette_coefficients)+2\nkmeans = KMeans(\n    init=\"random\",\n    n_clusters=n_clusters,\n    n_init=10,\n    max_iter=300,\n    random_state=42)\nkmeans.fit(X_sc.T)\nclusters_kmeans = {i: X_sc.columns[np.where(kmeans.labels_ == i)[0]].tolist() for i in np.unique(kmeans.labels_)}\n\n\noptics = OPTICS(min_cluster_size=2)\noptics.fit(X.T)\nclusters_optics = {i: X_sc.columns[np.where(optics.labels_ == i)[0]].tolist() for i in np.unique(optics.labels_)}\n\n\n# 오래 걸림.\nclusters_onc_dist = cluster.get_feature_clusters(X_sc, dependence_metric= 'distance_correlation')\n\nNo feature/s found with low silhouette score. All features belongs to its respective clusters\n\n\n\n\nmda - selection\n\n#labeling\ntrend_scanning_window = 60\ntrend_scanning_q = 3\nts_out = labeling.trend_scanning_label(market_df.close, window = trend_scanning_window, q = trend_scanning_q)\nmom_label = ts_out[0]\ny = np.sign(mom_label-1)+1 # up-trend vs others\n\n\nraw_X = X_sc.copy()\n\ntmp = raw_X.join(y).dropna()\nX=tmp.iloc[:,:-1]\ny=tmp.iloc[:,-1]\n\n# train & test split\n# use previous data for feature selection\nX = X.loc['2005':'2010']\ny = y.loc['2005':'2010']\n\n\n# CV\nn_cv=4\nt1 = ts_out[1].loc[X.index]\ncv = PKFold(n_cv,t1,0.01)\n\n\nclusters = [clusters_kmeans[i] for i in range(n_clusters)]\nclusters2 = [clusters_optics[i] for i in clusters_optics.keys()]\nclusters3 = clusters_onc_dist\n\n\nclf = RandomForestClassifier(n_estimators=1000,class_weight='balanced')\nmda_cluster = importance.mean_decrease_accuracy(clf,X,y,cv,clustered_subsets=clusters)\nmda_cluster2 = importance.mean_decrease_accuracy(clf,X,y,cv,clustered_subsets=clusters2)\nmda_cluster3 = importance.mean_decrease_accuracy(clf,X,y,cv,clustered_subsets=clusters3)\n\n\nfeatures_mda_kmeans = mda_cluster.loc[mda_cluster['mean'] == mda_cluster['mean'].max()].index\nfeatures_mda_optics = mda_cluster2.loc[mda_cluster2['mean'] == mda_cluster2['mean'].max()].index\nfeatures_mda_onc_dist = mda_cluster3.loc[mda_cluster3['mean'] == mda_cluster3['mean'].max()].index\n\n# 0에서 min 값으로 변경함.\n\n\nnew_X1 = X[features_mda_kmeans]\nnew_X2 = X[features_mda_optics]\nnew_X3 = X[features_mda_onc_dist]"
  },
  {
    "objectID": "posts/ml_trading/4_Feature_Selection.html#비-클러스터링-방법",
    "href": "posts/ml_trading/4_Feature_Selection.html#비-클러스터링-방법",
    "title": "머신러닝을 이용한 트레이딩: (4) 피쳐 선정",
    "section": "비-클러스터링 방법",
    "text": "비-클러스터링 방법\n\nRFECV(Rercursive Feature Elimination with CV)\n\n\n# 오래걸림\n\nrf = RandomForestClassifier(class_weight='balanced')\n\nmin_features_to_select = 2  # Minimum number of features to consider\nrfecv = RFECV(\n    estimator=rf,\n    step=1,\n    cv=cv,\n    scoring=\"accuracy\",\n    min_features_to_select=min_features_to_select,\n)\nnew_X5_ = rfecv.fit_transform(X,y)\n\n\nnew_X5 = pd.DataFrame(new_X5_, index=X.index, columns=rfecv.get_feature_names_out())\n\n\nX_list = [X,new_X1,new_X2,new_X3,new_X5]"
  },
  {
    "objectID": "posts/ml_trading/5_Get_Trading_Signals.html",
    "href": "posts/ml_trading/5_Get_Trading_Signals.html",
    "title": "머신러닝을 이용한 트레이딩: (5) 매매 시그널 분류",
    "section": "",
    "text": "모멘텀 분류기 (Momentum Classifier)\n\ninputs\n\nlabels: trend-scanning labeling (up vs. down & no trend)\nfeatures: market-data selected features\n\nmodels: SVM, Random Forest, Gradient Boosting, LSTM\noutputs\n\nmomentum signals\n\n\n\n# lib\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns;sns.set()\nplt.style.use('tableau-colorblind10')\n\n# different models\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score\n\nfrom features import tautil\nfrom labeling import labeling\nfrom mlutil.pkfold import PKFold\n\n\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n\nget X,y\n\nmarket_df = pd.read_csv('C:data/market_samsung.csv')\nmarket_df = market_df.rename(columns={market_df.columns[0]:'Date'})\nmarket_df.index = pd.to_datetime(market_df.Date)\nmarket_df.drop(columns='Date',inplace=True)\nmarket_df.dropna(inplace=True)\n\nfeature_df = pd.read_csv('C:data/features_samsung.csv')\nfeature_df = feature_df.rename(columns={feature_df.columns[0]:'Date'})\nfeature_df.index = pd.to_datetime(feature_df.Date)\nfeature_df.drop(columns='Date',inplace=True)\nfeature_df.dropna(inplace=True)\n\n\nselected_features = pd.read_csv('C:data/selected_features.csv').columns[1:]\n\n\nfeature = feature_df.dropna()\nfeature = feature[selected_features]\n\nsc = StandardScaler()\nX_sc = sc.fit_transform(feature)\nX_sc = pd.DataFrame(X_sc, index=feature.index, columns=feature.columns)\n\n\nfor i in feature.columns:\n    plt.figure(figsize=(10,1))\n    plt.title(i)\n    plt.plot(feature[i])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#labeling\ntrend_scanning_window = 60\ntrend_scanning_q = 3\nts_out = labeling.trend_scanning_label(market_df['2010':].close, window = trend_scanning_window, q = trend_scanning_q)\nmom_label = ts_out[0]\n\n\ny = np.sign(mom_label-1)+1 # up-trend vs. others(down-trend and no-trend)\n\n\ny_ = y[:'2020']\nclose = market_df.close.loc[y_.index]\nf, (a0, a1) = plt.subplots(2, gridspec_kw={'height_ratios': [5, 1]}, figsize=(15,5))\nf.suptitle(\"Trend Scanning Labels: 1(up-trend), 0(down & no-trend)\")\na0.plot(close,alpha=0.2)\na0.scatter(close.index,close,c=y_, cmap='vlag')\na1.plot(y_.fillna(0.5))\nf.show()\n\n\n\n\n\nraw_X = X_sc.copy()\n\ntmp = raw_X.join(y).dropna()\nX=tmp.iloc[:,:-1]\ny=tmp.iloc[:,-1]\n\n\n\nModels & Models with Hyperparameter tuning\n\n# Cross Validation (purged k-fold)\nn_cv=4\nt1= ts_out[1].loc[X.index]\ncv = PKFold(n_cv,t1,0.01)\n\n\n# Choose model (SVM-rbf)\nC = [0.1, 1, 10]\nparam_grid_rbf = dict(C=C)\nsvc_rbf = SVC(kernel='rbf', probability=True)\ngs_svc_rbf = GridSearchCV(estimator=svc_rbf, param_grid= param_grid_rbf, cv=cv)\ngs_svc_rbf.fit(X,y)\nsvc_best = gs_svc_rbf.best_estimator_\nsvc_best\n\nSVC(C=0.1, probability=True)\n\n\n\nn_estimators = [500,1000]\nmax_depth = [3,5]\nparam_grid_rfc = dict(n_estimators=n_estimators, max_depth=max_depth)\nrfc = RandomForestClassifier(class_weight='balanced')\ngs_rfc = GridSearchCV(estimator=rfc, param_grid= param_grid_rfc, cv=cv)\ngs_rfc.fit(X,y)\nrfc_best = gs_rfc.best_estimator_\nrfc_best\n\nRandomForestClassifier(class_weight='balanced', max_depth=5, n_estimators=1000)\n\n\n\nn_estimators_ab = [200,500]\nlearning_rate = [0.01,0.1]\nparam_grid_abc = dict(n_estimators=n_estimators_ab, learning_rate=learning_rate)\n\nabc=AdaBoostClassifier()\ngs_abc = GridSearchCV(estimator=abc, param_grid= param_grid_abc, cv=cv)\ngs_abc.fit(X,y)\nada_best = gs_abc.best_estimator_\nada_best\n\nAdaBoostClassifier(learning_rate=0.01, n_estimators=200)\n\n\n\nn_estimators_gb = [200,500]\nlearning_rate = [0.01,0.1]\nparam_grid_gbc = dict(n_estimators=n_estimators_gb, learning_rate=learning_rate)\ngbc=GradientBoostingClassifier()\ngs_gbc = GridSearchCV(estimator=gbc, param_grid= param_grid_gbc, cv=cv)\ngs_gbc.fit(X,y)\ngbc_best = gs_gbc.best_estimator_\ngbc_best\n\nGradientBoostingClassifier(n_estimators=200)\n\n\n\nclf_list = [svc_best, rfc_best, ada_best, gbc_best]\nestimators=['SVM_best','RF_best','AdaBoost_best','GradientBoost_best']\nscores_list = []\ny_preds_list = []\ny_probs_list = []\n\n# for ML model prediction\nfor clf in clf_list:\n    y_preds_ = []\n    y_probs_ = []\n\n    for train, test in cv.split(X, y):\n        clf.fit(X.iloc[train], y.iloc[train])\n        y_true = y.iloc[test]\n        y_pred = clf.predict(X.iloc[test])\n        y_probs = clf.predict_proba(X.iloc[test])\n        y_probs = y_probs[:, 1]\n        y_pred_series = pd.Series(y_pred,index=y[test].index)\n        y_probs_series = pd.Series(y_probs,index=y[test].index)\n        y_preds_.append(y_pred_series)\n        y_probs_.append(y_probs_series)\n    \n    \n    y_preds__ = pd.concat([i for i in y_preds_])\n    y_probs__ = pd.concat([i for i in y_probs_])\n    y_true__ = y.loc[y_preds__.index]\n    accs = accuracy_score(y_true__, y_preds__)\n    f1=f1_score(y_true__, y_preds__)\n    roc=roc_auc_score(y_true__, y_probs__)\n    prec=precision_score(y_true__, y_preds__)\n    score = [accs, f1, roc, prec]\n    scores_list.append(score)\n    y_preds_list.append(y_preds__)\n    y_probs_list.append(y_probs__)\n\n\nresults = pd.DataFrame(scores_list, columns=['accuracy','f1 score','roc auc score','precision score'],index=estimators)\nresult_show = results.sort_values('accuracy', ascending=False)\nresult_show\n\n\n\n\n\n  \n    \n      \n      accuracy\n      f1 score\n      roc auc score\n      precision score\n    \n  \n  \n    \n      AdaBoost_best\n      0.563135\n      0.123726\n      0.448611\n      0.412621\n    \n    \n      SVM_best\n      0.510160\n      0.299793\n      0.476269\n      0.380263\n    \n    \n      GradientBoost_best\n      0.493832\n      0.468166\n      0.489565\n      0.421993\n    \n    \n      RF_best\n      0.488389\n      0.503871\n      0.498126\n      0.427718\n    \n  \n\n\n\n\n\ny_probs_df = pd.concat(y_probs_list, axis=1).dropna()\ny_probs_df.columns = estimators\n\ny_probs_df['mean_'] = y_probs_df.mean(axis=1)\n\nmomentum = pd.Series(y_probs_df.mean_,index=y_probs_df.index)\n\n\n\nSelect the model\n\nplt.hist(momentum)[1]\n\narray([0.15405224, 0.22668779, 0.29932334, 0.37195889, 0.44459443,\n       0.51722998, 0.58986553, 0.66250108, 0.73513662, 0.80777217,\n       0.88040772])\n\n\n\n\n\n\nmomentum = momentum.loc['2010':'2020']\n\n\nclose = market_df.close.loc[momentum.index]\nplt.figure(figsize=(10,4))\nplt.plot(close, alpha=0.2)\n#plt.title('Momentum signals')\nplt.scatter(momentum.index, close, c=momentum, s=10,cmap='gray_r',vmin=0,vmax=1)\nplt.colorbar()\nplt.legend(['price','darker = long signals'])\nplt.show()\n\n\n\n\n\nmomentum.rename('signals').to_csv('C:data/momentum_signals.csv')"
  },
  {
    "objectID": "posts/ml_trading/6_Trading_Rules.html",
    "href": "posts/ml_trading/6_Trading_Rules.html",
    "title": "머신러닝을 이용한 트레이딩: (6) 매매 규칙",
    "section": "",
    "text": "Trading rules: 매수 진입만 허용"
  },
  {
    "objectID": "posts/ml_trading/6_Trading_Rules.html#진입-규칙",
    "href": "posts/ml_trading/6_Trading_Rules.html#진입-규칙",
    "title": "머신러닝을 이용한 트레이딩: (6) 매매 규칙",
    "section": "진입 규칙",
    "text": "진입 규칙\n\nMomentum Prediction\n\nsignals = pd.read_csv('C:data/momentum_signals.csv')\nsignals.index = pd.to_datetime(signals['Date'])\nsignals.drop(columns='Date',inplace=True)\n\n\nsignals = signals['signals'].loc['2010':'2020']\n\n\nscaler = normalize\nscaler2 = MinMaxScaler()\nsignals = pd.Series(scaler2.fit_transform(normalize(signals.values.reshape(-1,1),axis=0)).reshape((-1,)), \n                           index=signals.index).rename('signals')\n\n\nplt.hist(signals,bins=50)[2]\nplt.title('Distribution of momentum signals')\nplt.show()\n\n\n\n\n\nthresholds = [0, 0.3]\n\n\nenter_ml_list=[]\nfor h in thresholds:\n    enter_ml_list.append(signals.loc[signals>h].index)\n\n\nfor i in range(len(thresholds)):\n    plt.figure(figsize=(10,3))\n    plt.plot(close, alpha=0.5)\n    plt.title('Enter points ML Prediction (Option {})'.format(i+1))\n    plt.plot(close.loc[enter_ml_list[i]],marker='^',linewidth=0,alpha=0.3)\n    plt.legend(['price','Long signals from momentum classifier'])\n    plt.show()\n\n\n\n\n\n\n\n\n\nTech. Analysis Long/short decision\n\nopen = market_df.open['2010':'2020']\nrsi = tautil.RSIIndicator(open,14).rsi().dropna()\nlong = (rsi>=50) & (rsi<70)\nenter_ta = rsi.loc[long].index\n\n\nplt.figure(figsize=(10,3))\nplt.plot(close, alpha=0.5)\nplt.title('Enter points TA')\nplt.plot(close.loc[enter_ta],marker='^',linewidth=0,alpha=0.3)\nplt.legend(['price','Long signals from rsi'])\nplt.show()\n\n\n\n\n\nenter_list = [enter_ta]\nenter_list.append((enter_ml_list[1]& enter_ta).sort_values().drop_duplicates())\n\n\nfor i in range(len(thresholds)):\n    plt.figure(figsize=(10,3))\n    plt.plot(close, alpha=0.5)\n    plt.title('Enter points (Option {})'.format(i+1))\n    plt.plot(close.loc[enter_list[i]],marker='^',linewidth=0,alpha=0.3)\n    plt.legend(['price','Enter points'])\n    plt.show()\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10,3))\nplt.plot(close, alpha=0.5)\nplt.title('Position Enter points'.format(i+1))\nplt.plot(close.loc[enter_list[1]],marker='^',linewidth=0,alpha=0.3)\nplt.legend(['price','Enter points'])\nplt.show()"
  },
  {
    "objectID": "posts/ml_trading/6_Trading_Rules.html#청산-규칙",
    "href": "posts/ml_trading/6_Trading_Rules.html#청산-규칙",
    "title": "머신러닝을 이용한 트레이딩: (6) 매매 규칙",
    "section": "청산 규칙",
    "text": "청산 규칙\n\n# no Rule (benchmark)\npt_sl_bm = [1000,1000]\nmax_holding_bm = [1, 0]\nno_exit_rule = [pt_sl_bm,max_holding_bm]\n\n\n#dynamic target rule\nmax_holding = [60, 0]\nclose_ = market_df.close['2009':'2020']\nchanges = close_.pct_change(1).to_frame()\nfor i in range(2,max_holding[0]+1):\n    changes = changes.join(close_.pct_change(i).rename('close {}'.format(i)))\ndynamic_target = changes.abs().dropna().mean(axis=1)['2010':]\n\n\nbarrier_exit_list=[]\nbarrier_exit_list.append(get_barrier(close, enter_list[1], [1,1], max_holding, target = dynamic_target))  #dynamic  \n\nrts_exit_list=[]\nfor i in range(len(barrier_exit_list)):\n    rts_exit_list.append(make_rt(close,barrier_exit_list[i].dropna()))\n\n\nplt.figure(figsize=(10,2))\nplt.title('Dynamic exit rule returns')\nplt.plot(barrier_exit_list[0].ret)\nplt.legend(['Returns of dynamic exit target rate'])\n\n<matplotlib.legend.Legend at 0x1fe01fa7148>"
  },
  {
    "objectID": "posts/ml_trading/6_Trading_Rules.html#벤치마크",
    "href": "posts/ml_trading/6_Trading_Rules.html#벤치마크",
    "title": "머신러닝을 이용한 트레이딩: (6) 매매 규칙",
    "section": "벤치마크",
    "text": "벤치마크\n\n매번 매매시\n\n\nbarrier_bm = get_barrier(close, close.index, no_exit_rule[0], no_exit_rule[1]) #no rule\n\n\nrts_bm = make_rt(close,barrier_bm.dropna())\n\n\nround_trip.get_df_ann_sr(rts_bm,'Benchmark',years=11)\n\n\n\n\n\n  \n    \n      \n      Benchmark\n    \n  \n  \n    \n      avg_n_bets_per_year\n      243.272727\n    \n    \n      win_ratio\n      0.518835\n    \n    \n      annualized_sharpe_ratio\n      0.537080\n    \n  \n\n\n\n\n\nresult_df = pd.concat([round_trip.get_df_ann_sr(rts_bm,'No Rule')], axis=1)\nfor i in range(len(rts_exit_list)):\n    result_df = result_df.join(round_trip.get_df_ann_sr(rts_exit_list[i],'Enter & Exit Rule'))\n    \nresult_df\n\n\n\n\n\n  \n    \n      \n      No Rule\n      Enter & Exit Rule\n    \n  \n  \n    \n      avg_n_bets_per_year\n      243.272727\n      105.000000\n    \n    \n      win_ratio\n      0.518835\n      0.590988\n    \n    \n      annualized_sharpe_ratio\n      0.537080\n      1.800316"
  },
  {
    "objectID": "posts/ml_trading/6_Trading_Rules.html#최적-청산-규칙-파라미터",
    "href": "posts/ml_trading/6_Trading_Rules.html#최적-청산-규칙-파라미터",
    "title": "머신러닝을 이용한 트레이딩: (6) 매매 규칙",
    "section": "최적 청산 규칙 파라미터",
    "text": "최적 청산 규칙 파라미터\n\n#dynamic target rule\n# different maximum holding\nclose_ = market_df.close['2009':'2020']\nrolling = np.arange(20,260,10)\nmhs = [20,60,120,260]\nwin_ratios = pd.DataFrame()\n\nfor mh in mhs:\n    max_holding = [mh, 0]\n    dynamic_targets = []\n    for j in rolling:\n        for i in range(2,j+1):\n            changes = close_.pct_change(1).to_frame()\n            changes = changes.join(close_.pct_change(i).rename('close {}'.format(i)))\n        dynamic_target = changes.abs().dropna().mean(axis=1)['2010':]\n        dynamic_targets.append(dynamic_target)\n\n    barrier_exit_list_rolling=[]\n    for i in range(len(dynamic_targets)):\n        barrier_exit_list_rolling.append(get_barrier(close, enter_list[1], [1,1], max_holding, target = dynamic_targets[i]))  #dynamic  \n\n    rts_exit_list=[]\n    for i in range(len(barrier_exit_list_rolling)):\n        rts_exit_list.append(make_rt(close,barrier_exit_list_rolling[i].dropna()))\n\n    result_df = pd.concat([round_trip.get_df_ann_sr(rts_bm,'No Rule')], axis=1)\n    for i in range(len(rts_exit_list)):\n        result_df = result_df.join(round_trip.get_df_ann_sr(rts_exit_list[i],'{}'.format(rolling[i])))\n\n    win_ratios['Max. holding {} days'.format(mh)] = result_df.T.win_ratio\nwin_ratios\n\n\n\n\n\n  \n    \n      \n      Max. holding 20 days\n      Max. holding 60 days\n      Max. holding 120 days\n      Max. holding 260 days\n    \n  \n  \n    \n      No Rule\n      0.518835\n      0.518835\n      0.518835\n      0.518835\n    \n    \n      20\n      0.541054\n      0.576857\n      0.572169\n      0.575130\n    \n    \n      30\n      0.564014\n      0.573898\n      0.570069\n      0.570441\n    \n    \n      40\n      0.532872\n      0.559689\n      0.553155\n      0.550562\n    \n    \n      50\n      0.555363\n      0.588591\n      0.589455\n      0.587727\n    \n    \n      60\n      0.564991\n      0.605195\n      0.605195\n      0.603463\n    \n    \n      70\n      0.562392\n      0.605195\n      0.600866\n      0.595671\n    \n    \n      80\n      0.557192\n      0.600000\n      0.584416\n      0.586147\n    \n    \n      90\n      0.555459\n      0.599134\n      0.598787\n      0.601732\n    \n    \n      100\n      0.563258\n      0.587522\n      0.600520\n      0.599653\n    \n    \n      110\n      0.555459\n      0.575022\n      0.595486\n      0.604510\n    \n    \n      120\n      0.561525\n      0.585281\n      0.611785\n      0.620451\n    \n    \n      130\n      0.560659\n      0.593560\n      0.608014\n      0.620209\n    \n    \n      140\n      0.559792\n      0.601739\n      0.607485\n      0.623151\n    \n    \n      150\n      0.555844\n      0.579496\n      0.578261\n      0.597391\n    \n    \n      160\n      0.545927\n      0.574413\n      0.581010\n      0.600174\n    \n    \n      170\n      0.558925\n      0.598432\n      0.598082\n      0.625981\n    \n    \n      180\n      0.551127\n      0.586297\n      0.594266\n      0.615451\n    \n    \n      190\n      0.548918\n      0.594805\n      0.601386\n      0.629116\n    \n    \n      200\n      0.548918\n      0.591342\n      0.606586\n      0.623050\n    \n    \n      210\n      0.535065\n      0.596886\n      0.612987\n      0.627706\n    \n    \n      220\n      0.555844\n      0.614187\n      0.628571\n      0.643290\n    \n    \n      230\n      0.552768\n      0.595506\n      0.605195\n      0.624567\n    \n    \n      240\n      0.561525\n      0.618387\n      0.629887\n      0.647569\n    \n    \n      250\n      0.559792\n      0.609714\n      0.625543\n      0.644097\n    \n  \n\n\n\n\n\nplt.figure(figsize=(15,6))\nplt.title(\"Exit rules\")\nplt.plot(win_ratios)\nplt.legend(win_ratios)\nplt.ylabel('win ratio')\nplt.xlabel('Rolling days for calculating target rate')\nplt.show()"
  },
  {
    "objectID": "posts/ml_trading/7_Enhancing_and_Bet_Confidence.html",
    "href": "posts/ml_trading/7_Enhancing_and_Bet_Confidence.html",
    "title": "머신러닝을 이용한 트레이딩: (7) 매매 신뢰도 측정과 전략 강화",
    "section": "",
    "text": "전략 강화 모형\n\ninputs\n\n라벨: 매매 규칙에 따른 전략의 결과 = 각 매매 성공/실패 여부\n\nmodels: SVM, Random Forest, Gradient Boosting, LSTM\noutputs\n\n매매 신뢰도 (bet confidence)\n\n매매 신뢰도를 이용한 전략 강화\n\n\n# lib\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns;sns.set()\nplt.style.use('tableau-colorblind10')\n\n# different models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, normalize\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score\n\n# homemade\nfrom feature_engineering import dimension_reduction as DR\nfrom features import tautil\nfrom labeling import labeling\nfrom backtest import round_trip\nfrom triple_barrier import make_rt\n\nfrom mlutil.pkfold import PKFold\n\n\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n\nget X,y\n\nmarket_df = pd.read_csv('C:data/market_samsung.csv')\nmarket_df = market_df.rename(columns={market_df.columns[0]:'Date'})\nmarket_df.index = pd.to_datetime(market_df.Date)\nmarket_df.drop(columns='Date',inplace=True)\nmarket_df.dropna(inplace=True)\nclose = market_df.close['2010':'2020']\n\nfeature_df = pd.read_csv('C:data/features_samsung.csv')\nfeature_df = feature_df.rename(columns={feature_df.columns[0]:'Date'})\nfeature_df.index = pd.to_datetime(feature_df.Date)\nfeature_df.drop(columns='Date',inplace=True)\nfeature_df.dropna(inplace=True)\n\nselected_features = pd.read_csv('C:data/selected_features.csv').columns[1:]\n\n\nfeature = feature_df.dropna()\nfeature = feature[selected_features]\nsc = StandardScaler()\nX_sc = sc.fit_transform(feature)\nX_sc = pd.DataFrame(X_sc, index=feature.index, columns=feature.columns)\n\n\n#benchmark\nbarrier_bm = pd.read_csv('C:data/barrier_bm.csv')\nbarrier_bm.index = pd.to_datetime(barrier_bm.Date)\nbarrier_bm.exit = pd.to_datetime(barrier_bm.exit)\nbarrier_bm.drop(columns='Date',inplace=True)\n\n\n#labeling\nbarrier = pd.read_csv('C:data/barrier.csv')\nbarrier.index = pd.to_datetime(barrier.Date)\nbarrier.exit = pd.to_datetime(barrier.exit)\nbarrier.drop(columns='Date',inplace=True)\n\nrts = make_rt(close,barrier.dropna())\noutcome = rts.rt_returns\noutcome.index = rts.open_dt\n\n\n#meta-label\nwl = np.sign(np.sign(outcome)+1)\ny_ = wl\ny_.value_counts()\n\n1.0    608\n0.0    421\nName: rt_returns, dtype: int64\n\n\n\nloss = wl.value_counts()[0]\nwin = wl.value_counts()[1]\nplt.figure(figsize=(10,3))\nplt.scatter(wl[wl==1].index,close.loc[wl[wl==1].index], alpha=0.5)\nplt.scatter(wl[wl==0].index,close.loc[wl[wl==0].index], marker='x', alpha=0.5)\nplt.legend(['win 1','lose 0'])\nplt.title('y (meta-label): win {}, lose {}'.format(win,loss))\nplt.show()\n\n\n\n\n\nraw_X = X_sc.copy()\ntmp = raw_X.join(y_).dropna()\nX=tmp.iloc[:,:-1]\ny=tmp.iloc[:,-1]\n\n\n\nModel Construction\n\n# Choose model\n\n# Cross Validation (k-fold)\nn_cv=4\nt1 = pd.to_datetime(barrier.exit.loc[X.index])\ncv = PKFold(n_cv,t1,0)\n\n\n# Choose model (SVM-rbf)\nC = [0.1, 1,10]\nparam_grid_rbf = dict(C=C)\nsvc_rbf = SVC(kernel='rbf', probability=True)\ngs_svc_rbf = GridSearchCV(estimator=svc_rbf, param_grid= param_grid_rbf, cv=cv, scoring='precision')\ngs_svc_rbf.fit(X,y)\nsvc_best = gs_svc_rbf.best_estimator_\nsvc_best\n\nSVC(C=10, probability=True)\n\n\n\nn_estimators = [200,1000]\n#max_depth = [3,7]\nparam_grid_rfc = dict(n_estimators=n_estimators)\nrfc = RandomForestClassifier()\ngs_rfc = GridSearchCV(estimator=rfc, param_grid= param_grid_rfc, cv=cv, scoring='precision')\ngs_rfc.fit(X,y)\nrfc_best = gs_rfc.best_estimator_\nrfc_best\n\nRandomForestClassifier(n_estimators=200)\n\n\n\nn_estimators_ab = [50,100]\nlearning_rate = [1,0.1]\nparam_grid_abc = dict(n_estimators=n_estimators_ab, learning_rate=learning_rate)\n\nabc=AdaBoostClassifier()\ngs_abc = GridSearchCV(estimator=abc, param_grid= param_grid_abc, cv=cv, scoring='precision')\ngs_abc.fit(X,y)\nada_best = gs_abc.best_estimator_\nada_best\n\nAdaBoostClassifier(learning_rate=1, n_estimators=100)\n\n\n\nn_estimators_gb = [100,200]\nlearning_rate = [0.1,0.01]\nparam_grid_gbc = dict(n_estimators=n_estimators_gb, learning_rate=learning_rate)\ngbc=GradientBoostingClassifier()\ngs_gbc = GridSearchCV(estimator=gbc, param_grid= param_grid_gbc, cv=cv, scoring='precision')\ngs_gbc.fit(X,y)\ngbc_best = gs_gbc.best_estimator_\ngbc_best\n\nGradientBoostingClassifier(learning_rate=0.01, n_estimators=200)\n\n\n\n\nModel\n\nclf_list = [svc_best, rfc_best, ada_best, gbc_best]\nestimators=['SVM_best','RF_best','AdaBoost_best','GradientBoost_best']\nscores_list = []\ny_preds_list = []\ny_probs_list = []\n\n# for ML model prediction\nfor clf in clf_list:\n    y_preds_ = []\n    y_probs_ = []\n\n    for train, test in cv.split(X, y):\n        clf.fit(X.iloc[train], y.iloc[train])\n        y_true = y.iloc[test]\n        y_pred = clf.predict(X.iloc[test])\n        y_probs = clf.predict_proba(X.iloc[test])\n        y_probs = y_probs[:, 1]\n        y_pred_series = pd.Series(y_pred,index=y[test].index)\n        y_probs_series = pd.Series(y_probs,index=y[test].index)\n        y_preds_.append(y_pred_series)\n        y_probs_.append(y_probs_series)\n    \n    \n    y_preds__ = pd.concat([i for i in y_preds_])\n    y_probs__ = pd.concat([i for i in y_probs_])\n    y_true__ = y.loc[y_preds__.index]\n    accs = accuracy_score(y_true__, y_preds__)\n    f1=f1_score(y_true__, y_preds__)\n    roc=roc_auc_score(y_true__, y_probs__)\n    prec=precision_score(y_true__, y_preds__)\n    score = [accs, f1, roc, prec]\n    scores_list.append(score)\n    y_preds_list.append(y_preds__)\n    y_probs_list.append(y_probs__)\n\n\nresults = pd.DataFrame(scores_list, columns=['accuracy','f1 score','roc auc score','precision score'],index=estimators)\nresult_show = results.sort_values('precision score', ascending=False)\n\n\nresult_show\n\n\n\n\n\n  \n    \n      \n      accuracy\n      f1 score\n      roc auc score\n      precision score\n    \n  \n  \n    \n      AdaBoost_best\n      0.567541\n      0.631927\n      0.552471\n      0.635607\n    \n    \n      SVM_best\n      0.544218\n      0.585323\n      0.574228\n      0.632887\n    \n    \n      RF_best\n      0.549077\n      0.657817\n      0.537073\n      0.596257\n    \n    \n      GradientBoost_best\n      0.519922\n      0.609177\n      0.490364\n      0.586890\n    \n  \n\n\n\n\n\ny_probs_df = pd.DataFrame()\nfor i in range(len(estimators)):\n    y_probs_df[estimators[i]] = y_probs_list[i]\n\n\n#평균\npred_prob = pd.Series(y_probs_df.mean(axis=1),index=y_probs_df.index)\n\n#하나하나\n\n#y_probs_df_2 = y_probs_df[estimators[3]]\n#pred_prob = pd.Series(y_probs_df_2,index=y_probs_df_2.index)\n\n\npred_prob2=pd.Series(normalize(pred_prob.to_frame().T).reshape(-1,), index=y_probs_df.index).rename('bet_confidence')\n\n\nbet_confidence=pd.Series(MinMaxScaler().fit_transform(pred_prob2.to_frame()).reshape(-1,), index=y_probs_df.index).rename('bet_confidence')\n\n\nplt.title('Bet confidence distribution')\nplt.hist(bet_confidence, bins=30)[2]\nplt.xlabel('Bet confidence')\nplt.ylabel('counts')\n\nText(0, 0.5, 'counts')\n\n\n\n\n\n\nc = close.loc[bet_confidence.index]\nplt.figure(figsize=(10,5))\nplt.title('Bet confidence')\nplt.plot(close, alpha=0.1)\nplt.scatter(c.index,c, c = bet_confidence, s=20,cmap='vlag',vmin=0,vmax=1)\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\nAlgo Trading Backtest\n\nbarrier_bm = barrier_bm.dropna()\nbarrier_before = barrier.loc[bet_confidence.index].dropna()\nbarrier_enhanced = barrier_before.loc[bet_confidence.loc[bet_confidence>0.5].index]\n\n\nrts_bm = make_rt(close,barrier_bm)\nrts_before = make_rt(close,barrier_before)\nrts_enhanced = make_rt(close,barrier_enhanced)\n\n\nresult1 = pd.concat([round_trip.get_df_ann_sr(rts_bm,'Benchmark',years=11),\n                    round_trip.get_df_ann_sr(rts_before,'Trading Strategy (Primary)',years=11)],axis=1)\n\ndf_sr = round_trip.get_df_ann_sr(rts_enhanced,'Enhanced Trading Strategy (Second)',years=11)\nresult1 = result1.join(df_sr)\n\n\nresult1\n\n\n\n\n\n  \n    \n      \n      Benchmark\n      Trading Strategy (Primary)\n      Enhanced Trading Strategy (Second)\n    \n  \n  \n    \n      avg_n_bets_per_year\n      246.272727\n      93.545455\n      49.636364\n    \n    \n      win_ratio\n      0.520506\n      0.590467\n      0.612844\n    \n    \n      annualized_sharpe_ratio\n      0.538232\n      1.525995\n      1.623284\n    \n  \n\n\n\n\n\nresult2 = pd.concat([round_trip.get_df_ann_sr(rts_bm,'Benchmark',years=11),\n                    round_trip.get_df_ann_sr(rts_before,'Trading Strategy (Primary)',years=11)],axis=1)\nwinr = []\nfor i in np.linspace(0.1,0.9,9):\n    barrier_enhanced_ = barrier_before.loc[bet_confidence.loc[bet_confidence>=i].index]\n    rts_enhanced_ = make_rt(close,barrier_enhanced_)\n    df_sr = round_trip.get_df_ann_sr(rts_enhanced_,'b',years=11)\n    winr.append(df_sr.T.win_ratio[0])\n\n\ndict_ = dict(zip(np.linspace(0.1,0.9,9).round(2),winr))\n\n\ndf_res = pd.DataFrame.from_dict(dict_,orient='index')\nplt.figure(figsize=(10,5))\nplt.title(\"Hit-ratio of different thresholds strategy\")\nplt.bar(df_res.index, df_res[0], width=0.05)\nplt.plot(df_res)\nplt.ylabel('win ratio')\nplt.xlabel('bet confidence threshold')\nplt.ylim(0.5,0.8)\nplt.show()\n\n\n\n\n매매신뢰도의 전략 강화 결과 win ratio가 상승했으며, 신뢰도의 임계치에 따라 결과가 다른데, 임계치를 높이할수록 결과가 좋다."
  },
  {
    "objectID": "posts/systemic_risk/1_Systemic_Risk.html",
    "href": "posts/systemic_risk/1_Systemic_Risk.html",
    "title": "시스템 리스크 분석: (1) 시스템 리스크 개념",
    "section": "",
    "text": "시스템 리스크\n\n시스템 리스크 (systemic risk): 금융중개기능이 원활히 작동하지 못하여 경제성장과 사회후생에 심각하게 손상을 줄 정도의 심각한 금융불안정을 지칭하는 시스템적 사건이 발생할 위험 - 유럽중앙은행(ECB(2010))\n\n\n\n\nSystemic_Risk\n\n\n금융에서 시스템리스크는 금융시스템 전체 또는 한 지역이나 국가의 전체 시장, 나아가 글로벌 시장의 붕괴로 이어지는 위기를 말한다. 가장 큰 충격이었던 2008년 글로벌 금융위기의 강력한 경제 파괴력과 금융산업을 파괴하는 거대한 연쇄반응은 금융안전과 관련하여 시스템리스크의 중요성을 인식하게 했다.\n전통적인 금융 리스크와 다른 가장 중요한 특징은 내부 및 외부 요인의 지속적인 반응으로 인해 전체 금융 시스템에 걸친 위험 전이 및 시스템 장애의 문제이다. 따라서 시스템 리스크에 따른 결과는 “계단식” 실패로 이어지고 금융 시스템의 개인에게 영향을 미쳐 은행 시스템이 유동성 문제와 지불 불가 상태가 된다. 즉, 시스템 금융 리스크는 글로벌 또는 지역 금융 시스템의 유동성 위험을 유발하는 연쇄적 위기로 설명될 수 있다(Silva, Kimura, & Sobreiro, 2017).\n지난 10년이 넘는 기간 동안 금융 생태계의 연구, 금융 감독, 국경 간 자본 흐름 모니터링 등 시스템적 금융 위험에 초점을 맞춘 많은 양의 연구가 있었다. 그러나 현대 금융은 이미 광범위하고 상호 연결된 네트워크를 갖춘 상호의존적 시스템으로 구성되어 있으며, 이는 점점 더 글로벌해지는 사회의 특성을 따른다.\n따라서 현행 금융의 시스템 리스크에 대응하기 위해서는 비정상적인 위험행위를 자동으로 탐지하고 금융시장의 대규모 금융데이터를 신속하게 처리함으로써 정보기술(IT) 기법을 활용하여 위험 단서와 목적을 신속하게 파악하고 발견할 수 있는 새로운 도구가 개발되어야 한다. 지능적이고 자동화된 기계 학습 방법은 위험 성향과 함께 점점 더 복잡해지는 금융 네트워크, 금융 거래의 빅데이터, 시장 정서 등으로 인한 시스템 위험을 평가하고 탐지하는 도구가 될 수 있다.\n\n\n시스템 리스크의 특징\n시스템 리스크의 특징을 설명한 선행연구\n\nCifuentes(2003): 금융시스템에 집중도(concentration)가 높아질수록 충격이 금융기관들 간에 신속하게 잘 전파되어 시스템 리스크가 높아진다고 주장\nDe Nicolo et al.(2012): 미시적 금융불안정이 시스템리스크로 확산되는 과정에서 부의 외부성(negative externalities)이 중요한 역할을 한다고 강조\nBIS(2010): 시스템리스크를 특정 시점을 기준으로 위험도가 금융시스템에 어떤 모습으로 분포되어 있는지에 주목하는 횡단면 위험과 시스템리스크가 시간에 따라 증가하는지 여부에 주목하는 시계열적 위험으로 구분\n\n소수의 대형 금융기관에 위험이 집중되거나 금융기관들 간에 상호 연계성이 강할수록 시스템리스크는 커짐(횡다면 위험)\n경기호황기의 지나친 신용을 공급은 경기불황기에 나타날 수 있는 금융불안정을 더 심화시킴(시계열적 위험)\n\n\n\n\n시스템 리스크 요인\n시스템 리스크를 축적시키는 요인\n\n투자자, 금융기관의 집단적 협조실패(coordination failure): 은행예금인출 쇄도(bank run), 자산 급매도(asset fire-sale)\n정보 요인: 근시안적 행동(myopia), 정보의 비대칭성, 지나친 위험추구 성향\n유인체계: 금융기관의 도덕적 해이, 단기 위험 추구 성향을 부추기는 보상체계\n규제 및 감독체계 오류: 그림자 금융(shadow banking), 규제자본의 경기순응성\n\n시스템 리스크를 촉발시키는 요인\n\n금융시스템의 외부(exogenous) 또는 내부(endogenous) 충격(shock)\n신용위험이나 유동성 위험과 같은 미시적 성격의 충격 또는 공통요인의 발현과 같은 거시적 충격\n소수의 대형 금융기관의 도산 또는 소규모이지만 다수의 금융기관들이 한꺼번에 도산하는 형태\n\n시스템 리스크의 전염과 확산\n\n금융기관들 간의 다양한 형태의 부내(balance sheet) 또는 부외(off-balance sheet) 직접적 거래관계\n정보 경로: 자산급매도, 쏠림현상(herding), 비이성적 행동 등\n금융기관들의 투자나 위험관리 측면에서의 유사성\n해외은행의 큰 비중 (해외의 외생적 충격)\n금융시스템의 구조적 취약성(structural vulnerabilities)\n\n높은 단기자금조달 의존도\n노출 위험의 높은 유사성\n중앙청산체계가 마련되어 있지 않은 장외거래 시장에 대한 높은 의존도\n큰 비중을 차지하는 그림자 금융\n높은 경기순응성을 보이는 신용공급\n금융기관의 심각한 만기 및 통화의 불일치\n\n\n\n\n참고 문헌\n\nKou, G., Chao, X., Peng, Y., Alsaadi, F. E., & Herrera-Viedma, E. (2019). Machine learning methods for systemic risk analysis in financial sectors. Technological and Economic Development of Economy, 25(5), 716-742.\nSiegmann, A. (2017). Policy Lessons from Systemic Risk Modeling and Measurement. In Systemic Risk Tomography (pp. 239-273). Elsevier.\n서상원. (2018). 시스템리스크의 측정과 관리: 서베이와 제언. 금융안정연구, 19(1), 131-232."
  },
  {
    "objectID": "posts/systemic_risk/2-1_Systemic_Risk_Measures_Macro.html",
    "href": "posts/systemic_risk/2-1_Systemic_Risk_Measures_Macro.html",
    "title": "시스템 리스크 분석: (2-1) 거시경제지표를 활용한 경제전체 위험도 측정 방법",
    "section": "",
    "text": "시스템리스크 측정 방법 중 거시경제지표를 활용한 경제전체 위험도를 살펴본다."
  },
  {
    "objectID": "posts/systemic_risk/2-1_Systemic_Risk_Measures_Macro.html#fcycl-financial-cycle",
    "href": "posts/systemic_risk/2-1_Systemic_Risk_Measures_Macro.html#fcycl-financial-cycle",
    "title": "시스템 리스크 분석: (2-1) 거시경제지표를 활용한 경제전체 위험도 측정 방법",
    "section": "FCYCL: Financial Cycle",
    "text": "FCYCL: Financial Cycle\nDrehmann, Borio and Tsatsaronis(2012)\n실물부문에서 호황과 불황이 교대로 나타나는 경기변동이 있듯이 금융부문에서도 그와 유사한 변동으로 금융변동을 제시\n\n금융사이클 산출을 위한 변수 5개\n\n민간신용\nGDP 대비 신용 비율\n주가\n주택가격\n(주거용, 상업용 부동산 가격, 주가 등) 종합자산 가격\n\nBand-pass 필터: 5개의 금융변수 각각에 대해 특정주기에 대한 정보만을 추출 (단기 사이클 1-8년 / 중기 사이클 8-30년)\n전환점 분석법(turning-point analysis): 사이클의 정점(peak)과 저점(trough)을 특정\n\n단기 사이클\n\n매 분기마다 그 분기를 중심으로 전후 5분기 범위에서 그 분기의 값이 가장 크면 그 분기를 국지적 극대, 가장 작으면 국지적 극소로 정의\n이 국지적 극대와 극소 중에서 각 사이클의 최소 길이가 5분기 보다 길고 확장 또는 수축 국면의 기간이 최소 2분기 이상인 경우를 정점 또는 저점으로 지정\n\n중기 사이클\n\n전후 9분기 범위에서 국지적 극대와 극소를 정하고 사이클의 최소 길이는 8년으로 정한다. 전환점 분석법을 금융사이클 및 개별지 수에 적용하여 각각의 정점과 저점들을 정한 후\n금융사이클의 정점(저점)을 중심으로 개별 정점(저점)들이 6분기 이내에 있으면 정규(regular) 전환점으로, 그리고 6~12분기 이내에 있으면 약(weak) 전환점이라고 정의\n\n\n실험: 1960~2011년 기간동안 7개국에 대해 적용\n\n분석 결과: 금융사이클과 실물 경기변동간에 긴밀한 관계가 있음. 특히, 금융사이클 수축국면에 실물의 경기하락이 겹칠 경우에는 상대적으로 심한 경기하락을 경험. 이러한 분석결과는 금융사이클로 인한 추가적인 경기하락을 방지하고 경기변동을 완화하기 위해서는 금융 사이클을 함께 고려하여야 한다는 점을 시사.\n\n\n\n\nDrehmann, Borio, and Tsatsaronis (2012) Figure 1"
  },
  {
    "objectID": "posts/systemic_risk/2-1_Systemic_Risk_Measures_Macro.html#ccycl-credit-cycle",
    "href": "posts/systemic_risk/2-1_Systemic_Risk_Measures_Macro.html#ccycl-credit-cycle",
    "title": "시스템 리스크 분석: (2-1) 거시경제지표를 활용한 경제전체 위험도 측정 방법",
    "section": "CCYCL: Credit cycle",
    "text": "CCYCL: Credit cycle\nJuselius and Drehmann(2015)\n\nGDP대비 신용 비율이 담고 있는 정보를 활용: 지속가능 부분(sustainable)과 지속가능하지 않은 부분(unsustainable)으로 구분\n\nsustainable: 레버리지와 원리금부담(debt service burden)이 GDP대비 신용 비율과 장기적 관계를 가질 것이라고 가정하고 GDP대비 신용 비율 중에서 이 장기적 관계에 의해 설명되는 부분\n\n\\({slev}_t = constant + \\sum^{b}_{i=1} \\psi_i (p_{A_i,t}-p_t)\\)\n\n\\({slev}_t\\)는 레버리지를 감안한 지속가능한 GDP대비 신용 비율(로그), \\(p_{A_i,t}\\)는 \\(i\\)번째 자산가격(로그), \\(p_t\\)는 물가지수(로그), \\(\\sum^{b}_{i=1} \\psi_i = 1\\) 제약\n\n${sleb}_t = constant + r_t $\n\n\\({sleb}_t\\)는 원리금부담을 감안한 지속가능한 GDP대비 신용 비율(로그), \\(r_t\\)는 신용잔고에 대한 평균 명목대출이자율\n\n두 가지의 장기균형관계를 공적분(co-integration) 방법으로 추정\n실험: 미국 글로벌 금융위기 이전 기간 케이스\n분석 결과:\n\n실제 GDP대비 신용 비율이 레버리지를 감안한 지속가능한 GDP대비 신용 비율에 비해 낮게 나타남 \\(\\rightarrow\\) 높은 레버리지를 가지고 주택시장에 신규진입\n실제 GDP대비 신용 비율은 원리금부담을 감안한 지속가능한 GDP대비 신용 비율에 비해 높아짐 \\(\\rightarrow\\) 원리금부담이 높아지면 소비 및 투자 지출을 감소\n\n\n\n\n\nJuselius & Drehmann (2015) Figure 1"
  },
  {
    "objectID": "posts/systemic_risk/2-1_Systemic_Risk_Measures_Macro.html#ews-se-early-warning-system-signal-extraction",
    "href": "posts/systemic_risk/2-1_Systemic_Risk_Measures_Macro.html#ews-se-early-warning-system-signal-extraction",
    "title": "시스템 리스크 분석: (2-1) 거시경제지표를 활용한 경제전체 위험도 측정 방법",
    "section": "EWS-SE (Early warning system-Signal extraction)",
    "text": "EWS-SE (Early warning system-Signal extraction)\nKaminsky, Lizondo and Reinhart(1998)\n신호접근법(Signal extraction approach)는 여러 정보변수들을 비모수적 방법으로 활용하여 금융위기의 발생가능성을 예측하는 방법\n\n각 국가별로 먼저 위기에 대한 조작적 정의(operational definition)를 하고 그에 기반하여 위기를 나타내면 1을, 아니면 0을 갖는 더미변수 \\(S_{j,t}\\)를 구성 (\\(j\\)는 국가, \\(t\\)는 시간, \\(n\\)은 n번째 정보변수) - 정보변수는 거시경제변수 등\n\n\n\n\n\ncrisis\nno-crisis\n\n\nsignal\n\\(A^j_{n,t}\\)\n\\(B^j_{n,t}\\)\n\n\nno-signal\n\\(C^j_{n,t}\\)\n\\(D^j_{n,t}\\)\n\n\n\n\nin-sample 기간의 경우 합산하여 \\(A^j_{n}, B^j_{n}, C^j_{n}, D^j_{n}\\) 구함\nNSR(Noise-to-signal ratio)\n\n\\(w^j_n = \\frac{B^j_{n}}{B^j_{n}+D^j_{n}} \\div \\frac{A^j_{n}}{A^j_{n}+C^j_{n}}\\)\n예측이 정확할수록 0에 가까움\nNSR이 최소가 되도록 각 정보변수별 임계치를 설정\n\n여러 정보변수 중에서 몇 개가 위기신호가 발생하는지를 각 시점별로 관측\n\n\\(K^j_{1t} \\equiv \\sum^{N}_{n=1}I^j_{nt}\\)\n\\(I^j_{nt}\\)는 n번째 정보변수가 위기신호를 보내는지의 더미변수\n이러한 종합지수들에 대해 NSR이 최소가 되는 종합지수의 임계치를 설정\n\n외표본기간에서도 위기발생에 대해 높은 예측력을 보이는지를 점검"
  },
  {
    "objectID": "posts/systemic_risk/2-1_Systemic_Risk_Measures_Macro.html#ews-lr-early-warning-system-logit-regression",
    "href": "posts/systemic_risk/2-1_Systemic_Risk_Measures_Macro.html#ews-lr-early-warning-system-logit-regression",
    "title": "시스템 리스크 분석: (2-1) 거시경제지표를 활용한 경제전체 위험도 측정 방법",
    "section": "EWS-LR (Early warning system-Logit regression)",
    "text": "EWS-LR (Early warning system-Logit regression)\n\n로짓모형은 위기발생 확률을 직접 제시해줌. 그러나 통상 위기발생 확률을 직접 사용하기 보다는 그 확률을 이용하여 NSR을 구하고 내표본기간의 NSR을 최소화하는 임계치를 설정\n위기발생 확률이 임계치를 상회하는 경우 위기신호가 발생하는 방식으로 로짓모형을 조기경보 목적에 활용"
  },
  {
    "objectID": "posts/systemic_risk/2-1_Systemic_Risk_Measures_Macro.html#ews-hybrid-early-warning-system-hybrid",
    "href": "posts/systemic_risk/2-1_Systemic_Risk_Measures_Macro.html#ews-hybrid-early-warning-system-hybrid",
    "title": "시스템 리스크 분석: (2-1) 거시경제지표를 활용한 경제전체 위험도 측정 방법",
    "section": "EWS-Hybrid (Early warning system-Hybrid)",
    "text": "EWS-Hybrid (Early warning system-Hybrid)\nSuh(2017)\n조기경보모형으로 널리 사용되고 있는 신호접근법과 로짓모형의 각기 장단점을 서로 보완적으로 활용\n\n먼저 여러 정보변수들을 그 경제적 내용에 따라 소수의 하위부문으로 구분\n\n급작스런 자본유입의 중단 현상의 예측에 대해 17개의 개별 정보변수를 4개의 하위부문(거시경제, 금융, 대외부문 및 해외경제)으로 분류\n각 국가간 차이를 나타내는 변수 중에서 시기별로 큰 변동을 보이지 않는 5개의 변수를 따로 국가간 차이 변수로서 추가\n\n4개의 하위부문에 대해 각각 신호접근법을 적용하여 하위종합지수를 구성\n이렇게 구성된 4개의 하위종합지수와 5개의 국가간 차이변수 등 9개의 정보변수를 로짓 예측모형에 적용"
  },
  {
    "objectID": "posts/systemic_risk/2-2_Systemic_Risk_Measures_Market.html",
    "href": "posts/systemic_risk/2-2_Systemic_Risk_Measures_Market.html",
    "title": "시스템 리스크 분석: (2-2) 시장위험 측정 방법",
    "section": "",
    "text": "시스템리스크 측정 방법 중 시장위험을 측정하는 방법들을 살펴본다."
  },
  {
    "objectID": "posts/systemic_risk/2-2_Systemic_Risk_Measures_Market.html#turb-turbulence-index",
    "href": "posts/systemic_risk/2-2_Systemic_Risk_Measures_Market.html#turb-turbulence-index",
    "title": "시스템 리스크 분석: (2-2) 시장위험 측정 방법",
    "section": "TURB: Turbulence index",
    "text": "TURB: Turbulence index\nKritzman & Li (2010)\n금융시장이나 자산시장에서 형성되는 가격의 변동이 통상적인 패턴에서 벗어나는 경우를 금융혼란(financial turbulence)이라고 정의하고 이를 “Mahalanobis distance”라는 측도를 사용하여 측정\n\n혼란지수(turbulence index)는 여러 자산의 수익률들이 자신의 과거 평균치로부터 얼마나 괴리되어 형성되는지를 변동성을 감안하여 평균적으로 측정\n\\(d_t \\equiv (y_t - \\mu)\\sum^{-1}(y_t - \\mu)'\\)\n\n\\(y_t\\)는 자산 수익률 벡터, \\(\\mu\\)는 자산 수익률 표본평균 벡터, \\(\\sum\\)은 자산 수익률의 표본공분산 행렬\n\n분석 결과:\n\n혼란지수로 판단할 때 글로벌 금융위기기간이 1980년대 이후 혼란도가 가장 높았던 시기\n금융 혼란기와 평온기를 비교하면, 금융자산의 수익률-위험 비율은 평온기에 비해 혼란기에서 훨씬 낮게 나타남\n혼란기가 시작되면 대략 1개월 이상의 기간동안 혼란기가 지속되는 특성 관측.\n\n\n\n\n\nKritzman & Li (2010)_Figure 4"
  },
  {
    "objectID": "posts/systemic_risk/2-2_Systemic_Risk_Measures_Market.html#ciss-composite-indicator-of-systemic-stress",
    "href": "posts/systemic_risk/2-2_Systemic_Risk_Measures_Market.html#ciss-composite-indicator-of-systemic-stress",
    "title": "시스템 리스크 분석: (2-2) 시장위험 측정 방법",
    "section": "CISS: Composite Indicator of Systemic Stress",
    "text": "CISS: Composite Indicator of Systemic Stress\nHollo et al.(2012)\n다수의 금융시장 지표들을 종합하여 금융시스템의 위험 상황을 알려주는 종합지표\n\n금융시스템을 단기금융시장, 채권시장, 주식시장, 금융기관 및 외환 시장 등 5개의 하위부문으로 구분, 각 하위부문별로 3개씩의 정보변수들을 고려하여 총 15개의 변수를 이용하여 종합지수를 작성\n\n\n\n\nHollo_et_al._(2012)_Figure 1\n\n\n\n각 하위부문별로 3개씩의 정보변수 각각을 경험 누적분포함수(empirical cumulative distribution function)를 이용하여 [0, 1] 사이의 값을 가지도록 변환, 변환된 변수들을 단순 평균하여 해당 하위부문의 종합지수를 산출\n\\({CISS}_t = (w \\circ s_t)C_t(w \\circ s_t)'\\)\n\n\\(w\\)는 5개의 하위부문에 대한 가중치 벡터 (주관적)\n\\(s_t\\)는 5개의 하위부문 종합지수 벡터\n= Hadamard product (벡터 원소간 곱)\n\\(C_t\\)는 5개의 하위부문 종합지수간 상관계수 행렬\n\nCISS에 마코프 전환모형(Markov switching model)을 적용하여 정상, 경계, 위험 등 3개의 국면을 식별\n\n\n\n\nHollo_et_al._(2012)_Figure 8"
  },
  {
    "objectID": "posts/systemic_risk/2-2_Systemic_Risk_Measures_Market.html#nois-market-noise",
    "href": "posts/systemic_risk/2-2_Systemic_Risk_Measures_Market.html#nois-market-noise",
    "title": "시스템 리스크 분석: (2-2) 시장위험 측정 방법",
    "section": "NOIS: Market noise",
    "text": "NOIS: Market noise\nHu, Pan, and Wang(2013)\n미국 국채시장에서의 일별 거래정보를 통해 시장 잡음(market noise)을 측정\n\n이자율의 term structure에 대한 Svensson(1994) 모형을 이용하여 순간 선도이자율 (instantaneous forward rate) \\(f\\) 를 다음과 같이 표시\n\n\\(f(m,b) = \\beta_0+\\beta_1 exp(-\\frac{m}{\\tau_1})+\\beta_2\\frac{m}{\\tau_1}exp(-\\frac{m}{\\tau_1})+\\beta_3\\frac{m}{\\tau_2}exp(-\\frac{m}{\\tau_2})\\)\n\\(m\\)은 만기\n\\(b = (\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\tau_1, \\tau_2)\\)는 모수 벡터\n\n실제 거래된 미 국채수익률이 추정된 수익률곡선으로부터 평균적으로 얼마나 괴리되어 있는지를 다음과 같이 측정\n\n\\({Noise}_t = \\sqrt{\\frac{1}{N_t}\\sum^{N_t}_{i=1}[y^i_t-y^i(b_t)]^2}\\)\n\\(N_t\\)는 미국 국채 거래건수, \\(y^i_t\\)는 미국 국채의 실현된 수익률, \\(y^i(b_t)\\)는 수익률 곡선에 의해 추정된 수익률\n\n미국 국채시장이 금융시장의 중심적 위치를 차지하기 때문에 미국 국채시장에서의 시장 잡음은 단지 국채시장의 유동성만을 나타내는 것이 아니라 전반적인 금융시장의 유동성 상황을 나타낸다고 주장\n분석 결과: 시장 잡음 지표는 bid-ask spread나 on-the-run premium과 같은 통상적인 시장 유동성 지표보다 위기시에 더욱 민감한 움직임을 보임으로써 금융시장의 유동성 상황을 보다 적절히 반영하는 것으로 평가\n\n\n\n\nHu,Pan, and_Wang(2013)_Figure 2"
  },
  {
    "objectID": "posts/systemic_risk/2-2_Systemic_Risk_Measures_Market.html#pca-ar-pricipal-component-analysis-absorption-ratio",
    "href": "posts/systemic_risk/2-2_Systemic_Risk_Measures_Market.html#pca-ar-pricipal-component-analysis-absorption-ratio",
    "title": "시스템 리스크 분석: (2-2) 시장위험 측정 방법",
    "section": "PCA-AR: Pricipal component analysis-Absorption Ratio",
    "text": "PCA-AR: Pricipal component analysis-Absorption Ratio\nKritzman et al. (2010)\n금융자산 간에 강한 연관성이 형성되면 위기시에 부정적 영향이 보다 신속하고 광범위하게 파급됨\n\n\\(AR=\\frac{\\sum^{n}_{i=1}\\lambda_i}{\\sum^{N}_{i=1}\\lambda_i}\\)\n\n\\(\\lambda\\)는 \\(i\\)번째 eigenvalue (N개의 금융자산의 과거 수익률을 PCA 기법으로 분석했을 때의 주성분 공분산)\n\\(n\\)개 주성분\n\n실험: 미국의 주식시장에 대해 51개 산업별 지수간 동조성이 변화된 모습을 AR을 기준으로 측정 (N=51, n=10)\nAR이 크게 증가한 이후에 미국 주가가 크게 하락한 현상이 나타났으며, 반대로 AR이 낮은 모습을 보인 기간 이후에 주가가 상승함. 또한, AR은 주식시장 뿐만 아니라 주택시장 등을 포함하는 전반적인 금융 및 자산시장의 불안정과 국제 금융위기에 대해 선행성을 지니는 것으로 나타남.\n\n\n\n\nKritzman_et_al._(2010)_Figure 5"
  },
  {
    "objectID": "posts/systemic_risk/2-3_Systemic_Risk_Measures_Cross-sectional.html",
    "href": "posts/systemic_risk/2-3_Systemic_Risk_Measures_Cross-sectional.html",
    "title": "시스템 리스크 분석: (2-3) 금융기관 단면적 위험 측정방법",
    "section": "",
    "text": "시스템리스크 측정 방법 중 다수의 금융기관이 동시에 위험에 처하게 되는 위험을 직접적으로 측정하는 법을 알아본다. 금융기관의 부도사건에 대한 다변수 확률분포에 대한 정보를 추출하고 이를 이용하여 시스템리스크를 측정한다."
  },
  {
    "objectID": "posts/systemic_risk/2-3_Systemic_Risk_Measures_Cross-sectional.html#pcas-principal-component-analysis-based-systemic-risk-indicator",
    "href": "posts/systemic_risk/2-3_Systemic_Risk_Measures_Cross-sectional.html#pcas-principal-component-analysis-based-systemic-risk-indicator",
    "title": "시스템 리스크 분석: (2-3) 금융기관 단면적 위험 측정방법",
    "section": "PCAS : Principal component analysis-based systemic risk indicator",
    "text": "PCAS : Principal component analysis-based systemic risk indicator\nBillio et al.(2012)\n은행, 증권회사, 보험회사 및 헤지펀드 등 주요 금융기관들의 주가 수익률간의 연계성(interconnectedness)을 PCA 기법을 이용하여 측정하는 방법\n\n여러 금융기관들의 주가 수익률이 서로 긴밀하게 연계되어 있을수록 상위 몇개의 주성분으로 (분산을 기준으로 할 때) 전체의 변동을 잘 설명할 수 있게 됨\n\n\\(h_n = \\frac{\\sum^n_{i=1}\\lambda_i}{\\sum^n_{i=1}\\lambda_i}\\)\n\\(\\lambda_i\\)는 금융기관들의 주가 수익률 공분산 행렬의 \\(i\\)번째 특성치\n금융기관간 연계성이 강할수록 \\(h_n\\)이 증가\nKritzman et al.(2011)의 PCA-AR은 주식시장에서의 연계성을 측정한데 반해, Billio et al.(2012)는 금융기관간 연계성을 측정\n\n개별 금융기관이 시스템리스크에 미치는 기여도\n\n\\({PCAS}_{i,n} \\equiv \\frac{1}{2} \\frac{\\sigma^2_i}{\\sigma^2_S} \\frac{\\partial\\sigma^2_S}{\\partial\\sigma^2_i}|_{h_n \\geq H} = \\frac{\\sigma^2_i}{\\sigma^2_S} \\sum^n_{k=1}W^2_{ik}\\lambda_k|_{h_n} \\geq H\\)\n\\(\\sigma^2_i\\)는 주가수익률의 분산으로 측정한 개별 금융기관의 위험\n\\(\\sigma^2_S\\)개별 금융기관 주가수익률의 공분산의 합으로 측정한 금융시스템의 위험도\n\\(W^2_{ik}\\)는 금융기관 주가 수익률 공분산 행렬의 특성벡터로 이루어진 주성분 가중치 행렬 \\(W\\)의 \\((i,k)\\)번째 원소\n\\(H\\)는 금융기관간 연계성에 대한 임계치로서 \\(h_n\\)이 \\(H\\)를 상회할 경우 강한 연계성이 존재하는 것으로 판단\n\n즉, PCAS는 개별 금융기관 위험도의 상대적 증가가 금융시스템의 상대적 위험도 증가에 미치는 영향을 민감도를 이용하여 측정\n\n\n\n\nBillio et al. (2012) Figure 1"
  },
  {
    "objectID": "posts/systemic_risk/2-3_Systemic_Risk_Measures_Cross-sectional.html#covar-co-value-at-risk",
    "href": "posts/systemic_risk/2-3_Systemic_Risk_Measures_Cross-sectional.html#covar-co-value-at-risk",
    "title": "시스템 리스크 분석: (2-3) 금융기관 단면적 위험 측정방법",
    "section": "CoVaR: Co-Value-at-Risk",
    "text": "CoVaR: Co-Value-at-Risk\nAdrian & Brunnermeier (2008)\nVaR(Value-at-Risk)(금융기관의 위험도를 측정하는 대표적인 지표)개념을 활용하여 시스템리스크를 측정\n\n금융기관 \\(i\\)의 \\(q\\%\\) 유의수준에서 \\(VaR\\)\n\n\\(Pr[X^i \\leq {VaR}^i_q] = q\\%\\)\n\\(X^i\\)는 금융기관 \\(i\\)의 손실규모를 나타내는 확률변수\n\n금융기관 (또는 금융기관의 그룹) \\(j\\) 의 \\(CoVaR\\)\n\n\\(Pr[X^j |C(X^i) \\leq {CoVaR}^{j|C(X^i)}_q] = q\\%\\)\n\\(C(X^i)\\)는 금융기관 i가 손실허용한도인 \\({VaR}^i_q\\)를 넘어서는 손실을 보는 사건\n\n금융기관 \\(i\\)로 인한 \\(j\\)의 위험도 변화\n\n\\(\\Delta{CoVaR}^{j|i}_q \\equiv {CoVaR}^{j|X^i={VaR}^i_q}_q - {CoVaR}^{j|X^i={VaR}^i_{50}}_q\\)\n즉, 금융기관 \\(i\\)가 큰 손실을 보는 경우와 평균적인 경우에 금융기관 \\(j\\)의 위험수준의 차이\n\n시스템리스크 관점에서 금융기관 \\(i\\)가 시스템리스크에 미치는 기여도를 측정하기 위해 \\(j\\)를 금융시스템으로 간주\n\n\\(\\Delta{CoVaR}^{system|i}_q \\equiv {CoVaR}^{system|X^i={VaR}^i_q}_q - {CoVaR}^{system|X^i={VaR}^i_{50}}_q\\)\n\n반대로 금융기관을 \\(j\\), 시스템을 \\(i\\)로 하면, 금융기관이 시스템리스크의 변동에 따라 얼마나 영향을 받는지를 측정 (Exposure-CoVaR)/ 금융기관을 \\(i\\), \\(j\\)로 하면 Network-CoVaR\n\n\n\n\nAdrian, Brunnermeier (2008) Figure 4"
  },
  {
    "objectID": "posts/systemic_risk/2-3_Systemic_Risk_Measures_Cross-sectional.html#ses-systemic-expected-shortfall",
    "href": "posts/systemic_risk/2-3_Systemic_Risk_Measures_Cross-sectional.html#ses-systemic-expected-shortfall",
    "title": "시스템 리스크 분석: (2-3) 금융기관 단면적 위험 측정방법",
    "section": "SES: Systemic expected shortfall",
    "text": "SES: Systemic expected shortfall\nAcharya et al. (2010)\n금융 시스템 위기가 발생한 경우에 개별 금융기관의 적정 자본수준대비 자본의 부족규모 예상치를 측정\n\n가정 및 개념\n\n시스템리스크: 시장 주가지수의 하락폭이 하위 5%를 넘는 경우로 전제\nMES: 시스템리스크가 발생하였을 때 해당 금융기관의 주가 수익률 하락폭의 기댓값\n레버리지: ‘주식 시가총액’ 대비 ’장부가 자산 – 장부가 부채 + 주식 시가총액’의 비율로 측정\n\nSES 추정\n\n금융위기 기간에 금융기관별 주가수익률 하락폭을 실현된 SES로 간주하고 이 실현된 SES를 다음과 같이 MES 및 레버리지에 대해 회귀\n\n\\(SES_i = \\alpha+ \\beta MES_i + \\gamma LVG_i + \\delta'DUM\\)\n\\(DUM\\)은 은행, 증권, 보험 등 금융기관의 종류를 나타내는 더미변수 벡터\n\n이 회귀식을 통해 추정된 SES를 이용하여 미국의 100여개 금융기관에 대해 금융기관별 시스템리스크의 순위(rank)를 정함"
  },
  {
    "objectID": "posts/systemic_risk/2-3_Systemic_Risk_Measures_Cross-sectional.html#ces-component-expected-shortfall",
    "href": "posts/systemic_risk/2-3_Systemic_Risk_Measures_Cross-sectional.html#ces-component-expected-shortfall",
    "title": "시스템 리스크 분석: (2-3) 금융기관 단면적 위험 측정방법",
    "section": "CES: Component expected shortfall",
    "text": "CES: Component expected shortfall\nBanulescu and Dumitrescu(2015)\nMES와 해당 금융기관의 비중의 곱\n\n금융시스템 전체의 수익률은 다음과 같이 개별 금융기관의 수익률과 금융기관별 비중의 가중합으로 표현\n\n\\(r_{mt}=\\sum^n_{i=1}w_{it}t_{it}\\)\n\n금융시스템 전체의 수익률이 일정 임계치 C를 하회하면 시스템리스크가 발생한 것으로 정의\n\n조건부 기대손실규모 (ES):\n\\(ES_{mt-1}(C)=-E_{t-1}(r_{mt}|r_{mt}<C)\\)\n\nMES, CES\n\n\\(MES_{it} \\equiv \\frac{\\partial ES_{mt-1}}{\\partial w_{it}} = -E_{t-1}(r_{it}|r_{mt}<C)\\)\n\\(CES_{it} = w_{it}MES_{it} = -w_{it}E_{t-1}(r_{it}|r_{mt}<C)\\)\n즉, \\(ES_{mt-1}= \\sum^n_{i=1}CES_{it}\\)\n\nMES 또는 SRISK 등이 금융기관의 규모를 감안하지 못하는데 반해, 이 CES 지표는 명시적으로 개별 금융기관의 중요도를 감안한다는 차이점이 있으며, SIFI의 식별 등에 유용한 정보를 제공\n\n\n\n\nBanilescu and Dumitrescu (2015) Figure 3"
  },
  {
    "objectID": "posts/systemic_risk/2-3_Systemic_Risk_Measures_Cross-sectional.html#srisk-sytemic-risk",
    "href": "posts/systemic_risk/2-3_Systemic_Risk_Measures_Cross-sectional.html#srisk-sytemic-risk",
    "title": "시스템 리스크 분석: (2-3) 금융기관 단면적 위험 측정방법",
    "section": "SRISK: Sytemic risk",
    "text": "SRISK: Sytemic risk\nBrownlees & Engle (2010)\n금융 시스템 위기가 발생한 경우에 개별 금융기관의 자본 부족규모 예상치를 측정\n\n\\(SRISK_{it}=W_{it}[kLVG_{it}+(1-k)LRMES_{it}-1]\\)\n\n\\(W_{it}\\)는 주식 시가총액\n\\(k\\)는 자산대비 적정 자기자본 비율\n\\(LVG_{it}\\)는 자산대비 자기자본 비율 \\(A_{it}/W_{it}\\)\n\\(LRMES_{it}\\)는 시스템리스크가 발생하였을 때 개별 금융기관의 다기간 주가수익률 하락폭의 기댓값\n\n금융시스템의 불안정 수준: 개별 SRISK를 합산\n\n\\(SRISK_{t} \\equiv \\sum^N_{i=1} max(SRISK_{it},0)\\)\n\n개별 금융기관의 위험수준\n\n\\(SRISK\\%_{it} = \\frac{SRISK_{it}}{SRISK_{t}}\\)\n\nLRMES 측정 방법: GARCH-DCC 모형을 이용\n\n개별 금융기관 주가수익률 \\(r_{it}\\)과 시장수익률 \\(r_{mt}\\) 간의 관계를 GARCH-DCC 모형을 이용하여 구성 \\[\\begin{equation*}\n\\begin{bmatrix}\nr_{it} \\\\\nr_{mt}\n\\end{bmatrix} \\sim\n\\begin{pmatrix}\n  0, & \\begin{bmatrix}\n  \\sigma_{it}^2 & \\rho_{it}\\sigma_{it}\\sigma_{mt} \\\\\n  \\rho_{it}\\sigma_{it}\\sigma_{mt} & \\sigma_{mt}^2\n\\end{bmatrix}\n\\end{pmatrix}\n\\end{equation*}\\]\n여기서 변동성은 GJR-GARCH 모형을 이용하여 다음과 같이 가정\n\n\\(\\sigma_{it} = \\omega_{v_i}+\\alpha_{v_i}r^2_{it-1}+\\gamma_{v_i}r^2_{it-1}\\Gamma_{it-1}+\\beta_{v_i}\\sigma^2_{it-1}\\)\n\n\\(\\sigma_{mt} = \\omega_{v_m}+\\alpha_{v_m}r^2_{mt-1}+\\gamma_{v_m}r^2_{mt-1}\\Gamma_{mt-1}+\\beta_{v_i}\\sigma^2_{mt-1}\\)\n여기서 \\(\\Gamma_{mt-1}\\)은 \\(r_{it-1}\\)이 음(-)인 경우에 1을, 아닌 경우에 0을 가지는 변수\n\n\nDCC 모형을 위해 변동성으로 표준화한 수익률을 \\(\\epsilon_{it}\\equiv \\frac{r_{it}}{\\sigma_{it}}, \\epsilon_{mt}\\equiv \\frac{r_{mt}}{\\sigma_{mt}}\\)로 정의하여 상관계수 행렬을 구하면, \\[\\begin{equation*}\n    R_t \\equiv\n    \\begin{bmatrix}\n     1 & \\rho_{it} \\\\\n     \\rho_{it} & 1\n    \\end{bmatrix} =\n    diag(Q_{it})^{-1/2}Q_{it}diag(Q_{it})^{-1/2}, \\\\\n\\end{equation*}\\] \\[\\begin{equation*}\n    Q_{it} = (1-\\alpha_{C_i}-\\beta_{C_i})S_i+\\alpha_{C_i}\n    \\begin{bmatrix}\n     \\epsilon_{it-1} \\\\\n     \\epsilon_{mt-1}\n    \\end{bmatrix}\n    \\begin{bmatrix}\n     \\epsilon_{it-1} \\\\\n     \\epsilon_{mt-1}\n    \\end{bmatrix}' + \\beta_{C_i}Q_{it-1}\n\\end{equation*}\\]\n\n\\(S_i\\)는 \\(\\epsilon_{it}, \\epsilon_{mt}\\)간의 무조건부 상관계수행렬\n\n2단계 QML 방법을 이용하여 모형 추정, 시뮬레이션 방법을 이용하여 LRMES에 대한 예측치를 산출\nSES 지수는 역사적으로 시스템리스크를 경험한 사례가 있어야 추정이 가능하지만, SRISK는 시스템리스크를 경험한 사례가 없더라도 그 추정이나 예측이 가능하다는 장점이 있음\n\n\n\n\nBrownless and Engle (2010) Figure 1\n\n\n\n\n\nBrownless and Engle (2010) Figure 3"
  },
  {
    "objectID": "posts/systemic_risk/2-3_Systemic_Risk_Measures_Cross-sectional.html#structural-garch",
    "href": "posts/systemic_risk/2-3_Systemic_Risk_Measures_Cross-sectional.html#structural-garch",
    "title": "시스템 리스크 분석: (2-3) 금융기관 단면적 위험 측정방법",
    "section": "Structural GARCH",
    "text": "Structural GARCH\nEngle and Siriwardane(2014)\n\n“레버리지” 효과를 반영할 수 있도록 GARCH 모형을 수정한 Structural GARCH 모형을 제안\n\n금융기관의 자산수익률에 부정적인 충격이 발생하면 이로 인해 비대칭적으로 변동성이 증가할 뿐만 아니라 레버리지(즉, 자본 대비 부채 비율)가 증가하며 이 효과로 인해 자본의 수익률에 대한 변동성이 추가로 증가하기 때문\n\n주요 식\n\n\\(r_{Et}=LM_{t-1}r_{At},\\)\n\\(r_{At}=\\sqrt{h_{At}\\epsilon_{At}}, \\epsilon_{At} \\sim (0,1)\\)\n\\(h_{At}=\\omega + \\alpha \\left( \\frac{r_{Et-1}}{LM_{t-2}} \\right)^2 + \\gamma \\left( \\frac{r_{Et-1}}{LM_{t-2}} \\right)^2 I_{r_{E_{t-1}}}+\\beta h_{At-1},\\)\n\\(LM_{t-1} = \\left[\\Delta_{t-1}^{BSM}\\times g^{BSM}\\left(\\frac{E_{t-1}}{D_{t-1}},1,\\sigma^{f}_{At-1},\\tau\\right)\\times \\frac{E_{t-1}}{D_{t-1}}\\right]^\\psi\\)\n\n변수 설명\n\n\\(r_{Et}\\)는 주식 수익률, \\(r_{A}\\)는 관측불가능한 (시장가치) 자산 수익률\n변동성은 GJR GARCH 과정을 따름\n\\(LM\\)은 레버리지 효과\n\\(\\Delta^{BSM}\\)은 Black-Scholes-Merton 옵션모형에 따른 옵션델타\n\\(g^{BSM}( \\ \\bullet \\ )\\)은 콜옵션가격함수의 역함수\n\n주가의 변동성은 자산변동성 뿐만 아니라 레버리지 효과를 나타내는 \\(LM\\)에도 영향을 받음\n\n\\(vol_t \\left( \\frac{dE_t}{E_t} \\right) = LM_t \\sigma_{At}\\)\n\n레버리지 효과를 도입하면 MES 추정치가 증가하는 효과가 있음\n또한, Engle and Siriwardane(2014)는 새로운 시스템리스크 추정지표로서 예비적 자본규모(precautionary capital)을 제안\n\n예비적 자본규모는 시스템리스크 사건이 발생하더라도 해당 금융기관의 자본이 부족하지 않도록 하는 적정 자본규모와 현재의 자본규모와의 차이\n\n\n\n\n\nEngle, Siriwardane (2014) Figure4A\n\n\n\n\n\nEngle, Siriwardane (2014) Figure6A"
  },
  {
    "objectID": "posts/systemic_risk/2-3_Systemic_Risk_Measures_Cross-sectional.html#cimdo-consistent-multivariate-density-optimizing",
    "href": "posts/systemic_risk/2-3_Systemic_Risk_Measures_Cross-sectional.html#cimdo-consistent-multivariate-density-optimizing",
    "title": "시스템 리스크 분석: (2-3) 금융기관 단면적 위험 측정방법",
    "section": "CIMDO: Consistent multivariate density optimizing",
    "text": "CIMDO: Consistent multivariate density optimizing\nSegoviano & Goodhart (2009)\n개별 금융기관들의 위험 확률(PoD; probability of distress)들로부터 CIMDO(consistent multivariate density optimizing) 기법을 활용하여 개별 금융기관들간의 위험 사건에 대한 결합분포를 구하고 이를 이용하여 여러 금융 불안정 지수들을 제안\n\n설명을 위한 가정:\n\n두 개의 금융기관 X와 Y, 금융기관 주가의 로그 수익률 각각 \\(x,y\\)\n\\(q(x,y)\\)는 두 수익률에 대한 사전(prior) 분포, \\(p(x,y)\\)는 두 수익률에 대한 사후(posterior) 분포\n\n사후 분포는 두 분포의 cross-entropy를 최소화하도록 정함\n\nCIMDO 목적함수: \\(C[p,q]= \\int\\int p(x,y) \\ln \\left[\\frac{p(x,y)}{q(x,y)}\\right]dxdy\\)\n\n\\(x\\)(\\(y\\))가 임계치 \\(d_x\\)(\\(d_y\\))를 하회하면 금융기관 X(Y)에 위험 사건이 발생하는 것으로 가정\n\n데이터로부터 경험적으로 추정한 위험 확률 \\(PoD_x\\)와 사후 분포는 다음과 같은 조건을 만족시켜야 사후 분포가 데이터와 일관성을 유지할 수 있음\n\\(\\int^{d_{x(y)}}_{-\\infty}\\int^{\\infty}_{-\\infty} p(x,y)dxdy=PoD_{x(y)}\\)\n\\(\\int\\int p(x,y)dxdy=1\\)\n\n이러한 조건을 만족시키면서 CIMDO 목적함수를 최소화시키는 사후 결합밀도함수\n\n\\(p(x,y)=q(x,y)\\exp\\{-[1+\\mu+\\lambda_1I_{(-\\infty,d_x)}+\\lambda_2I_{(-\\infty,d_y)}]\\}\\)\n\\(\\lambda_1\\)과 \\(\\lambda_2\\)는 각각 \\(PoD_x\\)와 \\(PoD_y\\)에 대한 일치성 조건에 관한 라그랑지 승수\n\\(\\mu\\)는 확률밀도함수의 면적이 1이라는 조건에 대한 라그랑지 승수\n\n\n\nSegoviano and Goodhart(2009)는 결합확률밀도함수를 이용하여 다양한 금융시스템 불안정 지수를 제시\nJPoD - 금융시스템 내의 모든 금융기관들에서 동시에 위험 사건이 발생할 확률 (세 금융기관 X,Y,Z가 존재하는 경우) - \\(JPoD\\equiv \\int^{d_x}_{-infty}\\int^{d_y}_{-infty}\\int^{d_z}_{-infty}p(x,y,z)dxdydz\\)\n\n\n\nSegoviano and Goodhart (2009) Figure 4\n\n\nBSI(banking stability index) - 적어도 하나의 금융기관이 위험할 때 위험 금융기관 수의 기댓값 (세 금융기관 X,Y,Z가 존재하는 경우) - \\(BSI= \\frac{P(X\\leq d_x)+P(Y\\leq d_y)+P(Z\\leq d_z)}{1-P(X>d_x,Y>d_y,Z>d_z)}\\)\n\n\n\nSegoviano and Goodhart (2009) Figure 5\n\n\nPAO(probability that at least one bank becomes distressed)\n\n특정 금융기관이 위험한 상황일 때 다른 금융기관 중 적어도 하나의 금융기관에서 위험 사건이 발생할 확률\n\n\n\n\nSegoviano and Goodhart (2009) Figure 7\n\n\nDDM(distress dependence matrix)\n\n\\(N\\)개의 금융기관이 존재할 때 \\(N\\times N\\) 행렬로서 그 \\((i,j)\\)번째 원소는 \\(j\\)번째 금융기관이 위험할 때 \\(i\\)번째 금융기관이 위험할 조건부확률로 위험 사건에 대한 금융기관간 연계성에 대한 정보를 나타냄"
  },
  {
    "objectID": "posts/systemic_risk/2-4_Systemic_Risk_Measures_Probability_of_Default.html",
    "href": "posts/systemic_risk/2-4_Systemic_Risk_Measures_Probability_of_Default.html",
    "title": "시스템 리스크 분석: (2-4) 금융기관 부도확률 위험 측정방법",
    "section": "",
    "text": "시스템리스크 측정 방법 중 금융기관의 부도와 같은 위험사건을 명시적으로 정의하는 측정하는 법을 알아본다."
  },
  {
    "objectID": "posts/systemic_risk/2-4_Systemic_Risk_Measures_Probability_of_Default.html#cds-dip-credit-default-swap-distress-insurance-premium",
    "href": "posts/systemic_risk/2-4_Systemic_Risk_Measures_Probability_of_Default.html#cds-dip-credit-default-swap-distress-insurance-premium",
    "title": "시스템 리스크 분석: (2-4) 금융기관 부도확률 위험 측정방법",
    "section": "CDS-DIP : Credit default swap-distress insurance premium",
    "text": "CDS-DIP : Credit default swap-distress insurance premium\nHuang et al.(2009)\n\nDIP(distress insurance premium): CDS 시장가격을 이용하여 금융시스템내에 개별 금융기관들이 발행한 CDS들의 가상 포트폴리오(가중치: 금융기관의 부채규모)에 대해서 금융 위험(financial distress) 사건이 발생할 경우 보호받기 위해서 지불하여야 하는 프리미엄을 산출\nCDS 스프레드:\n\n금융기관 \\(i\\)의 신용사건을 기초자산으로 하는 CDS 스프레드 \\(s_{it}\\)는 해당 금융기관이 위험상황에 처할 경우 발생하는 금융손실의 위험을 보장받을 것을 대가로 ‘보장’ 매입자가 일정기간마다 지불하여야 하는 (원금 대비) 프리미엄 비율\n금융기관의 위험확률이 증가하면 CDS 스프레드도 같이 증가하므로 CDS 스프레드 시장가격은 금융기관의 위험도에 대해 시장에서 평가하는 정보를 가짐\n\n\\(PD_{it} = \\frac{a_{t}s_{it}}{a_t LGD_{it}+b_t s_{it}}\\)\n\\(a_t \\equiv \\int^{t+T}_{t}{e^{-r\\tau}d\\tau}\\)\n\\(b_t \\equiv \\int^{t+T}_{t}{\\tau^{-r\\tau}d\\tau}\\)\n\\(r\\)은 무위험수익률, \\(T\\)는 CDS 계약의 잔존만기, \\(LGD\\)는 부도시손실률(loss given default)을 나타냄\n이렇게 추정된 부도확률은 위험중립 확률(risk-neutral probability)이므로 이를 이용하여 산출한 DIP을 금융상품의 가격 즉, 프리미엄으로 간주\n또한 여러 금융기관간 위험 사건의 종속성을 시변하는 상관계수를 이용하여 측정 (향후 12주 후에 대한 상관계수 예측식)\n\n\\(\\rho_{t,t+12}=c+k_{1}\\rho_{t-12,t}+\\sum^{l}_{i=1}k_{2i}\\rho_{t-i,t-1+1}+\\eta X_t+v_t\\)\n\\(\\rho\\)는 하첨자로 표시된 (일주일 단위의) 두 기간간 평균 자산수익률 상관계수, \\(X_t\\)는 예측변수로 사용되는 금융시장 변수\n\n예측된 상관계수를 이용하여 DIP을 추정\n\n\n\n\nHuang et al. (2009) Figure 1"
  },
  {
    "objectID": "posts/systemic_risk/2-4_Systemic_Risk_Measures_Probability_of_Default.html#cds-corisk-credit-default-swap-corisk",
    "href": "posts/systemic_risk/2-4_Systemic_Risk_Measures_Probability_of_Default.html#cds-corisk-credit-default-swap-corisk",
    "title": "시스템 리스크 분석: (2-4) 금융기관 부도확률 위험 측정방법",
    "section": "CDS-CoRisk: Credit default swap-CoRisk",
    "text": "CDS-CoRisk: Credit default swap-CoRisk\nIMF(2009)\n금융기관간 위험도의 상호연계성을 CDS 스프레드를 이용하여 측정\n\nCoRisk는 quantile regression 기법을 사용하여 한 금융기관의 위험 사건 여부가 다른 금융기관에 미치는 영향을 측정\n\n\\(CDS_{it}=\\alpha_{\\tau}+\\sum^{K}_{k=1}\\beta_{\\tau k}R_{kt}+\\gamma_{\\tau j}CDS_{jt}+\\epsilon_{it}\\)\n\\(\\tau\\)는 백분위수, \\(R\\)은 \\(K\\)개의 위험요인\n위험요인: 일반적인 리스크 프리미엄, 수익률곡선의 기울기, LIBOR 스프레드, 단기자금시장의 유동성 상황, VIX 등\n\nCDS 스프레드가 높을수록 고위험도를 나타내며, 95 percentile을 상회하는 경우를 위험 사건이 발생한 것으로 간주\nCoRisk (금융기관 \\(j\\)에 대한 \\(i\\)의 조건부)\n\n\\(Conditional CoRisk_t(i,j) = 100 \\times (\\frac{CDS(95|j)}{CDS_i(95)}-1 )\\)\n\\(CDS(95|j)=\\alpha_{95}+\\sum^{K}_{k=1}\\beta_{95k}R_{kt}+\\gamma_{95j}CDS_j(95)\\)\nCoRisk는 한 금융기관의 CDS 스프레드의 무조건부 95 percentile과 다른 금융기관에서 위험 사건이 발생한 경우에 대한 조건부 95 percentile간의 상대적 차이를 나타냄\n\n\n\n\n\nIMF(2009) Fig2.5.\n\n\n\n\n\nIMF(2009) Fig2.6."
  },
  {
    "objectID": "posts/systemic_risk/2-4_Systemic_Risk_Measures_Probability_of_Default.html#option-ipod-option-implied-probability-of-default",
    "href": "posts/systemic_risk/2-4_Systemic_Risk_Measures_Probability_of_Default.html#option-ipod-option-implied-probability-of-default",
    "title": "시스템 리스크 분석: (2-4) 금융기관 부도확률 위험 측정방법",
    "section": "Option-iPoD : Option-implied probability of default",
    "text": "Option-iPoD : Option-implied probability of default\nCapuano(2008)\n옵션가격으로부터 부도확률을 추정\n\nMerton(1974)의 구조모형을 이용\n\n금융기관의 (시장가치 기준) 자산을 \\(V\\), 부채를 \\(D\\), 자본을 \\(E\\)라고 하면, 금융기관의 부도사건은 \\(V<D\\)로 정의\n자본은 부채를 먼저 상환하고 남는 부분에 대한 청구권으로서 \\(E=\\max(V-D,0)\\)\n이는 마치 기초자산을 \\(V\\)로, 행사가격을 \\(D\\)로 하는 콜옵션과 동일한 구조\n\n부도확률(PoD):\n\n\\(PoD = \\int^{D}_0 fdV\\)\n\n확률밀도함수 \\(f\\) 는 CIMDO 방법을 이용하여 다음과 같이 추정\n\n\\(\\min \\int^{\\infty}_0 f(V)ln(\\frac{f(V)}{f^0(V)})dV\\)\n사후 (posterior) 밀도함수 \\(f\\)는 사전(prior) 밀도함수 \\(f^0\\) 와의 cross-entropy를 최소화\n\n사후 밀도함수의 조건\n\n자본이 콜옵션 가치로 평가됨\n\n\\(E=\\int^{\\infty}_{D}(V-D)f(V)dV\\)\n\n행사가격이 \\(K\\)인 주식옵션의 가격이 \\(C\\)일 때 이 옵션가격과 일치성의 가짐\n\n\\(C=\\int^{\\infty}_{D+K}(V-D-K)f(V)dV\\)\n\n확률밀도함수의 면적은 1\n\n\\(1=\\int^{\\infty}_{0}f(V)dV\\)\n\n\n사후 밀도함수\n\n\\(f(V) = f^0(V)\\exp [ \\lambda_0-1+\\lambda_1e^{-rT}I_{V>D}(V-D)+\\lambda_2e^{-rT}I_{V>D+K}(V-D-K) ]\\)\n\\(\\lambda_0\\)는 확률밀도함수의 면적이 1이라는 조건에 대한 라그랑지 승수, \\(\\lambda_1\\)는 자본이 콜옵션의 가치로 평가된다는 조건에 대한 라그랑지 승수, \\(\\lambda_2\\)는 주식옵션의 가격이 만족하여야 하는 조건에 대한 라그랑지 승수"
  },
  {
    "objectID": "posts/systemic_risk/2-4_Systemic_Risk_Measures_Probability_of_Default.html#cca-contingent-claim-approach",
    "href": "posts/systemic_risk/2-4_Systemic_Risk_Measures_Probability_of_Default.html#cca-contingent-claim-approach",
    "title": "시스템 리스크 분석: (2-4) 금융기관 부도확률 위험 측정방법",
    "section": "CCA: Contingent claim approach",
    "text": "CCA: Contingent claim approach\nLehar(2005)\n주가에 기업의 부도확률에 대한 정보가 내재되어 있다는 점에 기반하여 주가 정보를 이용하여 내재된 부도확률에 대한 정보를 얻는 방법\n\nMerton(1974)의 콜옵션 구조모형(\\(E=\\max(V-D,0)\\))에서 주식이 일종의 콜옵션과 같은 조건부 증권(contingent claim)으로 봄\n금융기관의 자산 \\(V\\)가 다음과 같이 geometric 브라운 운동으로 따른다고 가정\n\n\\(dV=\\mu Vdt+\\sigma Vdz\\)\n주가 \\(E\\) 는 행사가격이 \\(D_t\\), 기초자산 가격이 \\(V_t\\)인 BSM(Black-Scholes-Merton) 콜옵션가격과 동일\n\n금융기관의 자산에 대한 확률과정의 모수 \\(\\mu, \\sigma\\)를 금융기관의 주가에 대한 정보를 이용하여 추정\n\n다음의 우도함수를 극대화하도록 추정\n\\(L(\\mu,\\sigma)=-\\frac{m-1}{2}\\ln(2\\pi)-\\frac{m-1}{2}\\ln\\sigma^2 - \\sum^{m}_{t=2}\\ln \\hat{V}_t(\\sigma) - \\sum^{m}_{t=2}\\ln N(\\hat{d}_t) - \\frac{1}{2\\sigma^2}\\sum^{m}_{t=2}[\\ln\\frac{\\hat{V}_t(\\sigma)}{\\hat{V}_{t-1}(\\sigma)}]^2\\)\n\\(\\hat{V}_t(\\sigma)\\)는 BSM의 콜옵션가격 공식에서 구해지는 해\n\n금융기관간의 연계성을 다음과 같이 지수가중 이동평균 방법을 이용하여 모형화\n\n금융기관 \\(i, j\\)의 공분산: \\(\\sigma_{ij,t} = \\lambda\\sigma_{ij,t-1}+(1-\\lambda)\\ln(\\frac{V^i_t}{V^i_{t-1}})\\ln(\\frac{V^j_t}{V^j_{t-1}})\\)\n\nLehar(2005)는 시스템리스크 측정지표로서 전체 금융시스템 내에 금융기관중 부실금융기관의 수가 일정 비율 (예를 들어, 10%) 이상일 확률을 나타내는 SIN과 자산기준 부실금융기관의 비중이 일정 비율 이상일 확률을 나타내는 SIV 등을 제시"
  },
  {
    "objectID": "posts/systemic_risk/2-5_Systemic_Risk_Measures_Network.html",
    "href": "posts/systemic_risk/2-5_Systemic_Risk_Measures_Network.html",
    "title": "시스템 리스크 분석: (2-5) 네트워크 측정방법",
    "section": "",
    "text": "금융기관들간의 직접적인 연계성이 존재하는 경우에 시스템리스크 측면에서 어떤 영향을 미치는지 효과적으로 분석한다."
  },
  {
    "objectID": "posts/systemic_risk/2-5_Systemic_Risk_Measures_Network.html#모형-설정",
    "href": "posts/systemic_risk/2-5_Systemic_Risk_Measures_Network.html#모형-설정",
    "title": "시스템 리스크 분석: (2-5) 네트워크 측정방법",
    "section": "모형 설정",
    "text": "모형 설정\n\n금융시스템 내에 \\(n\\)개의 금융기관이 존재하고 금융기관들은 다른 금융기관 또는 민간들과 금융거래를 함\n금융기관 \\(i\\)를 중심으로 생각하면,\n\n자산부문은 민간에 대한 자산 \\(c_i\\)와 다른 금융기관에 대한 자산 \\(\\sum_k \\bar{p}_{ki}\\)로 구성\n부채는 민간에 대한 부채 \\(b_i\\)와 다른 금융기관에 대한 부채 \\(\\sum_j \\bar{p}_{ij}\\)로 구성\n순자산 \\(w_i\\)는 자산과 부채의 차이에 해당\n여기서 \\(\\bar{p}_{ij}\\)는 금융기관 \\(i\\)가 \\(j\\)에게 지불하여야 하는 명목 부채금액, \\(p_i\\)는 금융기관 \\(i\\)의 부채 총계\n\n금융기관 \\(i\\)에 부정적인 충격이 발생하여 자산의 손실이 \\(x_i\\)만큼 발생했을 때,\n\n순자산은 \\(w_i(x_i)=c_i-x_i+\\sum_{j\\neq i}\\bar{p}_{ij}-\\bar{p}_i\\) 로 변동\n순자산 규모가 음(-)이면 금융기관 \\(i\\)는 부도 상황\n\n부도상황에서는 명목 부채중 일부만 상환이 이루어지는데, 통상 기존 부채액에 따른 비율대로(pro rata) 부분상환이 이루어진다고 가정\n실제 지불되는 금액을 정산 벡터(clearing payment vector)라고 함, 하지만 여러 금융기관의 금융거래에 따른 지불 및 수취가 서로 연계되어 있어 정산 벡터를 체계적으로 정하기 어려움\nEisenberg and Noe(2001): 정산 벡터는 외부 충격의 발생으로 인해 금융기관이 도산한 직접적인 효과를 반영할 뿐만 아니라 그 이후에 다른 금융기관이 전염효과로 인해 도산하는 간접적인 효과들도 반영하여 정해짐"
  },
  {
    "objectID": "posts/systemic_risk/2-5_Systemic_Risk_Measures_Network.html#여러-특징-부가",
    "href": "posts/systemic_risk/2-5_Systemic_Risk_Measures_Network.html#여러-특징-부가",
    "title": "시스템 리스크 분석: (2-5) 네트워크 측정방법",
    "section": "여러 특징 부가",
    "text": "여러 특징 부가\n\n금융기관이 부도 상황에 처하게 되면 그 금융기관과의 기존 금융거래에 대한 결제가 원활하게 이루어지지 못하게 되고 청산관련 비용들도 발생\n\nRogers and Veraart(2013)은 회수함수(recovery function)를 도입하여 부도발생시 부도 금융기관이 보유한 자산가치가 하락하는 현상을 반영\n\n주식은 최후순위의 청구권으로서 그 가치가 부도 발생 여부에 따라 차이가 생기는 비선형적인 특성을 보임\n\nGourieroux et al.(2013)은 금융기관간 주식 교차보유(cross-holding)와 같은 청구권의 우선순위 문제를 모형에 반영\n\n부도 위기에 몰린 금융기관은 결제자금용의 유동자산을 마련하기 위해 비유동자산을 시장에 급매(fire sales)하게 되고, 이로 인해 비유동자산의 가격이 하락하게 되는 손실을 그 비유동자산 또는 그와 유사한 비유동자산을 보유한 다른 금융기관들도 함께 보게 됨\n\nCifuentes et al.(2005)는 이러한 자산 급매 경로를 모형에 반영\n\n한 금융기관이 금융시장에서 신뢰성이 하락하게 되면 그 금융기관에 대해 청구권을 가지고 있는 상대 금융기관은 자신의 청구권 가치를 시장가격을 반영하여 조정\n\n시가 평가(mark to market) 관행으로 인한 네트워크 효과 반영"
  },
  {
    "objectID": "posts/systemic_risk/2-5_Systemic_Risk_Measures_Network.html#시스템리스크-측정",
    "href": "posts/systemic_risk/2-5_Systemic_Risk_Measures_Network.html#시스템리스크-측정",
    "title": "시스템 리스크 분석: (2-5) 네트워크 측정방법",
    "section": "시스템리스크 측정",
    "text": "시스템리스크 측정\n\n외부의 충격 발생으로 인한 시스템 손실(systemic loss in value)\n\n\\(L(x)=\\sum^{n}_{i=1}x_i+ \\sum^n_{i=1}(\\bar{p}-p_i(x))\\)\n첫째항은 외부충격 \\(x=(x_1,\\dotsc, x_n)\\)의 직접적 효과를, 둘째 항은 그 간접적 효과를 나타냄\n\n금융기관 \\(i\\)의 \\(j\\)에 대한 취약도(vulnerability)\n\n\\(\\alpha_{ij}\\frac{(c_i-w_i)}{w_j} = \\alpha_{ij}(\\lambda_i-1)\\frac{w_i}{w_j}\\)\n\\(\\lambda_i \\equiv c_i / w_i\\)는 금융기관 \\(i\\) 의 외부 레버리지\n\\(\\alpha_{ij} \\equiv \\bar{p}_{ij}/\\bar{p}_i\\)는 금융기관 \\(i\\)의 네트워크 부채중 \\(j\\)에 대한 상대부채비율\n이 취약도 지표가 1보다 작을 때는 금융기관 \\(i\\)에 외부충격이 발생하여도 금융기관 \\(j\\)는 직접적인 효과를 견딜 수 있음\n\n금융연계성(financial connectivity): 금융기관 \\(i\\)의 부채중에서 네트워크 부채의 비중\n\n\\(\\beta_i \\equiv (\\bar{p}_i-b_i)/\\bar{p}\\)\n\n전염지수(contagion index): 전염지수가 클수록 금융기관 \\(i\\) 에서 발생 하는 외부 충격이 네크워크에 주는 영향이 큼\n\n\\(w_i\\beta_i(\\lambda_i-1)\\)"
  },
  {
    "objectID": "posts/systemic_risk/2-5_Systemic_Risk_Measures_Network.html#네트워크-특성에-대한-요약-정보를-나타내는-지표",
    "href": "posts/systemic_risk/2-5_Systemic_Risk_Measures_Network.html#네트워크-특성에-대한-요약-정보를-나타내는-지표",
    "title": "시스템 리스크 분석: (2-5) 네트워크 측정방법",
    "section": "네트워크 특성에 대한 요약 정보를 나타내는 지표",
    "text": "네트워크 특성에 대한 요약 정보를 나타내는 지표\n\n인접행렬(adjacency matrix)\n\n금융기관 \\(i\\)가 \\(j\\)에 채무가 있다면 이를 (N N) 행렬 B의 (i,j)번째 원소에 1로 (즉, B_{ij}=1), 아니면 0으로 (B_{ij}=0) 표시하는 방식으로 연결관계를 나타내는 행렬\n\n수취관련도 & 지급관련도\n\n수취관련도(in-degree)\n\n한 금융기관이 몇 개의 금융기관으로부터 수취할 청구권을 보유하였는지를 나타냄\n\n지급관련도(out-degree)\n\n몇 개의 금융기관에 지급할 의무가 있는지를 나타냄\n\n\n실제 네트워크에서 수취 또는 지급 관련도는 전형적으로 파레토분포(Pareto distribution)의 꼬리와 같이 power law를 따른다고 알려짐. 이는 소수의 금융기관은 매우 높은 관련도를 보이는 반면, 대다수의 금융기관들은 낮은 관련도를 보인다는 것을 시사함\n\n시스템리스크 측면에서 네트워크 구조가 강하면 개별 금융기관의 위험을 분산시키는데 유리하다는 장점과 한 금융기관에 대한 충격이 다른 금융기관으로 전염되는 경로가 될 수 있다는 단점이 공존함\n이러한 장점과 단점 중 어느 효과가 더 큰지에 대해 실증적으로 분석하기 위하여 중심성 지표들을 산출하고 이를 이용하여 중심성이 강할수록 금융기관의 부도확률이 높아지는지를 경험적으로 분석\nNier et al.(2007)은 네트워크 연계성(connectivity)이 강할수록 충격을 흡수하는 능력과 충격의 전염효과 중 어느 효과가 더 커지는지를 시뮬레이션 방법을 이용하여 분석, 네트워크 연계성이 낮은 수준에서는 연계성이 강해질수록 전염효과가 더 크게 나타났으나, 네트워크 연계성이 높은 수준에서는 연계성이 강해질수록 충격 흡수 효과가 더 크게 나타남"
  },
  {
    "objectID": "posts/systemic_risk/2_Systemic_Risk_Measures.html",
    "href": "posts/systemic_risk/2_Systemic_Risk_Measures.html",
    "title": "시스템 리스크 분석: (2) 시스템 리스크 측정 방법 - 선행연구 요약",
    "section": "",
    "text": "시스템 리스크를 측정하는 기존의 방법에 대해 알아본다. 각 서베이 논문에서 정리한 표를 살펴보자."
  },
  {
    "objectID": "posts/systemic_risk/2_Systemic_Risk_Measures.html#서상원-2018의-서베이",
    "href": "posts/systemic_risk/2_Systemic_Risk_Measures.html#서상원-2018의-서베이",
    "title": "시스템 리스크 분석: (2) 시스템 리스크 측정 방법 - 선행연구 요약",
    "section": "서상원 (2018)의 서베이",
    "text": "서상원 (2018)의 서베이\n\n\n\n카테고리\n이름\n내용\n관련 논문\n\n\n\n\n거시경제\nFCYCL(금융사이클)\n금융부문의 불균형을 나타내는 집계변수를 고려\nDrehmann, Borio and Tsatsaronis(2012)\n\n\n거시경제\nCCYCL(신용사이클)\nGDP대비 신용 비율 중심 분석\nJuselius and Drehmann(2015)\n\n\n거시경제\nEWS-SE(조기경보모형-신호접근법)\n여러 정보변수들을 비모수적 방법으로 활용하여 금융위기의 발생가능성을 예측\nKaminsky, Lizondo and Reinhart(1998)\n\n\n거시경제\nEWS-LR(조기경보모형-로짓모형)\n여러 거시 및 금융부문의 정보변수를 활용하여 미리 예측\n\n\n\n거시경제\nEWS-Hybrid(조기경보모형-혼합모형)\n여러 거시 및 금융부문의 정보변수를 활용하여 미리 예측\nSuh(2017)\n\n\n시장위험\nTURB(시장혼란지수)\n금융시장 에서의 자산가격형성이 통상적인 패턴에서 벗어나는지를 판단\nKritzman & Li (2010)\n\n\n시장위험\nCISS(Composite Indicator of Systemic Stress)\n여러 금융시장에서 형성되는 중요한 정보변수들을 종합하여 현재의 전반적인 금융시장 상황이 극단적인 모습을 보이는지를 요약\nHollo et al.(2012)\n\n\n시장위험\nNOIS(시장 잡음)\n금융시장에서 차익거래 기회가 얼마나 존재하는지를 개별 금융거래 자료를 이용하여 측정\nHu, Pan, and Wang(2013)\n\n\n시장위험\nPCA-AR(absorbtion ratio)\n금융자산 수익률간의 상관성 정도 측정\nKritzman et al. (2010)\n\n\n금융기관 Distress\nPCAS\n금융자산 수익률간의 상관성 정도와 금융시스템에 대한 개별 금융기관의 기여도 측정\nBillio et al.(2012)\n\n\n금융기관 Distress\nCoVaR\n금융시스템의 주가수익률이 일정 임계치 이상으로 크게 하락하는 것을 시스템리스크라고 하고 한 금융기관에서 심각한 금융위험 사건이 발생하였다는 조건의 부과가 시스템리스크 수준을 얼마나 증가시키는지를 측정\nAdrian & Brunnermeier (2008)\n\n\n금융기관 Distress\nSES(Systemic expected shortfall)\n시스템리스크가 발생한 경우 개별 금융기관의 적정 자본수준 대비 자본의 부족규모 예상 치\nAcharya et al. (2010)\n\n\n금융기관 Distress\nCES(Component expected shortfall)\n금융기관별 MES(시스템리스크가 발생하였을 때 특정 금융기관의 주가 수익률 하락폭의 기댓값)에 해당 금융기관의 비중을 곱한 값\nBanulescu and Dumitrescu(2015)\n\n\n금융기관 Distress\nSRISK\n금융 시스템 위기가 발생한 경우에 개별 금융기관의 자본 부족규모 예상치를 측정 / 개별 금융기관 및 금융시스템의 주가 수익률이 GARCH-DCC 모형을 따른다고 가정\nBrownlees & Engle (2010)\n\n\n금융기관 Distress\nStructural GARCH\n통상적인 GARCH 모형을 수정한 Structural GARCH 모형을 제안하고 이를 SRISK 지표에 적용 - ‘레버리지’ 효과를 반영\nEngle and Siriwardane(2014)\n\n\n금융기관 Distress\nJPod(Joint probability of default)*\n금융시스템내의 모든 금융기관들에서 동시에 위험 사건이 발생할 확률\nSegoviano & Goodhart (2009)\n\n\n금융기관 Distress\nBSI(banking stability index)*\n적어도 하나의 금융기관이 위험할 때 위험 금융기관 수의 기댓값\nSegoviano & Goodhart (2009)\n\n\n금융기관 Distress\nPAO(probability that at least one bank becomes distressed)*\n특정 금융기관이 위험한 상황일 때 다른 금융기관 중 적어도 하나의 금융기관에서 위험 사건이 발생할 확률\nSegoviano & Goodhart (2009)\n\n\n금융기관 Distress\nDDM(distress dependence matrix)*\ni번째 금융기관이 위험할 때 j번째 금융기관이 위험에 처할 조건부확률\nSegoviano & Goodhart (2009)\n\n\n금융기관 부도위험\nCDS**-DIP(Distress insurance premium)\n금융시스템내에 개별 금융기관들이 발행한 CDS들의 (부채규모를 가중치로 하는) 가상적인 포트폴리오에 대해서 신용위험 사건 발생시 보호받기 위해서 지불하여야 하는 프리미엄\nHuang et al.(2009)\n\n\n금융기관 부도위험\nCDS-CoRisk\nCDS 스프레드에 대해 CoVaR와 유사한 개념을 적용\nIMF(2009)\n\n\n금융기관 부도위험\nOption-iPoD(Option-implied probability of default)\nCIMDO 기법을 적용하여 개별 금융기관의 부도확률을 추정하는 방법을 제시\nCapuano(2008)\n\n\n금융기관 부도위험\nCCA*** 기반 시스템리스크\nCCA 방법에 기반하여 금융기관의 부도확률에 대해 추정 - 전체 금융시스템 내에 금융기관중 부실금융기관의 비율이 일정 임계치 이상일 확률\nLehar(2005)\n\n\n금융기관 네트워크\nNETW\n금융기관들간의 직접적인 연계성이 존재하는 경우 네트워크 측정\n\n\n\nAgent-based\nABM\n다수의 독립적인 이질적이고 경제주체들(agents)이 실제 상황에서 행하는 것과 유사하게 상호작용을 하고 새로운 환경에 적응(adapt)하면서 경제적 행동을 하도록 모형화하고 이러한 경제행위들의 상호작용의 결과를 분석\nThurner(2011), Bookstaber et al.(2014), Bookstaber and Paddrik(2015)\n\n\n스트레스 테스트\nSTEST\n스트레스 시나리오를 구성하고, 스트레스 시나리오 하에서 금융기관이 어떤 영향을 받을지 추정하고, 그러한 금융부문의 영향으로 인해 거시경제가 어떤 영향을 받을지 추정하는 과정으로 이루어짐 - CCAR(Comprehensive Capital Analysis and Review), DFAST(Dodd-Frank Act Stress Testing)\n\n\n\n스트레스 테스트\nR(reverse)-STEST\n금융기관을 위험에 처하게 하는 시나리오가 어떤 것인지를 찾는 방법\n\n\n\n\n*개별 금융기관들의 위험 확률(PoD)들로부터 CIMDO(consistent multivariate density optimizing) 기법을 활용하여 개별 금융기관들간의 위험 사건에 대한 결합분포를 구하고 이를 이용한 금융안정 지수들 (Segoviano and Goodhart(2009))\n**CDS 스프레드는 해당 금융기관이 위험사건 발생시 금융손실의 위험을 보장받을 것을 대가로 ‘보장’ 매입자가 일정기간마다 ‘보장’ 매도자에게 지불하여야 하는 (원금 대비) 프리미엄 비율 / CDS 스프레드 시장 가격은 금융기관의 위험도에 대해 시장에서 평가하는 정보를 가지고 있으며, 이 CDS 스프레드를 통해 위험사건의 발생확률을 역산할 수 있음\n***CCA(contingent claim approach)는 만기 시점에서 기업의 (시장가치 기준) 자산이 부채를 하회하는 것을 기업의 부도사건으로 정의하면, 자본은 부채를 먼저 상환하고 남는 부분에 대한 청구권으로서 기초자산을 자산가치로 하고 행사가격을 부채가치로 하는 콜옵션과 동일하다는 Merton(1974)의 구조모형에 기반한 부도확률 측정법 (Merton(1974))\n각 측정 방법의 자세한 설명은 아래 글에 나와있다.\n시스템 리스크 분석: (2-1) 거시경제지표를 활용한 경제전체 위험도 측정방법\n시스템 리스크 분석: (2-2) 시장위험 측정방법\n시스템 리스크 분석: (2-3) 금융기관 단면적 위험 측정방법\n시스템 리스크 분석: (2-4) 금융기관 부도확률 위험 측정방법\n시스템 리스크 분석: (2-5) 금융기관 네트워크 측정방법"
  },
  {
    "objectID": "posts/systemic_risk/2_Systemic_Risk_Measures.html#tommasobelluzzosystemicriskgithub-repo의-서베이",
    "href": "posts/systemic_risk/2_Systemic_Risk_Measures.html#tommasobelluzzosystemicriskgithub-repo의-서베이",
    "title": "시스템 리스크 분석: (2) 시스템 리스크 측정 방법 - 선행연구 요약",
    "section": "TommasoBelluzzo/SystemicRisk(Github Repo)의 서베이",
    "text": "TommasoBelluzzo/SystemicRisk(Github Repo)의 서베이\nhttps://github.com/TommasoBelluzzo/SystemicRisk\n\n\n\n카테고리\n이름\n내용\n관련 논문\n\n\n\n\nBUBBLES DETECTION\nBUB (Bubbles Flag), BMPH (Boom Phases Flag), BRPH (Burst Phases Flag)\n\nPhillips et al. (2015), Phillips & Shi (2018), Phillips & Shi (2019)\n\n\nBUBBLES DETECTION\nBC (Bubbling Capitalization), BCP (Bubbling Capitalization Percentage)\n\nBrunnermeier et al. (2020)\n\n\nCOMPONENT MEASURES\nAR (Absorption Ratio)\n금융자산 수익률간의 상관성 정도 측정\nKritzman et al. (2010)\n\n\nCOMPONENT MEASURES\nCATFIN\n\nAllen et al. (2012)\n\n\nCOMPONENT MEASURES\nCS (Correlation Surprise)\n\nKinlaw & Turkington (2012)\n\n\nCOMPONENT MEASURES\nTI (Turbulence Index)\n금융시장 에서의 자산가격형성이 통상적인 패턴에서 벗어나는지를 판단\nKritzman & Li (2010)\n\n\nCOMPONENT MEASURES\nPCA\n금융자산 수익률간의 상관성 정도와 금융시스템에 대한 개별 금융기관의 기여도 측정\n\n\n\nCONNECTEDNESS MEASURES\nDCI (Dynamic Causality Index), CIO (“In & Out” Connections), CIOO (“In & Out - Other” Connections), Network Centralities\n\nBillio et al. (2011)\n\n\nCROSS-ENTROPY MEASURES\nJPod(Joint probability of default)\n금융시스템내의 모든 금융기관들에서 동시에 위험 사건이 발생할 확률\nSegoviano & Goodhart (2009)\n\n\nCROSS-ENTROPY MEASURES\nFSI (Financial Stability Index)\n적어도 하나의 금융기관이 위험할 때 위험 금융기관 수의 기댓값\nSegoviano & Goodhart (2009)\n\n\nCROSS-ENTROPY MEASURES\nPCE(Probability of Cascade Effects)\n특정 금융기관이 위험한 상황일 때 다른 금융기관 중 적어도 하나의 금융기관에서 위험 사건이 발생할 확률\nSegoviano & Goodhart (2009)\n\n\nCROSS-ENTROPY MEASURES\nDiDE(distress dependency)\ni번째 금융기관이 위험할 때 j번째 금융기관이 위험에 처할 조건부확률\nSegoviano & Goodhart (2009)\n\n\nCROSS-ENTROPY MEASURES\nSI (Systemic Importance)\n\n\n\n\nCROSS-ENTROPY MEASURES\nSV (Systemic Vulnerability)\n\n\n\n\nCROSS-ENTROPY MEASURES\nCoJPoDs (Conditional Joint Probabilities of Default)\n\n\n\n\nCROSS-QUANTILOGRAM MEASURES\nFull Cross-Quantilograms, Partial Cross-Quantilograms\n\nHan et al. (2016)\n\n\nCROSS-SECTIONAL MEASURES\nIdiosyncratic Metrics: Beta, Value-at-Risk & Expected Shortfall\n\n\n\n\nCROSS-SECTIONAL MEASURES\nCAViaR (Conditional Autoregressive Value-at-Risk)\n\nWhite et al. (2015)\n\n\nCROSS-SECTIONAL MEASURES\nCoVaR & Delta CoVaR (Conditional Value-at-Risk)\n금융시스템의 주가수익률이 일정 임계치 이상으로 크게 하락하는 것을 시스템리스크라고 하고 한 금융기관에서 심각한 금융위험 사건이 발생하였다는 조건의 부과가 시스템리스크 수준을 얼마나 증가시키는지를 측정\nAdrian & Brunnermeier (2008)\n\n\nCROSS-SECTIONAL MEASURES\nMES (Marginal Expected Shortfall), SES (Systemic Expected Shortfall)\n시스템리스크가 발생한 경우 개별 금융기관의 적정 자본수준 대비 자본의 부족규모 예상 치\nAcharya et al. (2010)\n\n\nCROSS-SECTIONAL MEASURES\nSRISK (Conditional Capital Shortfall Index)\n금융 시스템 위기가 발생한 경우에 개별 금융기관의 자본 부족규모 예상치를 측정 / 개별 금융기관 및 금융시스템의 주가 수익률이 GARCH-DCC 모형을 따른다고 가정\nBrownlees & Engle (2010)\n\n\nDEFAULT MEASURES\nD2C (Distance To Capital)\n\nChan-Lau & Sy (2007)\n\n\nDEFAULT MEASURES\nD2D (Distance To Default)\n\nVassalou & Xing (2004)\n\n\nDEFAULT MEASURES\nDIP (Distress Insurance Premium)\n\nBlack et al. (2012)\n\n\nDEFAULT MEASURES\nSCCA (Systemic Contingent Claims Analysis)\n\nJobst & Gray (2013)\n\n\nLIQUIDITY MEASURES\nILLIQ (Illiquidity Measure)\n\nAmihud (2002)\n\n\nLIQUIDITY MEASURES\nRIS (Roll Implicit Spread)\n\nHasbrouck (2009)\n\n\nLIQUIDITY MEASURES\nClassic Indicators: Hui-Heubel Liquidity Ratio, Turnover Ratio & Variance Ratio\n\nChan-Lau & Sy (2007)\n\n\nREGIME-SWITCHING MEASURES\n2-States Model: High & Low Volatility\n\nBillio et al. (2010)\n\n\nREGIME-SWITCHING MEASURES\n3-States Model: High, Medium & Low Volatility\n\nBillio et al. (2010)\n\n\nREGIME-SWITCHING MEASURES\n4-States Model: High & Low Volatility With Corrections\n\nBillio et al. (2010)\n\n\nREGIME-SWITCHING MEASURES\nAP (Average Probability of High Volatility)\n\nAbdymomunov (2011)\n\n\nREGIME-SWITCHING MEASURES\nJP (Joint Probability of High Volatility)\n\nAbdymomunov (2011)\n\n\nSPILLOVER MEASURES\nSI (Spillover Index), Spillovers From & To, Net Spillovers\n\nDiebold & Yilmaz (2008), Diebold & Yilmaz (2012), Diebold & Yilmaz (2014)\n\n\nTAIL DEPENDENCE MEASURES\nACHI (Average Chi)\n\nBalla et al. (2014)\n\n\nTAIL DEPENDENCE MEASURES\nADR (Asymptotic Dependence Rate)\n\nBalla et al. (2014)\n\n\nTAIL DEPENDENCE MEASURES\nFRM (Financial Risk Meter)\n\nMihoci et al. (2020)"
  }
]