[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "E-mail: jhcho1016@naver.com"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cho’s FinBlog",
    "section": "",
    "text": "Welcome to Cho’s FinBlog.\nCho의 FinBlog에 오신걸 환영합니다.\n\n\n이 블로그는 빅데이터, AI 등을 활용한 금융 및 경제 분야를 공부하며 정리한 짧은 글이나 마저 끝내지 못한 연구 등을 올리는 개인 블로그입니다.\n\n\n모든 블로그 글 보기\n\n이 블로그의 모든 글은 오직 교육용 목적입니다.\nAbout Me"
  },
  {
    "objectID": "listing.html",
    "href": "listing.html",
    "title": "Cho's FinBlog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n머신러닝을 이용한 트레이딩: (0) 트레이딩 개요\n\n\n\ntrading\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n머신러닝을 이용한 트레이딩: (1) 시장 데이터 수집\n\n\n\ntrading\n\n\ndata\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n머신러닝을 이용한 트레이딩: (2) 가격 모멘텀 라벨링\n\n\n\ntrading\n\n\nlabeling\n\n\nmomentum\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n머신러닝을 이용한 트레이딩: (3) 피쳐 생성\n\n\n\ntrading\n\n\nfeature\n\n\nmomentum\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n머신러닝을 이용한 트레이딩: (4) 피쳐 선정\n\n\n\ntrading\n\n\nfeature selection\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n머신러닝을 이용한 트레이딩: (5) 매매 시그널 분류\n\n\n\ntrading\n\n\nsignals\n\n\nclassification\n\n\nmachine learning\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n머신러닝을 이용한 트레이딩: (6) 매매 규칙\n\n\n\ntrading\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n머신러닝을 이용한 트레이딩: (7) 매매 신뢰도 측정과 전략 강화\n\n\n\ntrading\n\n\nenhancing\n\n\nmachine learning\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/ml_trading/0_Introduction.html",
    "href": "posts/ml_trading/0_Introduction.html",
    "title": "머신러닝을 이용한 트레이딩: (0) 트레이딩 개요",
    "section": "",
    "text": "머신러닝을 이용한 트레이딩\n머신러닝을 이용한 모멘텀 예측과 전략 강화\n아래는 구체적인 트레이딩에 머신러닝을 활용하는 구조를 보여준다. 다만, 블로그에서는 이 중 일부만 다루기로 한다.\n\n1. 금융 데이터와 바(bar)\n틱 데이터로부터 시간 기준 혹은 금액 기준 바 형성\n\n\n2. 매수, 매도 시그널 포착\n기술적분석을 이용한 기존 모멘텀 전략 (MA crossover, RSI..)\n추가적인 ML 분류기를 이용한 모멘텀 감지\n\n\n3. 매매 규칙\n위의 매매 시그널로 진입 규칙 설정\n이익실현, 손절, 최대보유기간 등을 고려해 청산 규칙 설정\n과거 시나리오를 바탕으로 각 매매 결과 기록 (뒤의 강화 전략을 위해)\n\n\n4. 전략 강화 ML 모형\n\n4.1. 피쳐 생성 (\\(X\\))\n시장 데이터(가격, 거래량 등)와 기술적 지표\n시장 미시구조적(microstructure) 특징\n거시경제 변수\n자산 펀더멘털\nSNS/뉴스 센티멘트, 분석 컨센서스 등\n\n\n4.2. 머신러닝 모형 최적화\n피쳐 선정\nCross-validation (Purged k-fold)\n하이퍼파라미터(hyperparameter) 튜닝\nAutoML 스태킹 (혹은 각각의 Random forest, Adaboost, SVM, GBM, XGBoost, LSTM 등)\n성능 (accuracy, f1 score, roc-auc)\n\n\n4.3. 결과\n매매 신뢰도 (각각의 매매 시그널에 따라 실제 매매했을 때의 성공 예측 확률)\n\n\n\n5. 트레이딩 결정\n모멘텀 전략으로부터의 매매 시그널에 따라 베팅할지 혹은 패스할지를 결정\n위의 매매 신뢰도를 바탕으로 매매 금액/비중을 고려\n\n\n6. 백테스팅\nCumulative returns, Sharpe ratio, max drawdown, win ratio\n\n\n\n참고 문헌:\n\nAdvances in Financial Machine Learning, Lopez de Prado (2018)\n\n\n\n플로우차트\nSimple version \n\nComplex version"
  },
  {
    "objectID": "posts/ml_trading/1_Get_Market_Data.html",
    "href": "posts/ml_trading/1_Get_Market_Data.html",
    "title": "머신러닝을 이용한 트레이딩: (1) 시장 데이터 수집",
    "section": "",
    "text": "기간: 2005년 7월 27일부터 2022년 1월 17일 까지\n삼성전자의 OHLCV(시가, 고가, 저가, 종가, 거래량)\n투자주체별 거래량: 개인, 외국인 기관\n\n파이썬 라이브러리인 FinanceDataReader와 yfinance을 이용한다. 순매수량 데이터는 대신증권 API로 받은 데이터를 이용한다.\n모든 시장 데이터는 일별 데이터이다.\n\n# lib\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns;sns.set()\n\nimport FinanceDataReader as fdr\nimport yfinance as yf\ndf_ohlc = fdr.DataReader('005930','2005-7-27','2022-1-17').iloc[:,0:4] #가격 수정되어 있음\nvolume_yf = yf.download('005930.KS','2005-7-27','2022-1-17',auto_adjust=True).Volume # 수정거래량\n\ndf_ohlcv = df_ohlc.join(volume_yf).dropna()\ndf = tautil.ohlcv(df_ohlcv)\n\nquantity_ = pd.read_csv('C:data/순매수량.csv')\nquantity_ = quantity_.iloc[:-1,1:5]\nquantity_.columns = ['Date','individual','foreign','institutional']\nquantity_.index = quantity_['Date'].apply(lambda x: pd.to_datetime(str(x), format='%Y%m%d'))\nquantity_.drop(columns='Date',inplace=True)\n\ndf = df.join(quantity_).dropna()\n\n얻은 데이터는 뒤에서 시각화하기로 한다."
  },
  {
    "objectID": "posts/ml_trading/2_Labeling.html",
    "href": "posts/ml_trading/2_Labeling.html",
    "title": "머신러닝을 이용한 트레이딩: (2) 가격 모멘텀 라벨링",
    "section": "",
    "text": "삼성전자 종가를 기준으로 가격의 트렌드와 트렌드 강도(모멘텀)을 측정하여 이를 라벨(label)로 사용한다.\n\nTrend scanning 기법\n\n향후 N 기간동안의 선형 회귀 시 t통계량의 절댓값의 최대값을 사용\n즉, 일마다 향후 주어진 기간 중 가장 강한 트렌드를 예측하고자 하는 그날의 정답 트렌드로 지정\n\n\n이 실험에서는 측정한 트렌드(\\(|\\hat{t}|\\))를 분위 수로 나누어 이산적인 값으로 지정해주었다. 이는 다음에 분류(classification) 작업을 하기 위함이다.\n아래 그래프에서 기간 N을 달리했을 때를 비교한 뒤, 투자자의 트레이딩 기간을 고려해 이를 정해준다.\n\n# lib\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns;sns.set()\n\n# homemade\nfrom features import tautil\nfrom labeling import labeling\n\n\nimport FinanceDataReader as fdr\ndf_ = fdr.DataReader('005930','2010-1-1','2021-6-1')\ndf = tautil.ohlcv(df_)\n\n\nclose =df.close\n\n\nwindows=[60]\n\n\ntrend_scanning_regime_q3 = []\nfor i in windows:\n    trend_scanning_regime_q3.append(labeling.trend_scanning_label(close,window=i,q=3)[0].abs())\n\n\ntrend_scanning_q3 = []\nfor i in windows:\n    trend_scanning_q3.append(labeling.trend_scanning_label(close,window=i,q=3)[0])\n\n\nfor j in range(len(windows)):\n    y = trend_scanning_q3[j][:'2020']\n    i = windows[j]\n    f, (a0, a1) = plt.subplots(2, gridspec_kw={'height_ratios': [5, 1]}, figsize=(15,5))\n    f.suptitle(\"Trend Scanning Labels {}span 3cut\".format(i))\n    a0.plot(close[:'2020'],alpha=0.4)\n    a0.scatter(close[:'2020'].index,close[:'2020'],c=y, cmap='vlag')\n    a1.plot(y.fillna(0))\n    f.show()\n\n\n\n\n\nfor j in range(len(windows)):\n    y = np.sign(trend_scanning_q3[j]-1)+1\n    i = windows[j]\n    f, (a0, a1) = plt.subplots(2, gridspec_kw={'height_ratios': [5, 1]}, figsize=(15,5))\n    f.suptitle(\"Trend Scanning (long position) Labels {}span 4cut\".format(i))\n    a0.plot(close,alpha=0.4)\n    a0.scatter(close.index,close,c=y, cmap='vlag')\n    a1.plot(y.fillna(0))\n    f.savefig(\"c:image/labeling/trend_scanning_long_pos_{}.png\".format(i))\n    f.show()"
  },
  {
    "objectID": "posts/ml_trading/3_Get_Features.html",
    "href": "posts/ml_trading/3_Get_Features.html",
    "title": "머신러닝을 이용한 트레이딩: (3) 피쳐 생성",
    "section": "",
    "text": "앞서 구한 시장 데이터를 이용하여 피쳐(feature)를 생성한다.\n피쳐의 종류는 아래에서 설명한다.\n\n# lib\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns;sns.set()\nplt.style.use('tableau-colorblind10')\n\n# homemade\nfrom features import tautil, sadf, trnd_scan, microstructure_features\n\n\n시장 데이터\n\nmtd_data = pd.read_excel('C:data/mtd_data.xlsx')\ndate = mtd_data.iloc[:,1].rename('Date')\nopen = mtd_data.iloc[:,2].rename('open')\nhigh = mtd_data.iloc[:,3].rename('high')\nlow = mtd_data.iloc[:,4].rename('low')\nclose = mtd_data.iloc[:,5].rename('close')\nvolume = mtd_data.iloc[:,6].rename('volume')\n\n\ndf = pd.DataFrame([open,high,low,close,volume]).T\ndf.index=date\n\n\nquantity_ = pd.read_csv('C:data/순매수량.csv')\nquantity_ = quantity_.iloc[:-1,1:5]\nquantity_.columns = ['Date','individual','foreign','institutional']\nquantity_.index = quantity_['Date'].apply(lambda x: pd.to_datetime(str(x), format='%Y%m%d'))\nquantity_.drop(columns='Date',inplace=True)\ndf = df.join(quantity_).dropna()\n\n\ndf.to_csv(\"C:data/market_samsung.csv\")\ndf\n\n\n\n\n\n  \n    \n      \n      open\n      high\n      low\n      close\n      volume\n      individual\n      foreign\n      institutional\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2005-07-27\n      11040\n      11180\n      10960\n      11020\n      18434300\n      -2543250.0\n      -1411300.0\n      -1210850.0\n    \n    \n      2005-07-28\n      11040\n      11320\n      11040\n      11200\n      23659800\n      -2067600.0\n      3772300.0\n      -1517250.0\n    \n    \n      2005-07-29\n      11320\n      11320\n      11200\n      11300\n      17875500\n      1583050.0\n      796450.0\n      -1843600.0\n    \n    \n      2005-08-01\n      11320\n      11380\n      11220\n      11380\n      16471100\n      -3111550.0\n      1520100.0\n      -2652100.0\n    \n    \n      2005-08-02\n      11400\n      11420\n      11240\n      11360\n      14254000\n      -1567950.0\n      -1895300.0\n      -1310950.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2021-10-08\n      72300\n      72400\n      71500\n      71500\n      14043287\n      2612855.0\n      319900.0\n      -2923031.0\n    \n    \n      2021-10-12\n      70700\n      70900\n      68700\n      69000\n      31001484\n      12155071.0\n      -11004329.0\n      -1421202.0\n    \n    \n      2021-10-13\n      68700\n      69600\n      68300\n      68800\n      24172015\n      2224620.0\n      -5271845.0\n      2725596.0\n    \n    \n      2021-10-14\n      69000\n      69800\n      68800\n      69400\n      19520641\n      2011163.0\n      -3787173.0\n      1696662.0\n    \n    \n      2021-10-15\n      70200\n      71000\n      70000\n      70100\n      18051612\n      822742.0\n      -1376104.0\n      456288.0\n    \n  \n\n3965 rows × 8 columns\n\n\n\n\nfor i in df.columns:\n    plt.figure(figsize=(10,1))\n    plt.title(i)\n    plt.plot(df[i])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n피쳐 변수 (Feature)\n\n14개의 기술적분석 지표 (괄호안 숫자는 각 지표를 구하기 위한 window)\n\nRSI (15)\nWillams’s R (15)\nADX (15)\n\nAROON Indicator (20)\n\nDPO (20)\nMACD Difference (25,10,9)\nMass Index (10, 25)\nTRIX (15)\nATR (10)\nUI (15)\nCMF (20)\nFI (15)\nMFI (15)\nEOM SMA (15)\nVPT\n\n5,10,20일 기간의 가격 수익률\n30일 기간의 가격 변동성(표준편차)\n개인, 기관, 외국인 별 순매수량의 5일, 20일\n트렌드-스캐닝 백워드 t value span (20,60)\n미시구조론 변수 각 20일 이동평균\n\nkyle_lambda\n\namihud_lambda\n\nhasbrouck_lambda\n\nbekker_parkinson_volatility\ncorwin_schultz_estimator\n\n\n\nwindows_TA = [1]\nTA = tautil.get_my_stationary_ta_windows(df_ohlcv,windows_TA).dropna()\n\n\nwindows_mom = [5,10,20]\nwindows_std = [30]\n\nmoms = tautil.mom_std(df,windows_mom, windows_std)\nmoms = moms.iloc[:,:len(windows_mom)+len(windows_std)]\n\n\nquantity_ = pd.read_csv('C:data/순매수량.csv')\nquantity_ = quantity_.iloc[:-1,1:5]\nquantity_.columns = ['Date','individual','foreign','institutional']\nquantity_.index = quantity_['Date'].apply(lambda x: pd.to_datetime(str(x), format='%Y%m%d'))\nquantity_.drop(columns='Date',inplace=True)\n\n\nwindows_ma=[5,20]\nfor i in quantity_.columns:\n    for j in windows_ma:\n        quantity_['{} sma_{}'.format(i,j)] = quantity_[i].rolling(j).mean()\n        quantity_.dropna(inplace=True)\nquantity = quantity_.iloc[:,3:]\n\n\nspans = [20,60]\ntrnd_back = pd.DataFrame()\nfor span in spans:\n    trnd_back['trend_back_scan_{}'.format(span)] = trnd_scan.trend_backward_scanning(df.close, span).t_value\n\n\ndollar_volume = df.volume*df.close\n\n\nclose=df.close\nkyle = microstructure_features.get_bar_based_kyle_lambda(close, df.volume, 20).rename('kyle_lambda')\namihud = microstructure_features.get_bar_based_amihud_lambda(close, dollar_volume, 20).rename('amihud_lambda')\nhasbrouk = microstructure_features.get_bar_based_hasbrouck_lambda(close, dollar_volume,20).rename('hasbrouck_lambda')\nbp_vol = microstructure_features.get_bekker_parkinson_vol(df.high,df.low,20).rename('bekker_parkinson_vol')\ncorsch = microstructure_features.get_corwin_schultz_estimator(df.high,df.low,20).rename('corwin_schultz_estimator')\n\n\nmicrostructure = pd.concat([kyle,amihud,hasbrouk,bp_vol,corsch],axis=1)\n\n\nfeatures = TA.join([moms,quantity,trnd_back,microstructure]).dropna()\nfeatures.to_csv('C:data/features_samsung.csv')\n\n\nfeatures = features['2010':'2020']\nf, axs = plt.subplots(len(features.T),figsize=(10,70),gridspec_kw={'hspace': 1})\nfor i in range(len(features.T)):\n    axs[i].title.set_text(features.columns[i])\n    axs[i].plot(features.iloc[:,i])\nf.savefig('C:image/features.png')"
  },
  {
    "objectID": "posts/ml_trading/4_Feature_Selection.html",
    "href": "posts/ml_trading/4_Feature_Selection.html",
    "title": "머신러닝을 이용한 트레이딩: (4) 피쳐 선정",
    "section": "",
    "text": "Input\n\nLabel: 트렌드 스캐닝 기준 up-trend vs. (down- or no-trend)\n기간 : 2005 - 2010\n피쳐: market data features\n\nModel: 랜덤포레스트\n\n5가지 피쳐 선정 기법 비교: original, mda-kmeans, mda-optics, mda-onc, rfecv, sbfs\n\nOutput\n\n최상의 방법으로 선정한 피쳐 사용"
  },
  {
    "objectID": "posts/ml_trading/4_Feature_Selection.html#클러스터-기반-방법",
    "href": "posts/ml_trading/4_Feature_Selection.html#클러스터-기반-방법",
    "title": "머신러닝을 이용한 트레이딩: (4) 피쳐 선정",
    "section": "클러스터 기반 방법",
    "text": "클러스터 기반 방법\n\nclustering\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_sc = sc.fit_transform(X)\nX_sc = pd.DataFrame(X_sc, index=X.index, columns=X.columns)\n\n\nX_sc=X_sc[:'2020']\n\n\nsilhouette_coefficients = []\nkmeans_kwargs = {\n    \"init\": \"random\",\n    \"n_init\": 10,\n    \"max_iter\": 300,\n    \"random_state\": 42,\n}\n\nfor k in range(2, 30):\n    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n    kmeans.fit(X.T)\n    score = silhouette_score(X.T, kmeans.labels_)\n    silhouette_coefficients.append(score)\n\n\nn_clusters=np.argmin(silhouette_coefficients)+2\nkmeans = KMeans(\n    init=\"random\",\n    n_clusters=n_clusters,\n    n_init=10,\n    max_iter=300,\n    random_state=42)\nkmeans.fit(X_sc.T)\nclusters_kmeans = {i: X_sc.columns[np.where(kmeans.labels_ == i)[0]].tolist() for i in np.unique(kmeans.labels_)}\n\n\noptics = OPTICS(min_cluster_size=2)\noptics.fit(X.T)\nclusters_optics = {i: X_sc.columns[np.where(optics.labels_ == i)[0]].tolist() for i in np.unique(optics.labels_)}\n\n\n# 오래 걸림.\nclusters_onc_dist = cluster.get_feature_clusters(X_sc, dependence_metric= 'distance_correlation')\n\nNo feature/s found with low silhouette score. All features belongs to its respective clusters\n\n\n\n\nmda - selection\n\n#labeling\ntrend_scanning_window = 60\ntrend_scanning_q = 3\nts_out = labeling.trend_scanning_label(market_df.close, window = trend_scanning_window, q = trend_scanning_q)\nmom_label = ts_out[0]\ny = np.sign(mom_label-1)+1 # up-trend vs others\n\n\nraw_X = X_sc.copy()\n\ntmp = raw_X.join(y).dropna()\nX=tmp.iloc[:,:-1]\ny=tmp.iloc[:,-1]\n\n# train & test split\n# use previous data for feature selection\nX = X.loc['2005':'2010']\ny = y.loc['2005':'2010']\n\n\n# CV\nn_cv=4\nt1 = ts_out[1].loc[X.index]\ncv = PKFold(n_cv,t1,0.01)\n\n\nclusters = [clusters_kmeans[i] for i in range(n_clusters)]\nclusters2 = [clusters_optics[i] for i in clusters_optics.keys()]\nclusters3 = clusters_onc_dist\n\n\nclf = RandomForestClassifier(n_estimators=1000,class_weight='balanced')\nmda_cluster = importance.mean_decrease_accuracy(clf,X,y,cv,clustered_subsets=clusters)\nmda_cluster2 = importance.mean_decrease_accuracy(clf,X,y,cv,clustered_subsets=clusters2)\nmda_cluster3 = importance.mean_decrease_accuracy(clf,X,y,cv,clustered_subsets=clusters3)\n\n\nfeatures_mda_kmeans = mda_cluster.loc[mda_cluster['mean'] == mda_cluster['mean'].max()].index\nfeatures_mda_optics = mda_cluster2.loc[mda_cluster2['mean'] == mda_cluster2['mean'].max()].index\nfeatures_mda_onc_dist = mda_cluster3.loc[mda_cluster3['mean'] == mda_cluster3['mean'].max()].index\n\n# 0에서 min 값으로 변경함.\n\n\nnew_X1 = X[features_mda_kmeans]\nnew_X2 = X[features_mda_optics]\nnew_X3 = X[features_mda_onc_dist]"
  },
  {
    "objectID": "posts/ml_trading/4_Feature_Selection.html#비-클러스터링-방법",
    "href": "posts/ml_trading/4_Feature_Selection.html#비-클러스터링-방법",
    "title": "머신러닝을 이용한 트레이딩: (4) 피쳐 선정",
    "section": "비-클러스터링 방법",
    "text": "비-클러스터링 방법\n\nRFECV(Rercursive Feature Elimination with CV)\n\n\n# 오래걸림\n\nrf = RandomForestClassifier(class_weight='balanced')\n\nmin_features_to_select = 2  # Minimum number of features to consider\nrfecv = RFECV(\n    estimator=rf,\n    step=1,\n    cv=cv,\n    scoring=\"accuracy\",\n    min_features_to_select=min_features_to_select,\n)\nnew_X5_ = rfecv.fit_transform(X,y)\n\n\nnew_X5 = pd.DataFrame(new_X5_, index=X.index, columns=rfecv.get_feature_names_out())\n\n\nX_list = [X,new_X1,new_X2,new_X3,new_X5]"
  },
  {
    "objectID": "posts/ml_trading/5_Get_Trading_Signals.html",
    "href": "posts/ml_trading/5_Get_Trading_Signals.html",
    "title": "머신러닝을 이용한 트레이딩: (5) 매매 시그널 분류",
    "section": "",
    "text": "모멘텀 분류기 (Momentum Classifier)\n\ninputs\n\nlabels: trend-scanning labeling (up vs. down & no trend)\nfeatures: market-data selected features\n\nmodels: SVM, Random Forest, Gradient Boosting, LSTM\noutputs\n\nmomentum signals\n\n\n\n# lib\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns;sns.set()\nplt.style.use('tableau-colorblind10')\n\n# different models\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score\n\nfrom features import tautil\nfrom labeling import labeling\nfrom mlutil.pkfold import PKFold\n\n\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n\nget X,y\n\nmarket_df = pd.read_csv('C:data/market_samsung.csv')\nmarket_df = market_df.rename(columns={market_df.columns[0]:'Date'})\nmarket_df.index = pd.to_datetime(market_df.Date)\nmarket_df.drop(columns='Date',inplace=True)\nmarket_df.dropna(inplace=True)\n\nfeature_df = pd.read_csv('C:data/features_samsung.csv')\nfeature_df = feature_df.rename(columns={feature_df.columns[0]:'Date'})\nfeature_df.index = pd.to_datetime(feature_df.Date)\nfeature_df.drop(columns='Date',inplace=True)\nfeature_df.dropna(inplace=True)\n\n\nselected_features = pd.read_csv('C:data/selected_features.csv').columns[1:]\n\n\nfeature = feature_df.dropna()\nfeature = feature[selected_features]\n\nsc = StandardScaler()\nX_sc = sc.fit_transform(feature)\nX_sc = pd.DataFrame(X_sc, index=feature.index, columns=feature.columns)\n\n\nfor i in feature.columns:\n    plt.figure(figsize=(10,1))\n    plt.title(i)\n    plt.plot(feature[i])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#labeling\ntrend_scanning_window = 60\ntrend_scanning_q = 3\nts_out = labeling.trend_scanning_label(market_df['2010':].close, window = trend_scanning_window, q = trend_scanning_q)\nmom_label = ts_out[0]\n\n\ny = np.sign(mom_label-1)+1 # up-trend vs. others(down-trend and no-trend)\n\n\ny_ = y[:'2020']\nclose = market_df.close.loc[y_.index]\nf, (a0, a1) = plt.subplots(2, gridspec_kw={'height_ratios': [5, 1]}, figsize=(15,5))\nf.suptitle(\"Trend Scanning Labels: 1(up-trend), 0(down & no-trend)\")\na0.plot(close,alpha=0.2)\na0.scatter(close.index,close,c=y_, cmap='vlag')\na1.plot(y_.fillna(0.5))\nf.show()\n\n\n\n\n\nraw_X = X_sc.copy()\n\ntmp = raw_X.join(y).dropna()\nX=tmp.iloc[:,:-1]\ny=tmp.iloc[:,-1]\n\n\n\nModels & Models with Hyperparameter tuning\n\n# Cross Validation (purged k-fold)\nn_cv=4\nt1= ts_out[1].loc[X.index]\ncv = PKFold(n_cv,t1,0.01)\n\n\n# Choose model (SVM-rbf)\nC = [0.1, 1, 10]\nparam_grid_rbf = dict(C=C)\nsvc_rbf = SVC(kernel='rbf', probability=True)\ngs_svc_rbf = GridSearchCV(estimator=svc_rbf, param_grid= param_grid_rbf, cv=cv)\ngs_svc_rbf.fit(X,y)\nsvc_best = gs_svc_rbf.best_estimator_\nsvc_best\n\nSVC(C=0.1, probability=True)\n\n\n\nn_estimators = [500,1000]\nmax_depth = [3,5]\nparam_grid_rfc = dict(n_estimators=n_estimators, max_depth=max_depth)\nrfc = RandomForestClassifier(class_weight='balanced')\ngs_rfc = GridSearchCV(estimator=rfc, param_grid= param_grid_rfc, cv=cv)\ngs_rfc.fit(X,y)\nrfc_best = gs_rfc.best_estimator_\nrfc_best\n\nRandomForestClassifier(class_weight='balanced', max_depth=5, n_estimators=1000)\n\n\n\nn_estimators_ab = [200,500]\nlearning_rate = [0.01,0.1]\nparam_grid_abc = dict(n_estimators=n_estimators_ab, learning_rate=learning_rate)\n\nabc=AdaBoostClassifier()\ngs_abc = GridSearchCV(estimator=abc, param_grid= param_grid_abc, cv=cv)\ngs_abc.fit(X,y)\nada_best = gs_abc.best_estimator_\nada_best\n\nAdaBoostClassifier(learning_rate=0.01, n_estimators=200)\n\n\n\nn_estimators_gb = [200,500]\nlearning_rate = [0.01,0.1]\nparam_grid_gbc = dict(n_estimators=n_estimators_gb, learning_rate=learning_rate)\ngbc=GradientBoostingClassifier()\ngs_gbc = GridSearchCV(estimator=gbc, param_grid= param_grid_gbc, cv=cv)\ngs_gbc.fit(X,y)\ngbc_best = gs_gbc.best_estimator_\ngbc_best\n\nGradientBoostingClassifier(n_estimators=200)\n\n\n\nclf_list = [svc_best, rfc_best, ada_best, gbc_best]\nestimators=['SVM_best','RF_best','AdaBoost_best','GradientBoost_best']\nscores_list = []\ny_preds_list = []\ny_probs_list = []\n\n# for ML model prediction\nfor clf in clf_list:\n    y_preds_ = []\n    y_probs_ = []\n\n    for train, test in cv.split(X, y):\n        clf.fit(X.iloc[train], y.iloc[train])\n        y_true = y.iloc[test]\n        y_pred = clf.predict(X.iloc[test])\n        y_probs = clf.predict_proba(X.iloc[test])\n        y_probs = y_probs[:, 1]\n        y_pred_series = pd.Series(y_pred,index=y[test].index)\n        y_probs_series = pd.Series(y_probs,index=y[test].index)\n        y_preds_.append(y_pred_series)\n        y_probs_.append(y_probs_series)\n    \n    \n    y_preds__ = pd.concat([i for i in y_preds_])\n    y_probs__ = pd.concat([i for i in y_probs_])\n    y_true__ = y.loc[y_preds__.index]\n    accs = accuracy_score(y_true__, y_preds__)\n    f1=f1_score(y_true__, y_preds__)\n    roc=roc_auc_score(y_true__, y_probs__)\n    prec=precision_score(y_true__, y_preds__)\n    score = [accs, f1, roc, prec]\n    scores_list.append(score)\n    y_preds_list.append(y_preds__)\n    y_probs_list.append(y_probs__)\n\n\nresults = pd.DataFrame(scores_list, columns=['accuracy','f1 score','roc auc score','precision score'],index=estimators)\nresult_show = results.sort_values('accuracy', ascending=False)\nresult_show\n\n\n\n\n\n  \n    \n      \n      accuracy\n      f1 score\n      roc auc score\n      precision score\n    \n  \n  \n    \n      AdaBoost_best\n      0.563135\n      0.123726\n      0.448611\n      0.412621\n    \n    \n      SVM_best\n      0.510160\n      0.299793\n      0.476269\n      0.380263\n    \n    \n      GradientBoost_best\n      0.493832\n      0.468166\n      0.489565\n      0.421993\n    \n    \n      RF_best\n      0.488389\n      0.503871\n      0.498126\n      0.427718\n    \n  \n\n\n\n\n\ny_probs_df = pd.concat(y_probs_list, axis=1).dropna()\ny_probs_df.columns = estimators\n\ny_probs_df['mean_'] = y_probs_df.mean(axis=1)\n\nmomentum = pd.Series(y_probs_df.mean_,index=y_probs_df.index)\n\n\n\nSelect the model\n\nplt.hist(momentum)[1]\n\narray([0.15405224, 0.22668779, 0.29932334, 0.37195889, 0.44459443,\n       0.51722998, 0.58986553, 0.66250108, 0.73513662, 0.80777217,\n       0.88040772])\n\n\n\n\n\n\nmomentum = momentum.loc['2010':'2020']\n\n\nclose = market_df.close.loc[momentum.index]\nplt.figure(figsize=(10,4))\nplt.plot(close, alpha=0.2)\n#plt.title('Momentum signals')\nplt.scatter(momentum.index, close, c=momentum, s=10,cmap='gray_r',vmin=0,vmax=1)\nplt.colorbar()\nplt.legend(['price','darker = long signals'])\nplt.show()\n\n\n\n\n\nmomentum.rename('signals').to_csv('C:data/momentum_signals.csv')"
  },
  {
    "objectID": "posts/ml_trading/6_Trading_Rules.html",
    "href": "posts/ml_trading/6_Trading_Rules.html",
    "title": "머신러닝을 이용한 트레이딩: (6) 매매 규칙",
    "section": "",
    "text": "Trading rules: 매수 진입만 허용"
  },
  {
    "objectID": "posts/ml_trading/6_Trading_Rules.html#진입-규칙",
    "href": "posts/ml_trading/6_Trading_Rules.html#진입-규칙",
    "title": "머신러닝을 이용한 트레이딩: (6) 매매 규칙",
    "section": "진입 규칙",
    "text": "진입 규칙\n\nMomentum Prediction\n\nsignals = pd.read_csv('C:data/momentum_signals.csv')\nsignals.index = pd.to_datetime(signals['Date'])\nsignals.drop(columns='Date',inplace=True)\n\n\nsignals = signals['signals'].loc['2010':'2020']\n\n\nscaler = normalize\nscaler2 = MinMaxScaler()\nsignals = pd.Series(scaler2.fit_transform(normalize(signals.values.reshape(-1,1),axis=0)).reshape((-1,)), \n                           index=signals.index).rename('signals')\n\n\nplt.hist(signals,bins=50)[2]\nplt.title('Distribution of momentum signals')\nplt.show()\n\n\n\n\n\nthresholds = [0, 0.3]\n\n\nenter_ml_list=[]\nfor h in thresholds:\n    enter_ml_list.append(signals.loc[signals>h].index)\n\n\nfor i in range(len(thresholds)):\n    plt.figure(figsize=(10,3))\n    plt.plot(close, alpha=0.5)\n    plt.title('Enter points ML Prediction (Option {})'.format(i+1))\n    plt.plot(close.loc[enter_ml_list[i]],marker='^',linewidth=0,alpha=0.3)\n    plt.legend(['price','Long signals from momentum classifier'])\n    plt.show()\n\n\n\n\n\n\n\n\n\nTech. Analysis Long/short decision\n\nopen = market_df.open['2010':'2020']\nrsi = tautil.RSIIndicator(open,14).rsi().dropna()\nlong = (rsi>=50) & (rsi<70)\nenter_ta = rsi.loc[long].index\n\n\nplt.figure(figsize=(10,3))\nplt.plot(close, alpha=0.5)\nplt.title('Enter points TA')\nplt.plot(close.loc[enter_ta],marker='^',linewidth=0,alpha=0.3)\nplt.legend(['price','Long signals from rsi'])\nplt.show()\n\n\n\n\n\nenter_list = [enter_ta]\nenter_list.append((enter_ml_list[1]& enter_ta).sort_values().drop_duplicates())\n\n\nfor i in range(len(thresholds)):\n    plt.figure(figsize=(10,3))\n    plt.plot(close, alpha=0.5)\n    plt.title('Enter points (Option {})'.format(i+1))\n    plt.plot(close.loc[enter_list[i]],marker='^',linewidth=0,alpha=0.3)\n    plt.legend(['price','Enter points'])\n    plt.show()\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10,3))\nplt.plot(close, alpha=0.5)\nplt.title('Position Enter points'.format(i+1))\nplt.plot(close.loc[enter_list[1]],marker='^',linewidth=0,alpha=0.3)\nplt.legend(['price','Enter points'])\nplt.show()"
  },
  {
    "objectID": "posts/ml_trading/6_Trading_Rules.html#청산-규칙",
    "href": "posts/ml_trading/6_Trading_Rules.html#청산-규칙",
    "title": "머신러닝을 이용한 트레이딩: (6) 매매 규칙",
    "section": "청산 규칙",
    "text": "청산 규칙\n\n# no Rule (benchmark)\npt_sl_bm = [1000,1000]\nmax_holding_bm = [1, 0]\nno_exit_rule = [pt_sl_bm,max_holding_bm]\n\n\n#dynamic target rule\nmax_holding = [60, 0]\nclose_ = market_df.close['2009':'2020']\nchanges = close_.pct_change(1).to_frame()\nfor i in range(2,max_holding[0]+1):\n    changes = changes.join(close_.pct_change(i).rename('close {}'.format(i)))\ndynamic_target = changes.abs().dropna().mean(axis=1)['2010':]\n\n\nbarrier_exit_list=[]\nbarrier_exit_list.append(get_barrier(close, enter_list[1], [1,1], max_holding, target = dynamic_target))  #dynamic  \n\nrts_exit_list=[]\nfor i in range(len(barrier_exit_list)):\n    rts_exit_list.append(make_rt(close,barrier_exit_list[i].dropna()))\n\n\nplt.figure(figsize=(10,2))\nplt.title('Dynamic exit rule returns')\nplt.plot(barrier_exit_list[0].ret)\nplt.legend(['Returns of dynamic exit target rate'])\n\n<matplotlib.legend.Legend at 0x1fe01fa7148>"
  },
  {
    "objectID": "posts/ml_trading/6_Trading_Rules.html#벤치마크",
    "href": "posts/ml_trading/6_Trading_Rules.html#벤치마크",
    "title": "머신러닝을 이용한 트레이딩: (6) 매매 규칙",
    "section": "벤치마크",
    "text": "벤치마크\n\n매번 매매시\n\n\nbarrier_bm = get_barrier(close, close.index, no_exit_rule[0], no_exit_rule[1]) #no rule\n\n\nrts_bm = make_rt(close,barrier_bm.dropna())\n\n\nround_trip.get_df_ann_sr(rts_bm,'Benchmark',years=11)\n\n\n\n\n\n  \n    \n      \n      Benchmark\n    \n  \n  \n    \n      avg_n_bets_per_year\n      243.272727\n    \n    \n      win_ratio\n      0.518835\n    \n    \n      annualized_sharpe_ratio\n      0.537080\n    \n  \n\n\n\n\n\nresult_df = pd.concat([round_trip.get_df_ann_sr(rts_bm,'No Rule')], axis=1)\nfor i in range(len(rts_exit_list)):\n    result_df = result_df.join(round_trip.get_df_ann_sr(rts_exit_list[i],'Enter & Exit Rule'))\n    \nresult_df\n\n\n\n\n\n  \n    \n      \n      No Rule\n      Enter & Exit Rule\n    \n  \n  \n    \n      avg_n_bets_per_year\n      243.272727\n      105.000000\n    \n    \n      win_ratio\n      0.518835\n      0.590988\n    \n    \n      annualized_sharpe_ratio\n      0.537080\n      1.800316"
  },
  {
    "objectID": "posts/ml_trading/6_Trading_Rules.html#최적-청산-규칙-파라미터",
    "href": "posts/ml_trading/6_Trading_Rules.html#최적-청산-규칙-파라미터",
    "title": "머신러닝을 이용한 트레이딩: (6) 매매 규칙",
    "section": "최적 청산 규칙 파라미터",
    "text": "최적 청산 규칙 파라미터\n\n#dynamic target rule\n# different maximum holding\nclose_ = market_df.close['2009':'2020']\nrolling = np.arange(20,260,10)\nmhs = [20,60,120,260]\nwin_ratios = pd.DataFrame()\n\nfor mh in mhs:\n    max_holding = [mh, 0]\n    dynamic_targets = []\n    for j in rolling:\n        for i in range(2,j+1):\n            changes = close_.pct_change(1).to_frame()\n            changes = changes.join(close_.pct_change(i).rename('close {}'.format(i)))\n        dynamic_target = changes.abs().dropna().mean(axis=1)['2010':]\n        dynamic_targets.append(dynamic_target)\n\n    barrier_exit_list_rolling=[]\n    for i in range(len(dynamic_targets)):\n        barrier_exit_list_rolling.append(get_barrier(close, enter_list[1], [1,1], max_holding, target = dynamic_targets[i]))  #dynamic  \n\n    rts_exit_list=[]\n    for i in range(len(barrier_exit_list_rolling)):\n        rts_exit_list.append(make_rt(close,barrier_exit_list_rolling[i].dropna()))\n\n    result_df = pd.concat([round_trip.get_df_ann_sr(rts_bm,'No Rule')], axis=1)\n    for i in range(len(rts_exit_list)):\n        result_df = result_df.join(round_trip.get_df_ann_sr(rts_exit_list[i],'{}'.format(rolling[i])))\n\n    win_ratios['Max. holding {} days'.format(mh)] = result_df.T.win_ratio\nwin_ratios\n\n\n\n\n\n  \n    \n      \n      Max. holding 20 days\n      Max. holding 60 days\n      Max. holding 120 days\n      Max. holding 260 days\n    \n  \n  \n    \n      No Rule\n      0.518835\n      0.518835\n      0.518835\n      0.518835\n    \n    \n      20\n      0.541054\n      0.576857\n      0.572169\n      0.575130\n    \n    \n      30\n      0.564014\n      0.573898\n      0.570069\n      0.570441\n    \n    \n      40\n      0.532872\n      0.559689\n      0.553155\n      0.550562\n    \n    \n      50\n      0.555363\n      0.588591\n      0.589455\n      0.587727\n    \n    \n      60\n      0.564991\n      0.605195\n      0.605195\n      0.603463\n    \n    \n      70\n      0.562392\n      0.605195\n      0.600866\n      0.595671\n    \n    \n      80\n      0.557192\n      0.600000\n      0.584416\n      0.586147\n    \n    \n      90\n      0.555459\n      0.599134\n      0.598787\n      0.601732\n    \n    \n      100\n      0.563258\n      0.587522\n      0.600520\n      0.599653\n    \n    \n      110\n      0.555459\n      0.575022\n      0.595486\n      0.604510\n    \n    \n      120\n      0.561525\n      0.585281\n      0.611785\n      0.620451\n    \n    \n      130\n      0.560659\n      0.593560\n      0.608014\n      0.620209\n    \n    \n      140\n      0.559792\n      0.601739\n      0.607485\n      0.623151\n    \n    \n      150\n      0.555844\n      0.579496\n      0.578261\n      0.597391\n    \n    \n      160\n      0.545927\n      0.574413\n      0.581010\n      0.600174\n    \n    \n      170\n      0.558925\n      0.598432\n      0.598082\n      0.625981\n    \n    \n      180\n      0.551127\n      0.586297\n      0.594266\n      0.615451\n    \n    \n      190\n      0.548918\n      0.594805\n      0.601386\n      0.629116\n    \n    \n      200\n      0.548918\n      0.591342\n      0.606586\n      0.623050\n    \n    \n      210\n      0.535065\n      0.596886\n      0.612987\n      0.627706\n    \n    \n      220\n      0.555844\n      0.614187\n      0.628571\n      0.643290\n    \n    \n      230\n      0.552768\n      0.595506\n      0.605195\n      0.624567\n    \n    \n      240\n      0.561525\n      0.618387\n      0.629887\n      0.647569\n    \n    \n      250\n      0.559792\n      0.609714\n      0.625543\n      0.644097\n    \n  \n\n\n\n\n\nplt.figure(figsize=(15,6))\nplt.title(\"Exit rules\")\nplt.plot(win_ratios)\nplt.legend(win_ratios)\nplt.ylabel('win ratio')\nplt.xlabel('Rolling days for calculating target rate')\nplt.show()"
  },
  {
    "objectID": "posts/ml_trading/7_Enhancing_and_Bet_Confidence.html",
    "href": "posts/ml_trading/7_Enhancing_and_Bet_Confidence.html",
    "title": "머신러닝을 이용한 트레이딩: (7) 매매 신뢰도 측정과 전략 강화",
    "section": "",
    "text": "전략 강화 모형\n\ninputs\n\n라벨: 매매 규칙에 따른 전략의 결과 = 각 매매 성공/실패 여부\n\nmodels: SVM, Random Forest, Gradient Boosting, LSTM\noutputs\n\n매매 신뢰도 (bet confidence)\n\n매매 신뢰도를 이용한 전략 강화\n\n\n# lib\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns;sns.set()\nplt.style.use('tableau-colorblind10')\n\n# different models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, normalize\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score\n\n# homemade\nfrom feature_engineering import dimension_reduction as DR\nfrom features import tautil\nfrom labeling import labeling\nfrom backtest import round_trip\nfrom triple_barrier import make_rt\n\nfrom mlutil.pkfold import PKFold\n\n\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n\nget X,y\n\nmarket_df = pd.read_csv('C:data/market_samsung.csv')\nmarket_df = market_df.rename(columns={market_df.columns[0]:'Date'})\nmarket_df.index = pd.to_datetime(market_df.Date)\nmarket_df.drop(columns='Date',inplace=True)\nmarket_df.dropna(inplace=True)\nclose = market_df.close['2010':'2020']\n\nfeature_df = pd.read_csv('C:data/features_samsung.csv')\nfeature_df = feature_df.rename(columns={feature_df.columns[0]:'Date'})\nfeature_df.index = pd.to_datetime(feature_df.Date)\nfeature_df.drop(columns='Date',inplace=True)\nfeature_df.dropna(inplace=True)\n\nselected_features = pd.read_csv('C:data/selected_features.csv').columns[1:]\n\n\nfeature = feature_df.dropna()\nfeature = feature[selected_features]\nsc = StandardScaler()\nX_sc = sc.fit_transform(feature)\nX_sc = pd.DataFrame(X_sc, index=feature.index, columns=feature.columns)\n\n\n#benchmark\nbarrier_bm = pd.read_csv('C:data/barrier_bm.csv')\nbarrier_bm.index = pd.to_datetime(barrier_bm.Date)\nbarrier_bm.exit = pd.to_datetime(barrier_bm.exit)\nbarrier_bm.drop(columns='Date',inplace=True)\n\n\n#labeling\nbarrier = pd.read_csv('C:data/barrier.csv')\nbarrier.index = pd.to_datetime(barrier.Date)\nbarrier.exit = pd.to_datetime(barrier.exit)\nbarrier.drop(columns='Date',inplace=True)\n\nrts = make_rt(close,barrier.dropna())\noutcome = rts.rt_returns\noutcome.index = rts.open_dt\n\n\n#meta-label\nwl = np.sign(np.sign(outcome)+1)\ny_ = wl\ny_.value_counts()\n\n1.0    608\n0.0    421\nName: rt_returns, dtype: int64\n\n\n\nloss = wl.value_counts()[0]\nwin = wl.value_counts()[1]\nplt.figure(figsize=(10,3))\nplt.scatter(wl[wl==1].index,close.loc[wl[wl==1].index], alpha=0.5)\nplt.scatter(wl[wl==0].index,close.loc[wl[wl==0].index], marker='x', alpha=0.5)\nplt.legend(['win 1','lose 0'])\nplt.title('y (meta-label): win {}, lose {}'.format(win,loss))\nplt.show()\n\n\n\n\n\nraw_X = X_sc.copy()\ntmp = raw_X.join(y_).dropna()\nX=tmp.iloc[:,:-1]\ny=tmp.iloc[:,-1]\n\n\n\nModel Construction\n\n# Choose model\n\n# Cross Validation (k-fold)\nn_cv=4\nt1 = pd.to_datetime(barrier.exit.loc[X.index])\ncv = PKFold(n_cv,t1,0)\n\n\n# Choose model (SVM-rbf)\nC = [0.1, 1,10]\nparam_grid_rbf = dict(C=C)\nsvc_rbf = SVC(kernel='rbf', probability=True)\ngs_svc_rbf = GridSearchCV(estimator=svc_rbf, param_grid= param_grid_rbf, cv=cv, scoring='precision')\ngs_svc_rbf.fit(X,y)\nsvc_best = gs_svc_rbf.best_estimator_\nsvc_best\n\nSVC(C=10, probability=True)\n\n\n\nn_estimators = [200,1000]\n#max_depth = [3,7]\nparam_grid_rfc = dict(n_estimators=n_estimators)\nrfc = RandomForestClassifier()\ngs_rfc = GridSearchCV(estimator=rfc, param_grid= param_grid_rfc, cv=cv, scoring='precision')\ngs_rfc.fit(X,y)\nrfc_best = gs_rfc.best_estimator_\nrfc_best\n\nRandomForestClassifier(n_estimators=200)\n\n\n\nn_estimators_ab = [50,100]\nlearning_rate = [1,0.1]\nparam_grid_abc = dict(n_estimators=n_estimators_ab, learning_rate=learning_rate)\n\nabc=AdaBoostClassifier()\ngs_abc = GridSearchCV(estimator=abc, param_grid= param_grid_abc, cv=cv, scoring='precision')\ngs_abc.fit(X,y)\nada_best = gs_abc.best_estimator_\nada_best\n\nAdaBoostClassifier(learning_rate=1, n_estimators=100)\n\n\n\nn_estimators_gb = [100,200]\nlearning_rate = [0.1,0.01]\nparam_grid_gbc = dict(n_estimators=n_estimators_gb, learning_rate=learning_rate)\ngbc=GradientBoostingClassifier()\ngs_gbc = GridSearchCV(estimator=gbc, param_grid= param_grid_gbc, cv=cv, scoring='precision')\ngs_gbc.fit(X,y)\ngbc_best = gs_gbc.best_estimator_\ngbc_best\n\nGradientBoostingClassifier(learning_rate=0.01, n_estimators=200)\n\n\n\n\nModel\n\nclf_list = [svc_best, rfc_best, ada_best, gbc_best]\nestimators=['SVM_best','RF_best','AdaBoost_best','GradientBoost_best']\nscores_list = []\ny_preds_list = []\ny_probs_list = []\n\n# for ML model prediction\nfor clf in clf_list:\n    y_preds_ = []\n    y_probs_ = []\n\n    for train, test in cv.split(X, y):\n        clf.fit(X.iloc[train], y.iloc[train])\n        y_true = y.iloc[test]\n        y_pred = clf.predict(X.iloc[test])\n        y_probs = clf.predict_proba(X.iloc[test])\n        y_probs = y_probs[:, 1]\n        y_pred_series = pd.Series(y_pred,index=y[test].index)\n        y_probs_series = pd.Series(y_probs,index=y[test].index)\n        y_preds_.append(y_pred_series)\n        y_probs_.append(y_probs_series)\n    \n    \n    y_preds__ = pd.concat([i for i in y_preds_])\n    y_probs__ = pd.concat([i for i in y_probs_])\n    y_true__ = y.loc[y_preds__.index]\n    accs = accuracy_score(y_true__, y_preds__)\n    f1=f1_score(y_true__, y_preds__)\n    roc=roc_auc_score(y_true__, y_probs__)\n    prec=precision_score(y_true__, y_preds__)\n    score = [accs, f1, roc, prec]\n    scores_list.append(score)\n    y_preds_list.append(y_preds__)\n    y_probs_list.append(y_probs__)\n\n\nresults = pd.DataFrame(scores_list, columns=['accuracy','f1 score','roc auc score','precision score'],index=estimators)\nresult_show = results.sort_values('precision score', ascending=False)\n\n\nresult_show\n\n\n\n\n\n  \n    \n      \n      accuracy\n      f1 score\n      roc auc score\n      precision score\n    \n  \n  \n    \n      AdaBoost_best\n      0.567541\n      0.631927\n      0.552471\n      0.635607\n    \n    \n      SVM_best\n      0.544218\n      0.585323\n      0.574228\n      0.632887\n    \n    \n      RF_best\n      0.549077\n      0.657817\n      0.537073\n      0.596257\n    \n    \n      GradientBoost_best\n      0.519922\n      0.609177\n      0.490364\n      0.586890\n    \n  \n\n\n\n\n\ny_probs_df = pd.DataFrame()\nfor i in range(len(estimators)):\n    y_probs_df[estimators[i]] = y_probs_list[i]\n\n\n#평균\npred_prob = pd.Series(y_probs_df.mean(axis=1),index=y_probs_df.index)\n\n#하나하나\n\n#y_probs_df_2 = y_probs_df[estimators[3]]\n#pred_prob = pd.Series(y_probs_df_2,index=y_probs_df_2.index)\n\n\npred_prob2=pd.Series(normalize(pred_prob.to_frame().T).reshape(-1,), index=y_probs_df.index).rename('bet_confidence')\n\n\nbet_confidence=pd.Series(MinMaxScaler().fit_transform(pred_prob2.to_frame()).reshape(-1,), index=y_probs_df.index).rename('bet_confidence')\n\n\nplt.title('Bet confidence distribution')\nplt.hist(bet_confidence, bins=30)[2]\nplt.xlabel('Bet confidence')\nplt.ylabel('counts')\n\nText(0, 0.5, 'counts')\n\n\n\n\n\n\nc = close.loc[bet_confidence.index]\nplt.figure(figsize=(10,5))\nplt.title('Bet confidence')\nplt.plot(close, alpha=0.1)\nplt.scatter(c.index,c, c = bet_confidence, s=20,cmap='vlag',vmin=0,vmax=1)\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\nAlgo Trading Backtest\n\nbarrier_bm = barrier_bm.dropna()\nbarrier_before = barrier.loc[bet_confidence.index].dropna()\nbarrier_enhanced = barrier_before.loc[bet_confidence.loc[bet_confidence>0.5].index]\n\n\nrts_bm = make_rt(close,barrier_bm)\nrts_before = make_rt(close,barrier_before)\nrts_enhanced = make_rt(close,barrier_enhanced)\n\n\nresult1 = pd.concat([round_trip.get_df_ann_sr(rts_bm,'Benchmark',years=11),\n                    round_trip.get_df_ann_sr(rts_before,'Trading Strategy (Primary)',years=11)],axis=1)\n\ndf_sr = round_trip.get_df_ann_sr(rts_enhanced,'Enhanced Trading Strategy (Second)',years=11)\nresult1 = result1.join(df_sr)\n\n\nresult1\n\n\n\n\n\n  \n    \n      \n      Benchmark\n      Trading Strategy (Primary)\n      Enhanced Trading Strategy (Second)\n    \n  \n  \n    \n      avg_n_bets_per_year\n      246.272727\n      93.545455\n      49.636364\n    \n    \n      win_ratio\n      0.520506\n      0.590467\n      0.612844\n    \n    \n      annualized_sharpe_ratio\n      0.538232\n      1.525995\n      1.623284\n    \n  \n\n\n\n\n\nresult2 = pd.concat([round_trip.get_df_ann_sr(rts_bm,'Benchmark',years=11),\n                    round_trip.get_df_ann_sr(rts_before,'Trading Strategy (Primary)',years=11)],axis=1)\nwinr = []\nfor i in np.linspace(0.1,0.9,9):\n    barrier_enhanced_ = barrier_before.loc[bet_confidence.loc[bet_confidence>=i].index]\n    rts_enhanced_ = make_rt(close,barrier_enhanced_)\n    df_sr = round_trip.get_df_ann_sr(rts_enhanced_,'b',years=11)\n    winr.append(df_sr.T.win_ratio[0])\n\n\ndict_ = dict(zip(np.linspace(0.1,0.9,9).round(2),winr))\n\n\ndf_res = pd.DataFrame.from_dict(dict_,orient='index')\nplt.figure(figsize=(10,5))\nplt.title(\"Hit-ratio of different thresholds strategy\")\nplt.bar(df_res.index, df_res[0], width=0.05)\nplt.plot(df_res)\nplt.ylabel('win ratio')\nplt.xlabel('bet confidence threshold')\nplt.ylim(0.5,0.8)\nplt.show()\n\n\n\n\n매매신뢰도의 전략 강화 결과 win ratio가 상승했으며, 신뢰도의 임계치에 따라 결과가 다른데, 임계치를 높이할수록 결과가 좋다."
  }
]