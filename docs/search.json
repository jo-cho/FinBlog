[
  {
    "objectID": "posts/systemic_risk/3_Systemic_Risk_And_Machine_Learning_2.html",
    "href": "posts/systemic_risk/3_Systemic_Risk_And_Machine_Learning_2.html",
    "title": "시스템 리스크 분석: (3-3) 시스템 리스크와 머신러닝 - 향후 연구 방향",
    "section": "",
    "text": "G. Kou et al. (2019)"
  },
  {
    "objectID": "posts/systemic_risk/3_Systemic_Risk_And_Machine_Learning_2.html#빅데이터의-영향",
    "href": "posts/systemic_risk/3_Systemic_Risk_And_Machine_Learning_2.html#빅데이터의-영향",
    "title": "시스템 리스크 분석: (3-3) 시스템 리스크와 머신러닝 - 향후 연구 방향",
    "section": "빅데이터의 영향",
    "text": "빅데이터의 영향\n빅데이터의 발전은 의심할 여지 없이 산업 데이터의 응용뿐만 아니라 이론적 연구를 위한 새로운 길을 열었다. 시스템 리스크 연구에서 빅데이터를 사용하는 것은 향후 연구의 주요 방향 중 하나가 될 수 밖에 없다.\n현재 시스템 금융 리스크에 대한 금융규제는 주로 거시적 관점과 경기순응적 리스크의 식별 및 대응에 초점을 맞추고 있다. 시스템적으로 중요한 기관의 관리, 데이터 마이닝 및 지식 발견의 적용은 향후 추가적인 조사가 필요하다. 따라서 빅데이터 기술의 사용은 비정상적인 행동의 탐지, 미시적 대상의 분류된 관리, 전형적인 사례 모델링 및 국가 간 기업 규제 모델링을 포함하여 미시적 관리 영역을 탐색하기 위해 현재 연구의 장벽을 돌파해야 한다.\n예를 들어, “대마불사(too big to fail)” 기관을 식별하는 것은 거시 건전성 금융 규제의 주요 작업 중 하나이다. 그러나 금융시스템이 고도로 네트워크화되고, 위험전파 불확실성과 금융시스템의 복잡성이 커짐에 따라 ’too big to fail’의 중요성은 점차 ’too complex to fail’의 문제로 바뀌고 있다. 복잡한 금융 네트워크에서 핵심 노드를 식별하는 것은 금융 규제 조치 및 정책을 촉진하는 효과적인 방법이다. 따라서 복잡한 금융 네트워크에 대한 주류 연구는 빅데이터 분석 기술 등에 기반한 금융 생태계의 안정성을 포함해야 한다."
  },
  {
    "objectID": "posts/systemic_risk/3_Systemic_Risk_And_Machine_Learning_2.html#데이터-드리븐",
    "href": "posts/systemic_risk/3_Systemic_Risk_And_Machine_Learning_2.html#데이터-드리븐",
    "title": "시스템 리스크 분석: (3-3) 시스템 리스크와 머신러닝 - 향후 연구 방향",
    "section": "데이터-드리븐",
    "text": "데이터-드리븐\n데이터 마이닝에 사용되는 시스템 리스크의 실제 데이터는 금융 데이터가 엄격한 리소스 및 예산 제약 하에 보관되기 때문에 주요 과제 중 하나이다.\n은행간 데이터 연계 등의 데이터 활용이 어렵기 때문에 시스템 리스크에 빅데이터가 아직 제대로 활용되지 못하고 있다. 따라서 국가 간 금융 통찰력을 갖춘 글로벌 금융 시장에 대한 실증적 근거 연구는 추가 주제이다.\n데이터를 수집하거나 사용 가능한 데이터를 삭제하는 것은 매우 어려운 작업이다.\n예를 들어, 전통적인 연구에서 금융 네트워크의 위험 전염 및 시스템 리스크의 원인은 시스템적으로 중요한 금융 기관에서 발생하는 것으로 여겨졌다. 따라서 대부분의 국가는 금융 규제를 위해 이러한 기관에 의존했다. 그러나 비금융 기관과 금융 기관을 연결하는 네트워크의 안정성이 시스템 전체의 안정성에 영향을 미치지 않을 것이라고 누가 보장할 수 있는가?\n금융네트워크의 다차원적 시스템 구조는 네트워크에 속한 금융기관이 비금융기관의 영향을 받아 불안정성이 커진다는 것을 의미한다. 현재 연구에서 네트워크 관계는 부채, 자본 대출 및 금융 기관 간의 현금 흐름에 집중되어 있다. 따라서 비금융 및 금융 기관, 특히 초국경 기관의 데이터 소스는 향후 연구의 주요 방향 중 하나가 될 것이다."
  },
  {
    "objectID": "posts/systemic_risk/3_Systemic_Risk_And_Machine_Learning_2.html#데이터-사이언스의-최적-정책",
    "href": "posts/systemic_risk/3_Systemic_Risk_And_Machine_Learning_2.html#데이터-사이언스의-최적-정책",
    "title": "시스템 리스크 분석: (3-3) 시스템 리스크와 머신러닝 - 향후 연구 방향",
    "section": "데이터 사이언스의 최적 정책",
    "text": "데이터 사이언스의 최적 정책\n금융규제와 마찬가지로 시스템 리스크에 대응하는 것은 이론적 연구와 관리실무를 유기적으로 연결하는 것이 중요하다. 그것은 경영과학의 본질이자 정보기술의 본질적인 요구이다. 정보기술을 활용해 금융리스크에 대응하는 레그테크(reg-tech)가 최근 화두가 되고 있다. 시스템 리스크의 규제는 데이터 과학의 정량적 연구 측면에서 포괄적인 정책 도구를 사용해야 하는 금융 시스템 엔지니어링 문제이다.\n예를 들어 건전한 금융 규제 연구는 거시적 및 미시적 건전성 요소와 행동 감독을 결합해야 한다. 그러나 최선의 정책을 선택하려면 비용과 편익에 대한 포괄적인 평가가 필요하다. 따라서 정책 집행의 실효성 평가는 시스템 리스크 규제의 중요한 영역 중 하나이다. 향후 연구의 주요 방향은 비용 효율적인 분석 방법의 적용 및 다목적 방법을 포함한다."
  },
  {
    "objectID": "posts/systemic_risk/3_Systemic_Risk_And_Machine_Learning_1.html",
    "href": "posts/systemic_risk/3_Systemic_Risk_And_Machine_Learning_1.html",
    "title": "시스템 리스크 분석: (3-1) 시스템 리스크와 머신러닝 - 네트워크 기반 및 빅데이터 분석",
    "section": "",
    "text": "G. Kou et al. (2019)"
  },
  {
    "objectID": "posts/systemic_risk/3_Systemic_Risk_And_Machine_Learning_1.html#네트워크-기반-금융기관의-시스템-리스크",
    "href": "posts/systemic_risk/3_Systemic_Risk_And_Machine_Learning_1.html#네트워크-기반-금융기관의-시스템-리스크",
    "title": "시스템 리스크 분석: (3-1) 시스템 리스크와 머신러닝 - 네트워크 기반 및 빅데이터 분석",
    "section": "네트워크 기반 금융기관의 시스템 리스크",
    "text": "네트워크 기반 금융기관의 시스템 리스크\n2008년 금융 위기 이전에는 시스템 리스크 규제 문제에 대한 연구가 드물었으며, 초기 연구는 단일 은행 운영 위험에 대응하는 시도 정도였다. 그 후로 금융 규제의 중요성이 널리 인식되고 있지만 현재 은행 시스템에 복잡한 네트워크가 형성되어 있는 상황에서 시스템 리스크를 다루는 것은 매우 어려운 일이며, 현재 금융 연구의 중요한 주제가 되었다.\n\n금융 네트워크의 발달\n금융기관의 광범위한 상호연계로 인한 복잡한 네트워크는 위험전이(risk transmission) 가능성을 높인다고 알려져있다. 은행 시스템의 유동성은 은행 간의 직접적인 연결을 필요로 하는 반면, 동종 상품과 위험 회피(risk aversion)는 은행 간의 광범위한 간접 연결로 이어진다. Battiston et al. (2016)에 따르면 경제 및 금융 정책은 네트워크 분석, 행동 모델링 및 복합 시스템 이론을 적용할 필요가 있으며 금융 규제 문제에 대한 연구는 복잡한 네트워크를 다루는 문제와 직접적으로 연결되어 있다.\n\n은행간 네트워크는 작은 충격 하에서는 안정성을 향상시키지만, 특정 지점을 넘어서면 위험 감수(risk-taking)하게 되고 결국 실패하게 된다(Acemoglu et al., 2015). 따라서 복잡한 금융 네트워크의 “tipping” 포인트와 안정성 문제를 고려하여 금융 정책 개발의 방향을 결정해야 한다.\nHaldane and May(2011)는 금융 시스템 내 파생상품 거래가 금융 네트워크 위험의 주요 원인 중 하나라고 주장한다.\n또한 네트워크 기반 금융 시스템과 위험의 전염에 대한 노출을 증폭시키고(Prasanna et al., 2011; Hu et al., 2012),\n은행 간 시스템(interbank system)의 유동성은 금융 네트워크의 불안정성을 증가시킨다(Ferrara et al., 2016).\n\n\n\n위험 노출 및 전이\n금융 네트워크는 항상 은행 부도의 프로세스를 시뮬레이션할 수 있는 위험 노출(risk exposure)의 행렬로 설정된다.\n\nGiudici et al.(2017)은 BIS 데이터를 기반으로 한 상호 연결성 측정뿐만 아니라 여러 국가 간의 다변량 네트워크 구조 및 위험 노출 프로세스를 평가했다. 제안된 모델은 조기 경보 및 예측 전염으로 사용될 수 있다.\nGiudici and Spelta(2016)는 다변량 그래픽 모델과 베이지안 그래픽 모델을 통해 개별 국가 그룹이 금융 네트워크의 중심 노드로서 서로 다른 행동을 한다는 것을 제안하고 증명했다.\nAmini et al.(2013)은 규제 정책에서 전염 노출과 관련하여 설정되어야 하는 자본의 최소 비율을 제안하면서 비균질 방향 그래프(inhomogeneous directed graphs)를 사용하여 금융 네트워크에서 캐스케이드(cascade) 프로세스 및 점근적(asymptotic) 전염을 연구했다.\nBluhm and Krahnen(2014)은 상호 연결된 은행 대차대조표와 제안된 체계적 value-at-risk 거시 건전성 방법을 기반으로 시스템 리스크를 노출하는 네트워크 모델을 개발했다.\nChoi(2014)는 기관이 클수록 금융 네트워크에 더 긍정적인 영향을 미치므로 약자보다 강자를 강화하는 것이 은행 시스템의 안정성을 유지하는 데 효과적으로 도움이 될 것이라고 인식했다.\n\n네트워크에서 위험이 이동하는 경로를 밝히고 경기 대응 감독을 통해 몇 가지 대응책을 구현한 연구도 있다.\n\nBillio et al.(2012)은 외생적 충격을 경험하는 금융 네트워크에는 항상 상호 상관관계가 존재함을 증명했다.\nCruz and Lind(2012)는 입자 이동 모델을 사용하여 거래 네트워크의 에이전트에 의해 일련의 지급 불능이 어떻게 촉발되는지 보여주었다.\nLadley(2013)는 큰 충격은 금융 시스템 리스크로 이어지는 반면, 작은 은행 디폴트에서는 시스템적 리스크를 관찰할 수 없다고 주장했다. 위험 분산과 위험 전가가 허용되는 상황에서 최적의 시장 구조는 없다. 반면 예금보험은 은행간 네트워크 내 상호의존성을 줄여 시스템리스크 발생 가능성을 낮춘다.\nBetz et al. (2016)은 서로 다른 기간 내의 시스템 위험 평가를 위한 프레임워크를 제공했다. 이 프레임워크는 은행 부문의 분열 과정과 국가 은행과의 연계를 드러냈다.\nSouza(2016)는 브라질 은행 시스템에서 더 큰 은행이 더 심각한 손실을 입을 것이라는 사실을 발견했다.\nCerchiello and Giudici(2015)는 공동 시장 데이터 및 대차대조표 데이터를 기반으로 중앙 기관을 식별하여 유럽 최대 은행의 시스템 리스크를 연구하기 위해 조건부 그래픽 모델을 평가했다.\nShen(2017)은 전자 물류 투자의 재정적 위험을 모델링하기 위해 베이지안 네트워크 접근 방식을 제안했다.\n\n\n\n실물경제와의 관계\n실물경제에 미치는 영향은 은행간(interbank) 시장과 실물경제 간의(Gabbi et al., 2014), 혹은 금융시장(Diebold & Yilmaz, 2014) 간의 네트워크와 같이 서로 다른 노드로 구축된 특별한 네트워크 측면의 한가지 주제이다.\n\nHelwege와 Zhang(2016)은 정보 전염이 동일한 시장의 경쟁사에 더 큰 영향을 미치고 파산 단계보다 위기(crisis)에서 더 심각한 피해를 입힌다고 주장했다.\nDiebold and Yilmaz(2014)는 2007~2008년 금융위기 당시 미국 주요 금융기관의 주식 수익률 변동성의 일별 변동(daily-varied) 연결성을 연구하기 위해 가중방향 그래프(weighted directed graph)의 연결성을 이용했다. 금융시장은 자산의 상호전도에 따른 시스템적 충격으로 피해를 입었다.\nGiudici and Parisi(2017)는 부채와 GDP의 상관관계를 조사하기 위해 상관 확률적 프로세스(correlated stochastic process)를 사용했으며, 소버린(sovereign) 리스크는 항상 부채와 GDP 성장의 영향을 받음을 밝혔다.\n\n\n\n금융 네트워크 구조\n다중 계층(multilayer) 은행 네트워크는 공통 자산에 대한 공통 노출로 구축되는 금융 시스템의 중요한 기능 중 하나이다.\n\n다중 계층 네트워크는 항상 금융 위험을 방어하는 능력을 감소시키고 금융 위기의 비용을 증가시킨다(Battiston, Farmer, & Flache, 2016).\nPoledna et al. (2015)은 금융 네트워크는 신용, 파생 상품, 외환 및 증권으로 연결된 다층 네트워크로 취급되어야 하며 네트워크가 단일 계층에만 집중될 경우 예상 손실이 전체 위험의 최대 90%까지 과소 평가될 수 있음을 밝혔다. 또 다른 결과로 금융 다층 네트워크는 모든 계층의 시스템 위험 합계가 총 위험보다 낮은 비선형성 이벤트를 제공한다.\n\n이 분야에서 은행 네트워크를 연구하는 데 사용되는 주요 방법론에는 무작위 그래프 이론(random graph theory), 입자 물리학(particle physics), 게임 이론, 편익-비용 분석(benefit-cost analysis), 복잡한 네트워크를 나타내는 다양한 수학적 모델 등이 있다. 또한 전염병 모델 및 확률적 최적 모델과 같은 다른 방법이 금융 네트워크 연구에 적용되었다. 그러나 현재 연구는 미국 은행 시스템, 영국 은행, 유럽 연합, 브라질 및 기타 신흥 경제와 같은 단일 경제 내의 은행 간 시스템에 국한되어 있다. 글로벌 연구 네트워크로서 국가 간 금융 기관 및 글로벌 금융 시스템에 대한 연구는 거의 없다.\n예를 들어, 기존 문헌은 주로 공동 금융과 경제 네트워크에 초점을 맞추었고, 향후 네트워크 기반의 시스템 리스크 연구에서 해결해야 할 시급한 문제인 지리, 인문사회 관련 네트워크와 기타 외부 요인에 대한 연구는 거의 없었다."
  },
  {
    "objectID": "posts/systemic_risk/3_Systemic_Risk_And_Machine_Learning_1.html#금융위험-관리에-대한-빅데이터-분석",
    "href": "posts/systemic_risk/3_Systemic_Risk_And_Machine_Learning_1.html#금융위험-관리에-대한-빅데이터-분석",
    "title": "시스템 리스크 분석: (3-1) 시스템 리스크와 머신러닝 - 네트워크 기반 및 빅데이터 분석",
    "section": "금융위험 관리에 대한 빅데이터 분석",
    "text": "금융위험 관리에 대한 빅데이터 분석\n오늘날 빅데이터는 데이터 마이닝 기술 및 비즈니스 인텔리전스를 통한 이론적 연구뿐만 아니라 금융 위험 분석, 정치 경제학, 비즈니스 데이터 리소스 서비스, 전자 상거래를 포함하여 여러 영역에서의 연구가 진행되고 있다.\n\n비즈니스 인텔리전스와 금융 시스템 리스크\n일반적으로 비즈니스 인텔리전스(BI)의 전략과 기술은 인터넷, 무선 주파수 식별, 클라우드 서비스, 의사 결정 지원 시스템 및 웹 기반 BI의 다섯 부분으로 나눌 수 있다(Choi, Chan, & Yue, 2017).\n금융 빅데이터는 은행간 유동성, 글로벌 자본흐름 등 관련 금융정보를 포함하며, 금융규제를 통한 리스크 대응을 위한 리스크 조기경보, 리스크 식별 등의 업무를 수행할 수 있다. 현재/최근 연구에는 빅데이터 알고리즘(Campbellverduyn et al., 2017), 웹 계약과 금융 시장 간의 상관관계(Flood et al., 2011), 금융 신용 위험 평가(Cui, 2015), 중국 주식 시장의 변동성 분석(Dong, Yang, & Tian, 2015), 주가와 대중 감정의 관계(Smailović et al., 2014) 등이 있다.\n금융 시스템 리스크에서 빅데이터는 위험원과 위험의 다각화 간의 상관관계를 분석하는 사용되었다.\n\nCerchiello and Giudici(2016)는 이 두 가지 소스에서 선택된 빅데이터를 기반으로 최초의 시스템 리스크 모델을 구축했다. 결과로 베이지안 방법을 사용하여 서로 다른 데이터 구조를 결합하고 금융 위험과 대중 감정 간의 상관 관계를 테스트했다.\nSarlin(2016a)은 기계 학습, 네트워크 분석, 시뮬레이션 및 시스템 리스크를 위한 퍼지 시스템과 같은 주요 지능형 알고리즘을 소개했다.\n또한 Sarlin(2016b)은 도표, 지도 및 네트워크의 세 가지 모듈로 금융 규제를 위한 시각적 도구를 제안했다.\nCerchiello et al. (2016)은 금융 관련 트위터와 금융 시장을 기반으로 한 위험 전염 모델을 연구했으며, 충격이 발생할 때 이들의 상관 관계가 예측되는 것을 보였다.\n\n현재 빅데이터 인사이트를 활용한 연구는 이질적인 금융 데이터를 융합하는 것을 목표로 하고 있지만, 데이터의 양은 글로벌한 금융시장으로부터는 제한되어 있다.\n\n\n데이터 한계\n금융 위험 분석은 고품질의 데이터에 의존한다. 글로벌 금융 시장의 데이터원을 사용할 수 없어 데이터를 확장하기 어렵고 데이터 품질도 보장되지 않는다.\n\nCerchiello & Giudici(2016)은 다중 원천의 데이터 통합이 빅데이터로 제공되어야 한다고 말한다.\nFlood et  al.(2011)은 금융의 위험은 항상 시간과 관련되어 있고 시변적이므로 데이터 자원에 대한 효과적인 위험 분석을 보장하기 어렵다고 주장한다. 또한 데이터 통합 및 품질은 금융 주체의 복잡성으로 인해 해결하기 어려운 문제이다.\nJagadish et al. (2014)은 빅데이터 분석에는 데이터 수집, 모델링 및 분석, 해석 및 배포와 같은 많은 단계가 포함되어야 한다고 제안했다. 그러나 이질성, 적시성, 비일관성, 불완전성을 포함한 빅데이터의 과제도 실생활 질문에 적용하는 데 걸림돌이 되고 있다. 마찬가지로 금융시장의 데이터 자원, 특히 은행 간 네트워크와 은행 네트워크에서 빅데이터 분석을 활용하는 것도 어렵다.\nYang et al. (2014)는 저장 데이터를 개선하기 위한 프레임워크 및 분류 표준을 제안했다.\nFardani Haryadi, Hulstijn, Wahyudi, van der Voort and Janssen(2016)은 다양한 은행의 데이터 품질을 정의하고 세부 목표를 제안했다.\nBrammertz & Mendelowitz(2014)는 금융 계약의 세분성(granularity) 정보에서 오는 금융 빅데이터에서 금융 규제를 위한 정보추출을 제안하고, 조건부(contingent) 현금 흐름으로 변환했으며, 기관, 시스템 및 개인 등의 다층의 금융 주체에 대한 혼합 모델을 개발했다.\n\n데이터 자원과 품질이 빅데이터 및 금융 데이터 분석과 관련되어 있음은 분명하다. 데이터 품질은 향후 연구의 중요한 방향이 될 것이며(Choi et al., 2017), 더 나아가 빅데이터 분석을 위한 견고한 기반을 마련하기 위해서는 데이터 품질 평가 지표 체계를 더욱 발전시켜야 한다. 그러나 데이터의 양은 관련 재무 및 경영학 문헌에서 아직 논의되지 않았으므로 새로운 도전이 될 것이다.\n\n\n금융시장의 감성 분석\n은행 간 시장의 데이터 획득으로 인해 트윗, 뉴스 및 법률과 같은 인터넷 데이터는 항상 금융 위험을 연구하는 텍스트 데이터 역할을 했다. 감정 분석 또는 오피니언 마이닝은 특정 분야에 대한 텍스트 마이닝을 사용하여 고객 및 시장의 태도, 의견 및 경향을 발견하는 것을 목표로 한다. 먼저, 시장 정서에 대한 텍스트 기반을 구축한다.\n\nO’Halloran et al.(2015)은 특징(features)의 순위를 매기고 법률을 다양한 레이블로 분류하기 위해 미국 연방법에서 추출한 금융 규제에 대한 뉴스 데이터베이스를 만들었다. 빅데이터 분석은 규제 기관이 다양한 금융 시장에 대응하기 위해 합리적인 법률을 선택하는 데 도움이 될 수 있다.\n또한 Twitter 데이터를 사용하여 금융 시스템 위험을 추정할 수 있다. 이 방법은 금융 기관의 감염을 개별화하는 데 사용할 수 있다(Cerchiello & Giudici, 2016; Giudici et al., 2016).\nGarcía(2013)는 20세기 New York Times를 바탕으로 자산 가격과 심리 간의 관계를 발견하고 불황 시 편차 충격을 보여 조건부 평균 수익률의 변화를 예측했다.\n\n대중의 감정은 감정 마이닝(Tsai & Wang, 2013)을 통해 잠재적 추세를 결정하는 데 사용될 수 있지만, 시장 분석과 금융 시스템 위험 사이에는 많은 종속성이 있다.\n\nNyman et al. (2014)은 텍스트 세트를 기반으로 한 시장 심리로 금융 시스템 위험을 측정할 수 있음을 발견했다.\nChiang and Chen(2015)은 금융 뉴스 웹사이트에서 금융 텍스트 데이터베이스를 수집 및 구축하고 인기 단어 집합을 추출하여 감정 지수를 계산했다. 또한 그들은 감정 지수를 사용하여 전기 산업의 주가 지수를 예측할 수 있음을 발견했다.\nTsai and Wang(2017)은 금융 은행 보고서의 텍스트 정보를 사용하여 금융 위험을 예측했으며, 결과는 금융 위험과 강한 상관 관계로 인식되는 소프트 정보를 보여준다. 그들의 발견은 금융 위험과 시장 정서 사이의 강한 상관관계를 증명할 수 있었다.\nMeyer et al. (2017)은 가격 및 위험 예측 모델을 밝히기 위해 금융 뉴스의 세분성 감정 분석을 구현했다. 그들의 모델에서는 감정을 예측하기 위해 구문 문장 패턴을 추출하는 데 자연어 처리가 사용된다.\n\n감정 분석 방법은 회귀 및 순위 방법(Tsai & Wang, 2017, 2013) 등의 고전적 머신러닝 알고리즘, 어휘 및 기계 학습 감정 분석(Meyer et al., 2017), 규칙 기반 방출 모델(Rule-based Emission Model) 알고리즘(Tromp et al., 2017), 긍정/부정 단어의 비율(García, 2013) 등이 있다.\n현재 금융 시장의 데이터 자원이 개방적이고 획일적인 기준으로 확립되지 않은 상황에서 감정 분석은 의심할 여지없이 금융 위험을 분석하는 효과적인 방법이 된다. 또한 텍스트 분석을 통한 시스템 리스크 관리 기술과 텍스트 데이터베이스 개발이 필요하다."
  },
  {
    "objectID": "posts/systemic_risk/2-5_Systemic_Risk_Measures_Network.html",
    "href": "posts/systemic_risk/2-5_Systemic_Risk_Measures_Network.html",
    "title": "시스템 리스크 분석: (2-5) 네트워크 측정방법",
    "section": "",
    "text": "서상원 (2018)\n금융기관들간의 직접적인 연계성이 존재하는 경우에 시스템리스크 측면에서 어떤 영향을 미치는지 효과적으로 분석한다."
  },
  {
    "objectID": "posts/systemic_risk/2-5_Systemic_Risk_Measures_Network.html#모형-설정",
    "href": "posts/systemic_risk/2-5_Systemic_Risk_Measures_Network.html#모형-설정",
    "title": "시스템 리스크 분석: (2-5) 네트워크 측정방법",
    "section": "모형 설정",
    "text": "모형 설정\n\n금융시스템 내에 \\(n\\)개의 금융기관이 존재하고 금융기관들은 다른 금융기관 또는 민간들과 금융거래를 함\n금융기관 \\(i\\)를 중심으로 생각하면,\n\n자산부문은 민간에 대한 자산 \\(c_i\\)와 다른 금융기관에 대한 자산 \\(\\sum_k \\bar{p}_{ki}\\)로 구성\n부채는 민간에 대한 부채 \\(b_i\\)와 다른 금융기관에 대한 부채 \\(\\sum_j \\bar{p}_{ij}\\)로 구성\n순자산 \\(w_i\\)는 자산과 부채의 차이에 해당\n여기서 \\(\\bar{p}_{ij}\\)는 금융기관 \\(i\\)가 \\(j\\)에게 지불하여야 하는 명목 부채금액, \\(p_i\\)는 금융기관 \\(i\\)의 부채 총계\n\n금융기관 \\(i\\)에 부정적인 충격이 발생하여 자산의 손실이 \\(x_i\\)만큼 발생했을 때,\n\n순자산은 \\(w_i(x_i)=c_i-x_i+\\sum_{j\\neq i}\\bar{p}_{ij}-\\bar{p}_i\\) 로 변동\n순자산 규모가 음(-)이면 금융기관 \\(i\\)는 부도 상황\n\n부도상황에서는 명목 부채중 일부만 상환이 이루어지는데, 통상 기존 부채액에 따른 비율대로(pro rata) 부분상환이 이루어진다고 가정\n실제 지불되는 금액을 정산 벡터(clearing payment vector)라고 함, 하지만 여러 금융기관의 금융거래에 따른 지불 및 수취가 서로 연계되어 있어 정산 벡터를 체계적으로 정하기 어려움\nEisenberg and Noe(2001): 정산 벡터는 외부 충격의 발생으로 인해 금융기관이 도산한 직접적인 효과를 반영할 뿐만 아니라 그 이후에 다른 금융기관이 전염효과로 인해 도산하는 간접적인 효과들도 반영하여 정해짐"
  },
  {
    "objectID": "posts/systemic_risk/2-5_Systemic_Risk_Measures_Network.html#여러-특징-부가",
    "href": "posts/systemic_risk/2-5_Systemic_Risk_Measures_Network.html#여러-특징-부가",
    "title": "시스템 리스크 분석: (2-5) 네트워크 측정방법",
    "section": "여러 특징 부가",
    "text": "여러 특징 부가\n\n금융기관이 부도 상황에 처하게 되면 그 금융기관과의 기존 금융거래에 대한 결제가 원활하게 이루어지지 못하게 되고 청산관련 비용들도 발생\n\nRogers and Veraart(2013)은 회수함수(recovery function)를 도입하여 부도발생시 부도 금융기관이 보유한 자산가치가 하락하는 현상을 반영\n\n주식은 최후순위의 청구권으로서 그 가치가 부도 발생 여부에 따라 차이가 생기는 비선형적인 특성을 보임\n\nGourieroux et al.(2013)은 금융기관간 주식 교차보유(cross-holding)와 같은 청구권의 우선순위 문제를 모형에 반영\n\n부도 위기에 몰린 금융기관은 결제자금용의 유동자산을 마련하기 위해 비유동자산을 시장에 급매(fire sales)하게 되고, 이로 인해 비유동자산의 가격이 하락하게 되는 손실을 그 비유동자산 또는 그와 유사한 비유동자산을 보유한 다른 금융기관들도 함께 보게 됨\n\nCifuentes et al.(2005)는 이러한 자산 급매 경로를 모형에 반영\n\n한 금융기관이 금융시장에서 신뢰성이 하락하게 되면 그 금융기관에 대해 청구권을 가지고 있는 상대 금융기관은 자신의 청구권 가치를 시장가격을 반영하여 조정\n\n시가 평가(mark to market) 관행으로 인한 네트워크 효과 반영"
  },
  {
    "objectID": "posts/systemic_risk/2-5_Systemic_Risk_Measures_Network.html#시스템리스크-측정",
    "href": "posts/systemic_risk/2-5_Systemic_Risk_Measures_Network.html#시스템리스크-측정",
    "title": "시스템 리스크 분석: (2-5) 네트워크 측정방법",
    "section": "시스템리스크 측정",
    "text": "시스템리스크 측정\n\n외부의 충격 발생으로 인한 시스템 손실(systemic loss in value)\n\n\\(L(x)=\\sum^{n}_{i=1}x_i+ \\sum^n_{i=1}(\\bar{p}-p_i(x))\\)\n첫째항은 외부충격 \\(x=(x_1,\\dotsc, x_n)\\)의 직접적 효과를, 둘째 항은 그 간접적 효과를 나타냄\n\n금융기관 \\(i\\)의 \\(j\\)에 대한 취약도(vulnerability)\n\n\\(\\alpha_{ij}\\frac{(c_i-w_i)}{w_j} = \\alpha_{ij}(\\lambda_i-1)\\frac{w_i}{w_j}\\)\n\\(\\lambda_i \\equiv c_i / w_i\\)는 금융기관 \\(i\\) 의 외부 레버리지\n\\(\\alpha_{ij} \\equiv \\bar{p}_{ij}/\\bar{p}_i\\)는 금융기관 \\(i\\)의 네트워크 부채중 \\(j\\)에 대한 상대부채비율\n이 취약도 지표가 1보다 작을 때는 금융기관 \\(i\\)에 외부충격이 발생하여도 금융기관 \\(j\\)는 직접적인 효과를 견딜 수 있음\n\n금융연계성(financial connectivity): 금융기관 \\(i\\)의 부채중에서 네트워크 부채의 비중\n\n\\(\\beta_i \\equiv (\\bar{p}_i-b_i)/\\bar{p}\\)\n\n전염지수(contagion index): 전염지수가 클수록 금융기관 \\(i\\) 에서 발생 하는 외부 충격이 네크워크에 주는 영향이 큼\n\n\\(w_i\\beta_i(\\lambda_i-1)\\)"
  },
  {
    "objectID": "posts/systemic_risk/2-5_Systemic_Risk_Measures_Network.html#네트워크-특성에-대한-요약-정보를-나타내는-지표",
    "href": "posts/systemic_risk/2-5_Systemic_Risk_Measures_Network.html#네트워크-특성에-대한-요약-정보를-나타내는-지표",
    "title": "시스템 리스크 분석: (2-5) 네트워크 측정방법",
    "section": "네트워크 특성에 대한 요약 정보를 나타내는 지표",
    "text": "네트워크 특성에 대한 요약 정보를 나타내는 지표\n\n인접행렬(adjacency matrix)\n\n금융기관 \\(i\\)가 \\(j\\)에 채무가 있다면 이를 (N N) 행렬 B의 (i,j)번째 원소에 1로 (즉, B_{ij}=1), 아니면 0으로 (B_{ij}=0) 표시하는 방식으로 연결관계를 나타내는 행렬\n\n수취관련도 & 지급관련도\n\n수취관련도(in-degree)\n\n한 금융기관이 몇 개의 금융기관으로부터 수취할 청구권을 보유하였는지를 나타냄\n\n지급관련도(out-degree)\n\n몇 개의 금융기관에 지급할 의무가 있는지를 나타냄\n\n\n실제 네트워크에서 수취 또는 지급 관련도는 전형적으로 파레토분포(Pareto distribution)의 꼬리와 같이 power law를 따른다고 알려짐. 이는 소수의 금융기관은 매우 높은 관련도를 보이는 반면, 대다수의 금융기관들은 낮은 관련도를 보인다는 것을 시사함\n\n시스템리스크 측면에서 네트워크 구조가 강하면 개별 금융기관의 위험을 분산시키는데 유리하다는 장점과 한 금융기관에 대한 충격이 다른 금융기관으로 전염되는 경로가 될 수 있다는 단점이 공존함\n이러한 장점과 단점 중 어느 효과가 더 큰지에 대해 실증적으로 분석하기 위하여 중심성 지표들을 산출하고 이를 이용하여 중심성이 강할수록 금융기관의 부도확률이 높아지는지를 경험적으로 분석\nNier et al.(2007)은 네트워크 연계성(connectivity)이 강할수록 충격을 흡수하는 능력과 충격의 전염효과 중 어느 효과가 더 커지는지를 시뮬레이션 방법을 이용하여 분석, 네트워크 연계성이 낮은 수준에서는 연계성이 강해질수록 전염효과가 더 크게 나타났으나, 네트워크 연계성이 높은 수준에서는 연계성이 강해질수록 충격 흡수 효과가 더 크게 나타남"
  },
  {
    "objectID": "posts/systemic_risk/2-3_Systemic_Risk_Measures_Cross-sectional.html",
    "href": "posts/systemic_risk/2-3_Systemic_Risk_Measures_Cross-sectional.html",
    "title": "시스템 리스크 분석: (2-3) 금융기관 단면적 위험 측정방법",
    "section": "",
    "text": "서상원 (2018)\n시스템리스크 측정 방법 중 다수의 금융기관이 동시에 위험에 처하게 되는 위험을 직접적으로 측정하는 법을 알아본다. 금융기관의 부도사건에 대한 다변수 확률분포에 대한 정보를 추출하고 이를 이용하여 시스템리스크를 측정한다."
  },
  {
    "objectID": "posts/systemic_risk/2-3_Systemic_Risk_Measures_Cross-sectional.html#pcas-principal-component-analysis-based-systemic-risk-indicator",
    "href": "posts/systemic_risk/2-3_Systemic_Risk_Measures_Cross-sectional.html#pcas-principal-component-analysis-based-systemic-risk-indicator",
    "title": "시스템 리스크 분석: (2-3) 금융기관 단면적 위험 측정방법",
    "section": "PCAS : Principal component analysis-based systemic risk indicator",
    "text": "PCAS : Principal component analysis-based systemic risk indicator\nBillio et al.(2012)\n은행, 증권회사, 보험회사 및 헤지펀드 등 주요 금융기관들의 주가 수익률간의 연계성(interconnectedness)을 PCA 기법을 이용하여 측정하는 방법\n\n여러 금융기관들의 주가 수익률이 서로 긴밀하게 연계되어 있을수록 상위 몇개의 주성분으로 (분산을 기준으로 할 때) 전체의 변동을 잘 설명할 수 있게 됨\n\n\\(h_n = \\frac{\\sum^n_{i=1}\\lambda_i}{\\sum^n_{i=1}\\lambda_i}\\)\n\\(\\lambda_i\\)는 금융기관들의 주가 수익률 공분산 행렬의 \\(i\\)번째 특성치\n금융기관간 연계성이 강할수록 \\(h_n\\)이 증가\nKritzman et al.(2011)의 PCA-AR은 주식시장에서의 연계성을 측정한데 반해, Billio et al.(2012)는 금융기관간 연계성을 측정\n\n개별 금융기관이 시스템리스크에 미치는 기여도\n\n\\({PCAS}_{i,n} \\equiv \\frac{1}{2} \\frac{\\sigma^2_i}{\\sigma^2_S} \\frac{\\partial\\sigma^2_S}{\\partial\\sigma^2_i}|_{h_n \\geq H} = \\frac{\\sigma^2_i}{\\sigma^2_S} \\sum^n_{k=1}W^2_{ik}\\lambda_k|_{h_n} \\geq H\\)\n\\(\\sigma^2_i\\)는 주가수익률의 분산으로 측정한 개별 금융기관의 위험\n\\(\\sigma^2_S\\)개별 금융기관 주가수익률의 공분산의 합으로 측정한 금융시스템의 위험도\n\\(W^2_{ik}\\)는 금융기관 주가 수익률 공분산 행렬의 특성벡터로 이루어진 주성분 가중치 행렬 \\(W\\)의 \\((i,k)\\)번째 원소\n\\(H\\)는 금융기관간 연계성에 대한 임계치로서 \\(h_n\\)이 \\(H\\)를 상회할 경우 강한 연계성이 존재하는 것으로 판단\n\n즉, PCAS는 개별 금융기관 위험도의 상대적 증가가 금융시스템의 상대적 위험도 증가에 미치는 영향을 민감도를 이용하여 측정\n\n\n\n\nBillio et al. (2012) Figure 1"
  },
  {
    "objectID": "posts/systemic_risk/2-3_Systemic_Risk_Measures_Cross-sectional.html#covar-co-value-at-risk",
    "href": "posts/systemic_risk/2-3_Systemic_Risk_Measures_Cross-sectional.html#covar-co-value-at-risk",
    "title": "시스템 리스크 분석: (2-3) 금융기관 단면적 위험 측정방법",
    "section": "CoVaR: Co-Value-at-Risk",
    "text": "CoVaR: Co-Value-at-Risk\nAdrian & Brunnermeier (2008)\nVaR(Value-at-Risk)(금융기관의 위험도를 측정하는 대표적인 지표)개념을 활용하여 시스템리스크를 측정\n\n금융기관 \\(i\\)의 \\(q\\%\\) 유의수준에서 \\(VaR\\)\n\n\\(Pr[X^i \\leq {VaR}^i_q] = q\\%\\)\n\\(X^i\\)는 금융기관 \\(i\\)의 손실규모를 나타내는 확률변수\n\n금융기관 (또는 금융기관의 그룹) \\(j\\) 의 \\(CoVaR\\)\n\n\\(Pr[X^j |C(X^i) \\leq {CoVaR}^{j|C(X^i)}_q] = q\\%\\)\n\\(C(X^i)\\)는 금융기관 i가 손실허용한도인 \\({VaR}^i_q\\)를 넘어서는 손실을 보는 사건\n\n금융기관 \\(i\\)로 인한 \\(j\\)의 위험도 변화\n\n\\(\\Delta{CoVaR}^{j|i}_q \\equiv {CoVaR}^{j|X^i={VaR}^i_q}_q - {CoVaR}^{j|X^i={VaR}^i_{50}}_q\\)\n즉, 금융기관 \\(i\\)가 큰 손실을 보는 경우와 평균적인 경우에 금융기관 \\(j\\)의 위험수준의 차이\n\n시스템리스크 관점에서 금융기관 \\(i\\)가 시스템리스크에 미치는 기여도를 측정하기 위해 \\(j\\)를 금융시스템으로 간주\n\n\\(\\Delta{CoVaR}^{system|i}_q \\equiv {CoVaR}^{system|X^i={VaR}^i_q}_q - {CoVaR}^{system|X^i={VaR}^i_{50}}_q\\)\n\n반대로 금융기관을 \\(j\\), 시스템을 \\(i\\)로 하면, 금융기관이 시스템리스크의 변동에 따라 얼마나 영향을 받는지를 측정 (Exposure-CoVaR)/ 금융기관을 \\(i\\), \\(j\\)로 하면 Network-CoVaR\n\n\n\n\nAdrian, Brunnermeier (2008) Figure 4"
  },
  {
    "objectID": "posts/systemic_risk/2-3_Systemic_Risk_Measures_Cross-sectional.html#ses-systemic-expected-shortfall",
    "href": "posts/systemic_risk/2-3_Systemic_Risk_Measures_Cross-sectional.html#ses-systemic-expected-shortfall",
    "title": "시스템 리스크 분석: (2-3) 금융기관 단면적 위험 측정방법",
    "section": "SES: Systemic expected shortfall",
    "text": "SES: Systemic expected shortfall\nAcharya et al. (2010)\n금융 시스템 위기가 발생한 경우에 개별 금융기관의 적정 자본수준대비 자본의 부족규모 예상치를 측정\n\n가정 및 개념\n\n시스템리스크: 시장 주가지수의 하락폭이 하위 5%를 넘는 경우로 전제\nMES: 시스템리스크가 발생하였을 때 해당 금융기관의 주가 수익률 하락폭의 기댓값\n레버리지: ‘주식 시가총액’ 대비 ’장부가 자산 – 장부가 부채 + 주식 시가총액’의 비율로 측정\n\nSES 추정\n\n금융위기 기간에 금융기관별 주가수익률 하락폭을 실현된 SES로 간주하고 이 실현된 SES를 다음과 같이 MES 및 레버리지에 대해 회귀\n\n\\(SES_i = \\alpha+ \\beta MES_i + \\gamma LVG_i + \\delta'DUM\\)\n\\(DUM\\)은 은행, 증권, 보험 등 금융기관의 종류를 나타내는 더미변수 벡터\n\n이 회귀식을 통해 추정된 SES를 이용하여 미국의 100여개 금융기관에 대해 금융기관별 시스템리스크의 순위(rank)를 정함"
  },
  {
    "objectID": "posts/systemic_risk/2-3_Systemic_Risk_Measures_Cross-sectional.html#ces-component-expected-shortfall",
    "href": "posts/systemic_risk/2-3_Systemic_Risk_Measures_Cross-sectional.html#ces-component-expected-shortfall",
    "title": "시스템 리스크 분석: (2-3) 금융기관 단면적 위험 측정방법",
    "section": "CES: Component expected shortfall",
    "text": "CES: Component expected shortfall\nBanulescu and Dumitrescu(2015)\nMES와 해당 금융기관의 비중의 곱\n\n금융시스템 전체의 수익률은 다음과 같이 개별 금융기관의 수익률과 금융기관별 비중의 가중합으로 표현\n\n\\(r_{mt}=\\sum^n_{i=1}w_{it}t_{it}\\)\n\n금융시스템 전체의 수익률이 일정 임계치 C를 하회하면 시스템리스크가 발생한 것으로 정의\n\n조건부 기대손실규모 (ES):\n\\(ES_{mt-1}(C)=-E_{t-1}(r_{mt}|r_{mt}&lt;C)\\)\n\nMES, CES\n\n\\(MES_{it} \\equiv \\frac{\\partial ES_{mt-1}}{\\partial w_{it}} = -E_{t-1}(r_{it}|r_{mt}&lt;C)\\)\n\\(CES_{it} = w_{it}MES_{it} = -w_{it}E_{t-1}(r_{it}|r_{mt}&lt;C)\\)\n즉, \\(ES_{mt-1}= \\sum^n_{i=1}CES_{it}\\)\n\nMES 또는 SRISK 등이 금융기관의 규모를 감안하지 못하는데 반해, 이 CES 지표는 명시적으로 개별 금융기관의 중요도를 감안한다는 차이점이 있으며, SIFI의 식별 등에 유용한 정보를 제공\n\n\n\n\nBanilescu and Dumitrescu (2015) Figure 3"
  },
  {
    "objectID": "posts/systemic_risk/2-3_Systemic_Risk_Measures_Cross-sectional.html#srisk-sytemic-risk",
    "href": "posts/systemic_risk/2-3_Systemic_Risk_Measures_Cross-sectional.html#srisk-sytemic-risk",
    "title": "시스템 리스크 분석: (2-3) 금융기관 단면적 위험 측정방법",
    "section": "SRISK: Sytemic risk",
    "text": "SRISK: Sytemic risk\nBrownlees & Engle (2010)\n금융 시스템 위기가 발생한 경우에 개별 금융기관의 자본 부족규모 예상치를 측정\n\n\\(SRISK_{it}=W_{it}[kLVG_{it}+(1-k)LRMES_{it}-1]\\)\n\n\\(W_{it}\\)는 주식 시가총액\n\\(k\\)는 자산대비 적정 자기자본 비율\n\\(LVG_{it}\\)는 자산대비 자기자본 비율 \\(A_{it}/W_{it}\\)\n\\(LRMES_{it}\\)는 시스템리스크가 발생하였을 때 개별 금융기관의 다기간 주가수익률 하락폭의 기댓값\n\n금융시스템의 불안정 수준: 개별 SRISK를 합산\n\n\\(SRISK_{t} \\equiv \\sum^N_{i=1} max(SRISK_{it},0)\\)\n\n개별 금융기관의 위험수준\n\n\\(SRISK\\%_{it} = \\frac{SRISK_{it}}{SRISK_{t}}\\)\n\nLRMES 측정 방법: GARCH-DCC 모형을 이용\n\n개별 금융기관 주가수익률 \\(r_{it}\\)과 시장수익률 \\(r_{mt}\\) 간의 관계를 GARCH-DCC 모형을 이용하여 구성 \\[\\begin{equation*}\n\\begin{bmatrix}\nr_{it} \\\\\nr_{mt}\n\\end{bmatrix} \\sim\n\\begin{pmatrix}\n  0, & \\begin{bmatrix}\n  \\sigma_{it}^2 & \\rho_{it}\\sigma_{it}\\sigma_{mt} \\\\\n  \\rho_{it}\\sigma_{it}\\sigma_{mt} & \\sigma_{mt}^2\n\\end{bmatrix}\n\\end{pmatrix}\n\\end{equation*}\\]\n여기서 변동성은 GJR-GARCH 모형을 이용하여 다음과 같이 가정\n\n\\(\\sigma_{it} = \\omega_{v_i}+\\alpha_{v_i}r^2_{it-1}+\\gamma_{v_i}r^2_{it-1}\\Gamma_{it-1}+\\beta_{v_i}\\sigma^2_{it-1}\\)\n\n\\(\\sigma_{mt} = \\omega_{v_m}+\\alpha_{v_m}r^2_{mt-1}+\\gamma_{v_m}r^2_{mt-1}\\Gamma_{mt-1}+\\beta_{v_i}\\sigma^2_{mt-1}\\)\n여기서 \\(\\Gamma_{mt-1}\\)은 \\(r_{it-1}\\)이 음(-)인 경우에 1을, 아닌 경우에 0을 가지는 변수\n\n\nDCC 모형을 위해 변동성으로 표준화한 수익률을 \\(\\epsilon_{it}\\equiv \\frac{r_{it}}{\\sigma_{it}}, \\epsilon_{mt}\\equiv \\frac{r_{mt}}{\\sigma_{mt}}\\)로 정의하여 상관계수 행렬을 구하면, \\[\\begin{equation*}\n    R_t \\equiv\n    \\begin{bmatrix}\n     1 & \\rho_{it} \\\\\n     \\rho_{it} & 1\n    \\end{bmatrix} =\n    diag(Q_{it})^{-1/2}Q_{it}diag(Q_{it})^{-1/2}, \\\\\n\\end{equation*}\\] \\[\\begin{equation*}\n    Q_{it} = (1-\\alpha_{C_i}-\\beta_{C_i})S_i+\\alpha_{C_i}\n    \\begin{bmatrix}\n     \\epsilon_{it-1} \\\\\n     \\epsilon_{mt-1}\n    \\end{bmatrix}\n    \\begin{bmatrix}\n     \\epsilon_{it-1} \\\\\n     \\epsilon_{mt-1}\n    \\end{bmatrix}' + \\beta_{C_i}Q_{it-1}\n\\end{equation*}\\]\n\n\\(S_i\\)는 \\(\\epsilon_{it}, \\epsilon_{mt}\\)간의 무조건부 상관계수행렬\n\n2단계 QML 방법을 이용하여 모형 추정, 시뮬레이션 방법을 이용하여 LRMES에 대한 예측치를 산출\nSES 지수는 역사적으로 시스템리스크를 경험한 사례가 있어야 추정이 가능하지만, SRISK는 시스템리스크를 경험한 사례가 없더라도 그 추정이나 예측이 가능하다는 장점이 있음\n\n\n\n\nBrownless and Engle (2010) Figure 1\n\n\n\n\n\nBrownless and Engle (2010) Figure 3"
  },
  {
    "objectID": "posts/systemic_risk/2-3_Systemic_Risk_Measures_Cross-sectional.html#structural-garch",
    "href": "posts/systemic_risk/2-3_Systemic_Risk_Measures_Cross-sectional.html#structural-garch",
    "title": "시스템 리스크 분석: (2-3) 금융기관 단면적 위험 측정방법",
    "section": "Structural GARCH",
    "text": "Structural GARCH\nEngle and Siriwardane(2014)\n\n“레버리지” 효과를 반영할 수 있도록 GARCH 모형을 수정한 Structural GARCH 모형을 제안\n\n금융기관의 자산수익률에 부정적인 충격이 발생하면 이로 인해 비대칭적으로 변동성이 증가할 뿐만 아니라 레버리지(즉, 자본 대비 부채 비율)가 증가하며 이 효과로 인해 자본의 수익률에 대한 변동성이 추가로 증가하기 때문\n\n주요 식\n\n\\(r_{Et}=LM_{t-1}r_{At},\\)\n\\(r_{At}=\\sqrt{h_{At}\\epsilon_{At}}, \\epsilon_{At} \\sim (0,1)\\)\n\\(h_{At}=\\omega + \\alpha \\left( \\frac{r_{Et-1}}{LM_{t-2}} \\right)^2 + \\gamma \\left( \\frac{r_{Et-1}}{LM_{t-2}} \\right)^2 I_{r_{E_{t-1}}}+\\beta h_{At-1},\\)\n\\(LM_{t-1} = \\left[\\Delta_{t-1}^{BSM}\\times g^{BSM}\\left(\\frac{E_{t-1}}{D_{t-1}},1,\\sigma^{f}_{At-1},\\tau\\right)\\times \\frac{E_{t-1}}{D_{t-1}}\\right]^\\psi\\)\n\n변수 설명\n\n\\(r_{Et}\\)는 주식 수익률, \\(r_{A}\\)는 관측불가능한 (시장가치) 자산 수익률\n변동성은 GJR GARCH 과정을 따름\n\\(LM\\)은 레버리지 효과\n\\(\\Delta^{BSM}\\)은 Black-Scholes-Merton 옵션모형에 따른 옵션델타\n\\(g^{BSM}( \\ \\bullet \\ )\\)은 콜옵션가격함수의 역함수\n\n주가의 변동성은 자산변동성 뿐만 아니라 레버리지 효과를 나타내는 \\(LM\\)에도 영향을 받음\n\n\\(vol_t \\left( \\frac{dE_t}{E_t} \\right) = LM_t \\sigma_{At}\\)\n\n레버리지 효과를 도입하면 MES 추정치가 증가하는 효과가 있음\n또한, Engle and Siriwardane(2014)는 새로운 시스템리스크 추정지표로서 예비적 자본규모(precautionary capital)을 제안\n\n예비적 자본규모는 시스템리스크 사건이 발생하더라도 해당 금융기관의 자본이 부족하지 않도록 하는 적정 자본규모와 현재의 자본규모와의 차이\n\n\n\n\n\nEngle, Siriwardane (2014) Figure4A\n\n\n\n\n\nEngle, Siriwardane (2014) Figure6A"
  },
  {
    "objectID": "posts/systemic_risk/2-3_Systemic_Risk_Measures_Cross-sectional.html#cimdo-consistent-multivariate-density-optimizing",
    "href": "posts/systemic_risk/2-3_Systemic_Risk_Measures_Cross-sectional.html#cimdo-consistent-multivariate-density-optimizing",
    "title": "시스템 리스크 분석: (2-3) 금융기관 단면적 위험 측정방법",
    "section": "CIMDO: Consistent multivariate density optimizing",
    "text": "CIMDO: Consistent multivariate density optimizing\nSegoviano & Goodhart (2009)\n개별 금융기관들의 위험 확률(PoD; probability of distress)들로부터 CIMDO(consistent multivariate density optimizing) 기법을 활용하여 개별 금융기관들간의 위험 사건에 대한 결합분포를 구하고 이를 이용하여 여러 금융 불안정 지수들을 제안\n\n설명을 위한 가정:\n\n두 개의 금융기관 X와 Y, 금융기관 주가의 로그 수익률 각각 \\(x,y\\)\n\\(q(x,y)\\)는 두 수익률에 대한 사전(prior) 분포, \\(p(x,y)\\)는 두 수익률에 대한 사후(posterior) 분포\n\n사후 분포는 두 분포의 cross-entropy를 최소화하도록 정함\n\nCIMDO 목적함수: \\(C[p,q]= \\int\\int p(x,y) \\ln \\left[\\frac{p(x,y)}{q(x,y)}\\right]dxdy\\)\n\n\\(x\\)(\\(y\\))가 임계치 \\(d_x\\)(\\(d_y\\))를 하회하면 금융기관 X(Y)에 위험 사건이 발생하는 것으로 가정\n\n데이터로부터 경험적으로 추정한 위험 확률 \\(PoD_x\\)와 사후 분포는 다음과 같은 조건을 만족시켜야 사후 분포가 데이터와 일관성을 유지할 수 있음\n\\(\\int^{d_{x(y)}}_{-\\infty}\\int^{\\infty}_{-\\infty} p(x,y)dxdy=PoD_{x(y)}\\)\n\\(\\int\\int p(x,y)dxdy=1\\)\n\n이러한 조건을 만족시키면서 CIMDO 목적함수를 최소화시키는 사후 결합밀도함수\n\n\\(p(x,y)=q(x,y)\\exp\\{-[1+\\mu+\\lambda_1I_{(-\\infty,d_x)}+\\lambda_2I_{(-\\infty,d_y)}]\\}\\)\n\\(\\lambda_1\\)과 \\(\\lambda_2\\)는 각각 \\(PoD_x\\)와 \\(PoD_y\\)에 대한 일치성 조건에 관한 라그랑지 승수\n\\(\\mu\\)는 확률밀도함수의 면적이 1이라는 조건에 대한 라그랑지 승수\n\n\n\nSegoviano and Goodhart(2009)는 결합확률밀도함수를 이용하여 다양한 금융시스템 불안정 지수를 제시\nJPoD - 금융시스템 내의 모든 금융기관들에서 동시에 위험 사건이 발생할 확률 (세 금융기관 X,Y,Z가 존재하는 경우) - \\(JPoD\\equiv \\int^{d_x}_{-infty}\\int^{d_y}_{-infty}\\int^{d_z}_{-infty}p(x,y,z)dxdydz\\)\n\n\n\nSegoviano and Goodhart (2009) Figure 4\n\n\nBSI(banking stability index) - 적어도 하나의 금융기관이 위험할 때 위험 금융기관 수의 기댓값 (세 금융기관 X,Y,Z가 존재하는 경우) - \\(BSI= \\frac{P(X\\leq d_x)+P(Y\\leq d_y)+P(Z\\leq d_z)}{1-P(X&gt;d_x,Y&gt;d_y,Z&gt;d_z)}\\)\n\n\n\nSegoviano and Goodhart (2009) Figure 5\n\n\nPAO(probability that at least one bank becomes distressed)\n\n특정 금융기관이 위험한 상황일 때 다른 금융기관 중 적어도 하나의 금융기관에서 위험 사건이 발생할 확률\n\n\n\n\nSegoviano and Goodhart (2009) Figure 7\n\n\nDDM(distress dependence matrix)\n\n\\(N\\)개의 금융기관이 존재할 때 \\(N\\times N\\) 행렬로서 그 \\((i,j)\\)번째 원소는 \\(j\\)번째 금융기관이 위험할 때 \\(i\\)번째 금융기관이 위험할 조건부확률로 위험 사건에 대한 금융기관간 연계성에 대한 정보를 나타냄"
  },
  {
    "objectID": "posts/systemic_risk/2-1_Systemic_Risk_Measures_Macro.html",
    "href": "posts/systemic_risk/2-1_Systemic_Risk_Measures_Macro.html",
    "title": "시스템 리스크 분석: (2-1) 거시경제지표를 활용한 경제전체 위험도 측정 방법",
    "section": "",
    "text": "서상원 (2018)\n시스템리스크 측정 방법 중 거시경제지표를 활용한 경제전체 위험도를 살펴본다."
  },
  {
    "objectID": "posts/systemic_risk/2-1_Systemic_Risk_Measures_Macro.html#fcycl-financial-cycle",
    "href": "posts/systemic_risk/2-1_Systemic_Risk_Measures_Macro.html#fcycl-financial-cycle",
    "title": "시스템 리스크 분석: (2-1) 거시경제지표를 활용한 경제전체 위험도 측정 방법",
    "section": "FCYCL: Financial Cycle",
    "text": "FCYCL: Financial Cycle\nDrehmann, Borio and Tsatsaronis(2012)\n실물부문에서 호황과 불황이 교대로 나타나는 경기변동이 있듯이 금융부문에서도 그와 유사한 변동으로 금융변동을 제시\n\n금융사이클 산출을 위한 변수 5개\n\n민간신용\nGDP 대비 신용 비율\n주가\n주택가격\n(주거용, 상업용 부동산 가격, 주가 등) 종합자산 가격\n\nBand-pass 필터: 5개의 금융변수 각각에 대해 특정주기에 대한 정보만을 추출 (단기 사이클 1-8년 / 중기 사이클 8-30년)\n전환점 분석법(turning-point analysis): 사이클의 정점(peak)과 저점(trough)을 특정\n\n단기 사이클\n\n매 분기마다 그 분기를 중심으로 전후 5분기 범위에서 그 분기의 값이 가장 크면 그 분기를 국지적 극대, 가장 작으면 국지적 극소로 정의\n이 국지적 극대와 극소 중에서 각 사이클의 최소 길이가 5분기 보다 길고 확장 또는 수축 국면의 기간이 최소 2분기 이상인 경우를 정점 또는 저점으로 지정\n\n중기 사이클\n\n전후 9분기 범위에서 국지적 극대와 극소를 정하고 사이클의 최소 길이는 8년으로 정한다. 전환점 분석법을 금융사이클 및 개별지 수에 적용하여 각각의 정점과 저점들을 정한 후\n금융사이클의 정점(저점)을 중심으로 개별 정점(저점)들이 6분기 이내에 있으면 정규(regular) 전환점으로, 그리고 6~12분기 이내에 있으면 약(weak) 전환점이라고 정의\n\n\n실험: 1960~2011년 기간동안 7개국에 대해 적용\n\n분석 결과: 금융사이클과 실물 경기변동간에 긴밀한 관계가 있음. 특히, 금융사이클 수축국면에 실물의 경기하락이 겹칠 경우에는 상대적으로 심한 경기하락을 경험. 이러한 분석결과는 금융사이클로 인한 추가적인 경기하락을 방지하고 경기변동을 완화하기 위해서는 금융 사이클을 함께 고려하여야 한다는 점을 시사.\n\n\n\n\nDrehmann, Borio, and Tsatsaronis (2012) Figure 1"
  },
  {
    "objectID": "posts/systemic_risk/2-1_Systemic_Risk_Measures_Macro.html#ccycl-credit-cycle",
    "href": "posts/systemic_risk/2-1_Systemic_Risk_Measures_Macro.html#ccycl-credit-cycle",
    "title": "시스템 리스크 분석: (2-1) 거시경제지표를 활용한 경제전체 위험도 측정 방법",
    "section": "CCYCL: Credit cycle",
    "text": "CCYCL: Credit cycle\nJuselius and Drehmann(2015)\n\nGDP대비 신용 비율이 담고 있는 정보를 활용: 지속가능 부분(sustainable)과 지속가능하지 않은 부분(unsustainable)으로 구분\n\nsustainable: 레버리지와 원리금부담(debt service burden)이 GDP대비 신용 비율과 장기적 관계를 가질 것이라고 가정하고 GDP대비 신용 비율 중에서 이 장기적 관계에 의해 설명되는 부분\n\n\\({slev}_t = constant + \\sum^{b}_{i=1} \\psi_i (p_{A_i,t}-p_t)\\)\n\n\\({slev}_t\\)는 레버리지를 감안한 지속가능한 GDP대비 신용 비율(로그), \\(p_{A_i,t}\\)는 \\(i\\)번째 자산가격(로그), \\(p_t\\)는 물가지수(로그), \\(\\sum^{b}_{i=1} \\psi_i = 1\\) 제약\n\n${sleb}_t = constant + r_t $\n\n\\({sleb}_t\\)는 원리금부담을 감안한 지속가능한 GDP대비 신용 비율(로그), \\(r_t\\)는 신용잔고에 대한 평균 명목대출이자율\n\n두 가지의 장기균형관계를 공적분(co-integration) 방법으로 추정\n실험: 미국 글로벌 금융위기 이전 기간 케이스\n분석 결과:\n\n실제 GDP대비 신용 비율이 레버리지를 감안한 지속가능한 GDP대비 신용 비율에 비해 낮게 나타남 \\(\\rightarrow\\) 높은 레버리지를 가지고 주택시장에 신규진입\n실제 GDP대비 신용 비율은 원리금부담을 감안한 지속가능한 GDP대비 신용 비율에 비해 높아짐 \\(\\rightarrow\\) 원리금부담이 높아지면 소비 및 투자 지출을 감소\n\n\n\n\n\nJuselius & Drehmann (2015) Figure 1"
  },
  {
    "objectID": "posts/systemic_risk/2-1_Systemic_Risk_Measures_Macro.html#ews-se-early-warning-system-signal-extraction",
    "href": "posts/systemic_risk/2-1_Systemic_Risk_Measures_Macro.html#ews-se-early-warning-system-signal-extraction",
    "title": "시스템 리스크 분석: (2-1) 거시경제지표를 활용한 경제전체 위험도 측정 방법",
    "section": "EWS-SE (Early warning system-Signal extraction)",
    "text": "EWS-SE (Early warning system-Signal extraction)\nKaminsky, Lizondo and Reinhart(1998)\n신호접근법(Signal extraction approach)는 여러 정보변수들을 비모수적 방법으로 활용하여 금융위기의 발생가능성을 예측하는 방법\n\n각 국가별로 먼저 위기에 대한 조작적 정의(operational definition)를 하고 그에 기반하여 위기를 나타내면 1을, 아니면 0을 갖는 더미변수 \\(S_{j,t}\\)를 구성 (\\(j\\)는 국가, \\(t\\)는 시간, \\(n\\)은 n번째 정보변수) - 정보변수는 거시경제변수 등\n\n\n\n\n\ncrisis\nno-crisis\n\n\nsignal\n\\(A^j_{n,t}\\)\n\\(B^j_{n,t}\\)\n\n\nno-signal\n\\(C^j_{n,t}\\)\n\\(D^j_{n,t}\\)\n\n\n\n\nin-sample 기간의 경우 합산하여 \\(A^j_{n}, B^j_{n}, C^j_{n}, D^j_{n}\\) 구함\nNSR(Noise-to-signal ratio)\n\n\\(w^j_n = \\frac{B^j_{n}}{B^j_{n}+D^j_{n}} \\div \\frac{A^j_{n}}{A^j_{n}+C^j_{n}}\\)\n예측이 정확할수록 0에 가까움\nNSR이 최소가 되도록 각 정보변수별 임계치를 설정\n\n여러 정보변수 중에서 몇 개가 위기신호가 발생하는지를 각 시점별로 관측\n\n\\(K^j_{1t} \\equiv \\sum^{N}_{n=1}I^j_{nt}\\)\n\\(I^j_{nt}\\)는 n번째 정보변수가 위기신호를 보내는지의 더미변수\n이러한 종합지수들에 대해 NSR이 최소가 되는 종합지수의 임계치를 설정\n\n외표본기간에서도 위기발생에 대해 높은 예측력을 보이는지를 점검"
  },
  {
    "objectID": "posts/systemic_risk/2-1_Systemic_Risk_Measures_Macro.html#ews-lr-early-warning-system-logit-regression",
    "href": "posts/systemic_risk/2-1_Systemic_Risk_Measures_Macro.html#ews-lr-early-warning-system-logit-regression",
    "title": "시스템 리스크 분석: (2-1) 거시경제지표를 활용한 경제전체 위험도 측정 방법",
    "section": "EWS-LR (Early warning system-Logit regression)",
    "text": "EWS-LR (Early warning system-Logit regression)\n\n로짓모형은 위기발생 확률을 직접 제시해줌. 그러나 통상 위기발생 확률을 직접 사용하기 보다는 그 확률을 이용하여 NSR을 구하고 내표본기간의 NSR을 최소화하는 임계치를 설정\n위기발생 확률이 임계치를 상회하는 경우 위기신호가 발생하는 방식으로 로짓모형을 조기경보 목적에 활용"
  },
  {
    "objectID": "posts/systemic_risk/2-1_Systemic_Risk_Measures_Macro.html#ews-hybrid-early-warning-system-hybrid",
    "href": "posts/systemic_risk/2-1_Systemic_Risk_Measures_Macro.html#ews-hybrid-early-warning-system-hybrid",
    "title": "시스템 리스크 분석: (2-1) 거시경제지표를 활용한 경제전체 위험도 측정 방법",
    "section": "EWS-Hybrid (Early warning system-Hybrid)",
    "text": "EWS-Hybrid (Early warning system-Hybrid)\nSuh(2017)\n조기경보모형으로 널리 사용되고 있는 신호접근법과 로짓모형의 각기 장단점을 서로 보완적으로 활용\n\n먼저 여러 정보변수들을 그 경제적 내용에 따라 소수의 하위부문으로 구분\n\n급작스런 자본유입의 중단 현상의 예측에 대해 17개의 개별 정보변수를 4개의 하위부문(거시경제, 금융, 대외부문 및 해외경제)으로 분류\n각 국가간 차이를 나타내는 변수 중에서 시기별로 큰 변동을 보이지 않는 5개의 변수를 따로 국가간 차이 변수로서 추가\n\n4개의 하위부문에 대해 각각 신호접근법을 적용하여 하위종합지수를 구성\n이렇게 구성된 4개의 하위종합지수와 5개의 국가간 차이변수 등 9개의 정보변수를 로짓 예측모형에 적용"
  },
  {
    "objectID": "posts/ml_trading/7_Enhancing_and_Bet_Confidence.html",
    "href": "posts/ml_trading/7_Enhancing_and_Bet_Confidence.html",
    "title": "머신러닝을 이용한 트레이딩: (7) 매매 신뢰도 측정과 전략 강화",
    "section": "",
    "text": "전략 강화 모형\n\ninputs\n\n라벨: 매매 규칙에 따른 전략의 결과 = 각 매매 성공/실패 여부\n\nmodels: SVM, Random Forest, Gradient Boosting, LSTM\noutputs\n\n매매 신뢰도 (bet confidence)\n\n매매 신뢰도를 이용한 전략 강화\n\n\n# lib\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns;sns.set()\nplt.style.use('tableau-colorblind10')\n\n# different models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, normalize\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score\n\n# homemade\nfrom feature_engineering import dimension_reduction as DR\nfrom features import tautil\nfrom labeling import labeling\nfrom backtest import round_trip\nfrom triple_barrier import make_rt\n\nfrom mlutil.pkfold import PKFold\n\n\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n\nget X,y\n\nmarket_df = pd.read_csv('C:data/market_samsung.csv')\nmarket_df = market_df.rename(columns={market_df.columns[0]:'Date'})\nmarket_df.index = pd.to_datetime(market_df.Date)\nmarket_df.drop(columns='Date',inplace=True)\nmarket_df.dropna(inplace=True)\nclose = market_df.close['2010':'2020']\n\nfeature_df = pd.read_csv('C:data/features_samsung.csv')\nfeature_df = feature_df.rename(columns={feature_df.columns[0]:'Date'})\nfeature_df.index = pd.to_datetime(feature_df.Date)\nfeature_df.drop(columns='Date',inplace=True)\nfeature_df.dropna(inplace=True)\n\nselected_features = pd.read_csv('C:data/selected_features.csv').columns[1:]\n\n\nfeature = feature_df.dropna()\nfeature = feature[selected_features]\nsc = StandardScaler()\nX_sc = sc.fit_transform(feature)\nX_sc = pd.DataFrame(X_sc, index=feature.index, columns=feature.columns)\n\n\n#benchmark\nbarrier_bm = pd.read_csv('C:data/barrier_bm.csv')\nbarrier_bm.index = pd.to_datetime(barrier_bm.Date)\nbarrier_bm.exit = pd.to_datetime(barrier_bm.exit)\nbarrier_bm.drop(columns='Date',inplace=True)\n\n\n#labeling\nbarrier = pd.read_csv('C:data/barrier.csv')\nbarrier.index = pd.to_datetime(barrier.Date)\nbarrier.exit = pd.to_datetime(barrier.exit)\nbarrier.drop(columns='Date',inplace=True)\n\nrts = make_rt(close,barrier.dropna())\noutcome = rts.rt_returns\noutcome.index = rts.open_dt\n\n\n#meta-label\nwl = np.sign(np.sign(outcome)+1)\ny_ = wl\ny_.value_counts()\n\n1.0    608\n0.0    421\nName: rt_returns, dtype: int64\n\n\n\nloss = wl.value_counts()[0]\nwin = wl.value_counts()[1]\nplt.figure(figsize=(10,3))\nplt.scatter(wl[wl==1].index,close.loc[wl[wl==1].index], alpha=0.5)\nplt.scatter(wl[wl==0].index,close.loc[wl[wl==0].index], marker='x', alpha=0.5)\nplt.legend(['win 1','lose 0'])\nplt.title('y (meta-label): win {}, lose {}'.format(win,loss))\nplt.show()\n\n\n\n\n\nraw_X = X_sc.copy()\ntmp = raw_X.join(y_).dropna()\nX=tmp.iloc[:,:-1]\ny=tmp.iloc[:,-1]\n\n\n\nModel Construction\n\n# Choose model\n\n# Cross Validation (k-fold)\nn_cv=4\nt1 = pd.to_datetime(barrier.exit.loc[X.index])\ncv = PKFold(n_cv,t1,0)\n\n\n# Choose model (SVM-rbf)\nC = [0.1, 1,10]\nparam_grid_rbf = dict(C=C)\nsvc_rbf = SVC(kernel='rbf', probability=True)\ngs_svc_rbf = GridSearchCV(estimator=svc_rbf, param_grid= param_grid_rbf, cv=cv, scoring='precision')\ngs_svc_rbf.fit(X,y)\nsvc_best = gs_svc_rbf.best_estimator_\nsvc_best\n\nSVC(C=10, probability=True)\n\n\n\nn_estimators = [200,1000]\n#max_depth = [3,7]\nparam_grid_rfc = dict(n_estimators=n_estimators)\nrfc = RandomForestClassifier()\ngs_rfc = GridSearchCV(estimator=rfc, param_grid= param_grid_rfc, cv=cv, scoring='precision')\ngs_rfc.fit(X,y)\nrfc_best = gs_rfc.best_estimator_\nrfc_best\n\nRandomForestClassifier(n_estimators=200)\n\n\n\nn_estimators_ab = [50,100]\nlearning_rate = [1,0.1]\nparam_grid_abc = dict(n_estimators=n_estimators_ab, learning_rate=learning_rate)\n\nabc=AdaBoostClassifier()\ngs_abc = GridSearchCV(estimator=abc, param_grid= param_grid_abc, cv=cv, scoring='precision')\ngs_abc.fit(X,y)\nada_best = gs_abc.best_estimator_\nada_best\n\nAdaBoostClassifier(learning_rate=1, n_estimators=100)\n\n\n\nn_estimators_gb = [100,200]\nlearning_rate = [0.1,0.01]\nparam_grid_gbc = dict(n_estimators=n_estimators_gb, learning_rate=learning_rate)\ngbc=GradientBoostingClassifier()\ngs_gbc = GridSearchCV(estimator=gbc, param_grid= param_grid_gbc, cv=cv, scoring='precision')\ngs_gbc.fit(X,y)\ngbc_best = gs_gbc.best_estimator_\ngbc_best\n\nGradientBoostingClassifier(learning_rate=0.01, n_estimators=200)\n\n\n\n\nModel\n\nclf_list = [svc_best, rfc_best, ada_best, gbc_best]\nestimators=['SVM_best','RF_best','AdaBoost_best','GradientBoost_best']\nscores_list = []\ny_preds_list = []\ny_probs_list = []\n\n# for ML model prediction\nfor clf in clf_list:\n    y_preds_ = []\n    y_probs_ = []\n\n    for train, test in cv.split(X, y):\n        clf.fit(X.iloc[train], y.iloc[train])\n        y_true = y.iloc[test]\n        y_pred = clf.predict(X.iloc[test])\n        y_probs = clf.predict_proba(X.iloc[test])\n        y_probs = y_probs[:, 1]\n        y_pred_series = pd.Series(y_pred,index=y[test].index)\n        y_probs_series = pd.Series(y_probs,index=y[test].index)\n        y_preds_.append(y_pred_series)\n        y_probs_.append(y_probs_series)\n    \n    \n    y_preds__ = pd.concat([i for i in y_preds_])\n    y_probs__ = pd.concat([i for i in y_probs_])\n    y_true__ = y.loc[y_preds__.index]\n    accs = accuracy_score(y_true__, y_preds__)\n    f1=f1_score(y_true__, y_preds__)\n    roc=roc_auc_score(y_true__, y_probs__)\n    prec=precision_score(y_true__, y_preds__)\n    score = [accs, f1, roc, prec]\n    scores_list.append(score)\n    y_preds_list.append(y_preds__)\n    y_probs_list.append(y_probs__)\n\n\nresults = pd.DataFrame(scores_list, columns=['accuracy','f1 score','roc auc score','precision score'],index=estimators)\nresult_show = results.sort_values('precision score', ascending=False)\n\n\nresult_show\n\n\n\n\n\n\n\n\naccuracy\nf1 score\nroc auc score\nprecision score\n\n\n\n\nAdaBoost_best\n0.567541\n0.631927\n0.552471\n0.635607\n\n\nSVM_best\n0.544218\n0.585323\n0.574228\n0.632887\n\n\nRF_best\n0.549077\n0.657817\n0.537073\n0.596257\n\n\nGradientBoost_best\n0.519922\n0.609177\n0.490364\n0.586890\n\n\n\n\n\n\n\n\ny_probs_df = pd.DataFrame()\nfor i in range(len(estimators)):\n    y_probs_df[estimators[i]] = y_probs_list[i]\n\n\n#평균\npred_prob = pd.Series(y_probs_df.mean(axis=1),index=y_probs_df.index)\n\n#하나하나\n\n#y_probs_df_2 = y_probs_df[estimators[3]]\n#pred_prob = pd.Series(y_probs_df_2,index=y_probs_df_2.index)\n\n\npred_prob2=pd.Series(normalize(pred_prob.to_frame().T).reshape(-1,), index=y_probs_df.index).rename('bet_confidence')\n\n\nbet_confidence=pd.Series(MinMaxScaler().fit_transform(pred_prob2.to_frame()).reshape(-1,), index=y_probs_df.index).rename('bet_confidence')\n\n\nplt.title('Bet confidence distribution')\nplt.hist(bet_confidence, bins=30)[2]\nplt.xlabel('Bet confidence')\nplt.ylabel('counts')\n\nText(0, 0.5, 'counts')\n\n\n\n\n\n\nc = close.loc[bet_confidence.index]\nplt.figure(figsize=(10,5))\nplt.title('Bet confidence')\nplt.plot(close, alpha=0.1)\nplt.scatter(c.index,c, c = bet_confidence, s=20,cmap='vlag',vmin=0,vmax=1)\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\nAlgo Trading Backtest\n\nbarrier_bm = barrier_bm.dropna()\nbarrier_before = barrier.loc[bet_confidence.index].dropna()\nbarrier_enhanced = barrier_before.loc[bet_confidence.loc[bet_confidence&gt;0.5].index]\n\n\nrts_bm = make_rt(close,barrier_bm)\nrts_before = make_rt(close,barrier_before)\nrts_enhanced = make_rt(close,barrier_enhanced)\n\n\nresult1 = pd.concat([round_trip.get_df_ann_sr(rts_bm,'Benchmark',years=11),\n                    round_trip.get_df_ann_sr(rts_before,'Trading Strategy (Primary)',years=11)],axis=1)\n\ndf_sr = round_trip.get_df_ann_sr(rts_enhanced,'Enhanced Trading Strategy (Second)',years=11)\nresult1 = result1.join(df_sr)\n\n\nresult1\n\n\n\n\n\n\n\n\nBenchmark\nTrading Strategy (Primary)\nEnhanced Trading Strategy (Second)\n\n\n\n\navg_n_bets_per_year\n246.272727\n93.545455\n49.636364\n\n\nwin_ratio\n0.520506\n0.590467\n0.612844\n\n\nannualized_sharpe_ratio\n0.538232\n1.525995\n1.623284\n\n\n\n\n\n\n\n\nresult2 = pd.concat([round_trip.get_df_ann_sr(rts_bm,'Benchmark',years=11),\n                    round_trip.get_df_ann_sr(rts_before,'Trading Strategy (Primary)',years=11)],axis=1)\nwinr = []\nfor i in np.linspace(0.1,0.9,9):\n    barrier_enhanced_ = barrier_before.loc[bet_confidence.loc[bet_confidence&gt;=i].index]\n    rts_enhanced_ = make_rt(close,barrier_enhanced_)\n    df_sr = round_trip.get_df_ann_sr(rts_enhanced_,'b',years=11)\n    winr.append(df_sr.T.win_ratio[0])\n\n\ndict_ = dict(zip(np.linspace(0.1,0.9,9).round(2),winr))\n\n\ndf_res = pd.DataFrame.from_dict(dict_,orient='index')\nplt.figure(figsize=(10,5))\nplt.title(\"Hit-ratio of different thresholds strategy\")\nplt.bar(df_res.index, df_res[0], width=0.05)\nplt.plot(df_res)\nplt.ylabel('win ratio')\nplt.xlabel('bet confidence threshold')\nplt.ylim(0.5,0.8)\nplt.show()\n\n\n\n\n매매신뢰도의 전략 강화 결과 win ratio가 상승했으며, 신뢰도의 임계치에 따라 결과가 다른데, 임계치를 높이할수록 결과가 좋다."
  },
  {
    "objectID": "posts/ml_trading/5_Get_Trading_Signals.html",
    "href": "posts/ml_trading/5_Get_Trading_Signals.html",
    "title": "머신러닝을 이용한 트레이딩: (5) 매매 시그널 분류",
    "section": "",
    "text": "모멘텀 분류기 (Momentum Classifier)\n\ninputs\n\nlabels: trend-scanning labeling (up vs. down & no trend)\nfeatures: market-data selected features\n\nmodels: SVM, Random Forest, Gradient Boosting, LSTM\noutputs\n\nmomentum signals\n\n\n\n# lib\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns;sns.set()\nplt.style.use('tableau-colorblind10')\n\n# different models\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score\n\nfrom features import tautil\nfrom labeling import labeling\nfrom mlutil.pkfold import PKFold\n\n\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n\nget X,y\n\nmarket_df = pd.read_csv('C:data/market_samsung.csv')\nmarket_df = market_df.rename(columns={market_df.columns[0]:'Date'})\nmarket_df.index = pd.to_datetime(market_df.Date)\nmarket_df.drop(columns='Date',inplace=True)\nmarket_df.dropna(inplace=True)\n\nfeature_df = pd.read_csv('C:data/features_samsung.csv')\nfeature_df = feature_df.rename(columns={feature_df.columns[0]:'Date'})\nfeature_df.index = pd.to_datetime(feature_df.Date)\nfeature_df.drop(columns='Date',inplace=True)\nfeature_df.dropna(inplace=True)\n\n\nselected_features = pd.read_csv('C:data/selected_features.csv').columns[1:]\n\n\nfeature = feature_df.dropna()\nfeature = feature[selected_features]\n\nsc = StandardScaler()\nX_sc = sc.fit_transform(feature)\nX_sc = pd.DataFrame(X_sc, index=feature.index, columns=feature.columns)\n\n\nfor i in feature.columns:\n    plt.figure(figsize=(10,1))\n    plt.title(i)\n    plt.plot(feature[i])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#labeling\ntrend_scanning_window = 60\ntrend_scanning_q = 3\nts_out = labeling.trend_scanning_label(market_df['2010':].close, window = trend_scanning_window, q = trend_scanning_q)\nmom_label = ts_out[0]\n\n\ny = np.sign(mom_label-1)+1 # up-trend vs. others(down-trend and no-trend)\n\n\ny_ = y[:'2020']\nclose = market_df.close.loc[y_.index]\nf, (a0, a1) = plt.subplots(2, gridspec_kw={'height_ratios': [5, 1]}, figsize=(15,5))\nf.suptitle(\"Trend Scanning Labels: 1(up-trend), 0(down & no-trend)\")\na0.plot(close,alpha=0.2)\na0.scatter(close.index,close,c=y_, cmap='vlag')\na1.plot(y_.fillna(0.5))\nf.show()\n\n\n\n\n\nraw_X = X_sc.copy()\n\ntmp = raw_X.join(y).dropna()\nX=tmp.iloc[:,:-1]\ny=tmp.iloc[:,-1]\n\n\n\nModels & Models with Hyperparameter tuning\n\n# Cross Validation (purged k-fold)\nn_cv=4\nt1= ts_out[1].loc[X.index]\ncv = PKFold(n_cv,t1,0.01)\n\n\n# Choose model (SVM-rbf)\nC = [0.1, 1, 10]\nparam_grid_rbf = dict(C=C)\nsvc_rbf = SVC(kernel='rbf', probability=True)\ngs_svc_rbf = GridSearchCV(estimator=svc_rbf, param_grid= param_grid_rbf, cv=cv)\ngs_svc_rbf.fit(X,y)\nsvc_best = gs_svc_rbf.best_estimator_\nsvc_best\n\nSVC(C=0.1, probability=True)\n\n\n\nn_estimators = [500,1000]\nmax_depth = [3,5]\nparam_grid_rfc = dict(n_estimators=n_estimators, max_depth=max_depth)\nrfc = RandomForestClassifier(class_weight='balanced')\ngs_rfc = GridSearchCV(estimator=rfc, param_grid= param_grid_rfc, cv=cv)\ngs_rfc.fit(X,y)\nrfc_best = gs_rfc.best_estimator_\nrfc_best\n\nRandomForestClassifier(class_weight='balanced', max_depth=5, n_estimators=1000)\n\n\n\nn_estimators_ab = [200,500]\nlearning_rate = [0.01,0.1]\nparam_grid_abc = dict(n_estimators=n_estimators_ab, learning_rate=learning_rate)\n\nabc=AdaBoostClassifier()\ngs_abc = GridSearchCV(estimator=abc, param_grid= param_grid_abc, cv=cv)\ngs_abc.fit(X,y)\nada_best = gs_abc.best_estimator_\nada_best\n\nAdaBoostClassifier(learning_rate=0.01, n_estimators=200)\n\n\n\nn_estimators_gb = [200,500]\nlearning_rate = [0.01,0.1]\nparam_grid_gbc = dict(n_estimators=n_estimators_gb, learning_rate=learning_rate)\ngbc=GradientBoostingClassifier()\ngs_gbc = GridSearchCV(estimator=gbc, param_grid= param_grid_gbc, cv=cv)\ngs_gbc.fit(X,y)\ngbc_best = gs_gbc.best_estimator_\ngbc_best\n\nGradientBoostingClassifier(n_estimators=200)\n\n\n\nclf_list = [svc_best, rfc_best, ada_best, gbc_best]\nestimators=['SVM_best','RF_best','AdaBoost_best','GradientBoost_best']\nscores_list = []\ny_preds_list = []\ny_probs_list = []\n\n# for ML model prediction\nfor clf in clf_list:\n    y_preds_ = []\n    y_probs_ = []\n\n    for train, test in cv.split(X, y):\n        clf.fit(X.iloc[train], y.iloc[train])\n        y_true = y.iloc[test]\n        y_pred = clf.predict(X.iloc[test])\n        y_probs = clf.predict_proba(X.iloc[test])\n        y_probs = y_probs[:, 1]\n        y_pred_series = pd.Series(y_pred,index=y[test].index)\n        y_probs_series = pd.Series(y_probs,index=y[test].index)\n        y_preds_.append(y_pred_series)\n        y_probs_.append(y_probs_series)\n    \n    \n    y_preds__ = pd.concat([i for i in y_preds_])\n    y_probs__ = pd.concat([i for i in y_probs_])\n    y_true__ = y.loc[y_preds__.index]\n    accs = accuracy_score(y_true__, y_preds__)\n    f1=f1_score(y_true__, y_preds__)\n    roc=roc_auc_score(y_true__, y_probs__)\n    prec=precision_score(y_true__, y_preds__)\n    score = [accs, f1, roc, prec]\n    scores_list.append(score)\n    y_preds_list.append(y_preds__)\n    y_probs_list.append(y_probs__)\n\n\nresults = pd.DataFrame(scores_list, columns=['accuracy','f1 score','roc auc score','precision score'],index=estimators)\nresult_show = results.sort_values('accuracy', ascending=False)\nresult_show\n\n\n\n\n\n\n\n\naccuracy\nf1 score\nroc auc score\nprecision score\n\n\n\n\nAdaBoost_best\n0.563135\n0.123726\n0.448611\n0.412621\n\n\nSVM_best\n0.510160\n0.299793\n0.476269\n0.380263\n\n\nGradientBoost_best\n0.493832\n0.468166\n0.489565\n0.421993\n\n\nRF_best\n0.488389\n0.503871\n0.498126\n0.427718\n\n\n\n\n\n\n\n\ny_probs_df = pd.concat(y_probs_list, axis=1).dropna()\ny_probs_df.columns = estimators\n\ny_probs_df['mean_'] = y_probs_df.mean(axis=1)\n\nmomentum = pd.Series(y_probs_df.mean_,index=y_probs_df.index)\n\n\n\nSelect the model\n\nplt.hist(momentum)[1]\n\narray([0.15405224, 0.22668779, 0.29932334, 0.37195889, 0.44459443,\n       0.51722998, 0.58986553, 0.66250108, 0.73513662, 0.80777217,\n       0.88040772])\n\n\n\n\n\n\nmomentum = momentum.loc['2010':'2020']\n\n\nclose = market_df.close.loc[momentum.index]\nplt.figure(figsize=(10,4))\nplt.plot(close, alpha=0.2)\n#plt.title('Momentum signals')\nplt.scatter(momentum.index, close, c=momentum, s=10,cmap='gray_r',vmin=0,vmax=1)\nplt.colorbar()\nplt.legend(['price','darker = long signals'])\nplt.show()\n\n\n\n\n\nmomentum.rename('signals').to_csv('C:data/momentum_signals.csv')"
  },
  {
    "objectID": "posts/ml_trading/3_Get_Features.html",
    "href": "posts/ml_trading/3_Get_Features.html",
    "title": "머신러닝을 이용한 트레이딩: (3) 피쳐 생성",
    "section": "",
    "text": "앞서 구한 시장 데이터를 이용하여 피쳐(feature)를 생성한다.\n피쳐의 종류는 아래에서 설명한다.\n\n# lib\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns;sns.set()\nplt.style.use('tableau-colorblind10')\n\n# homemade\nfrom features import tautil, sadf, trnd_scan, microstructure_features\n\n\n시장 데이터\n\nmtd_data = pd.read_excel('C:data/mtd_data.xlsx')\ndate = mtd_data.iloc[:,1].rename('Date')\nopen = mtd_data.iloc[:,2].rename('open')\nhigh = mtd_data.iloc[:,3].rename('high')\nlow = mtd_data.iloc[:,4].rename('low')\nclose = mtd_data.iloc[:,5].rename('close')\nvolume = mtd_data.iloc[:,6].rename('volume')\n\n\ndf = pd.DataFrame([open,high,low,close,volume]).T\ndf.index=date\n\n\nquantity_ = pd.read_csv('C:data/순매수량.csv')\nquantity_ = quantity_.iloc[:-1,1:5]\nquantity_.columns = ['Date','individual','foreign','institutional']\nquantity_.index = quantity_['Date'].apply(lambda x: pd.to_datetime(str(x), format='%Y%m%d'))\nquantity_.drop(columns='Date',inplace=True)\ndf = df.join(quantity_).dropna()\n\n\ndf.to_csv(\"C:data/market_samsung.csv\")\ndf\n\n\n\n\n\n\n\n\nopen\nhigh\nlow\nclose\nvolume\nindividual\nforeign\ninstitutional\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n2005-07-27\n11040\n11180\n10960\n11020\n18434300\n-2543250.0\n-1411300.0\n-1210850.0\n\n\n2005-07-28\n11040\n11320\n11040\n11200\n23659800\n-2067600.0\n3772300.0\n-1517250.0\n\n\n2005-07-29\n11320\n11320\n11200\n11300\n17875500\n1583050.0\n796450.0\n-1843600.0\n\n\n2005-08-01\n11320\n11380\n11220\n11380\n16471100\n-3111550.0\n1520100.0\n-2652100.0\n\n\n2005-08-02\n11400\n11420\n11240\n11360\n14254000\n-1567950.0\n-1895300.0\n-1310950.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2021-10-08\n72300\n72400\n71500\n71500\n14043287\n2612855.0\n319900.0\n-2923031.0\n\n\n2021-10-12\n70700\n70900\n68700\n69000\n31001484\n12155071.0\n-11004329.0\n-1421202.0\n\n\n2021-10-13\n68700\n69600\n68300\n68800\n24172015\n2224620.0\n-5271845.0\n2725596.0\n\n\n2021-10-14\n69000\n69800\n68800\n69400\n19520641\n2011163.0\n-3787173.0\n1696662.0\n\n\n2021-10-15\n70200\n71000\n70000\n70100\n18051612\n822742.0\n-1376104.0\n456288.0\n\n\n\n\n3965 rows × 8 columns\n\n\n\n\nfor i in df.columns:\n    plt.figure(figsize=(10,1))\n    plt.title(i)\n    plt.plot(df[i])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n피쳐 변수 (Feature)\n\n14개의 기술적분석 지표 (괄호안 숫자는 각 지표를 구하기 위한 window)\n\nRSI (15)\nWillams’s R (15)\nADX (15)\n\nAROON Indicator (20)\n\nDPO (20)\nMACD Difference (25,10,9)\nMass Index (10, 25)\nTRIX (15)\nATR (10)\nUI (15)\nCMF (20)\nFI (15)\nMFI (15)\nEOM SMA (15)\nVPT\n\n5,10,20일 기간의 가격 수익률\n30일 기간의 가격 변동성(표준편차)\n개인, 기관, 외국인 별 순매수량의 5일, 20일\n트렌드-스캐닝 백워드 t value span (20,60)\n미시구조론 변수 각 20일 이동평균\n\nkyle_lambda\n\namihud_lambda\n\nhasbrouck_lambda\n\nbekker_parkinson_volatility\ncorwin_schultz_estimator\n\n\n\nwindows_TA = [1]\nTA = tautil.get_my_stationary_ta_windows(df_ohlcv,windows_TA).dropna()\n\n\nwindows_mom = [5,10,20]\nwindows_std = [30]\n\nmoms = tautil.mom_std(df,windows_mom, windows_std)\nmoms = moms.iloc[:,:len(windows_mom)+len(windows_std)]\n\n\nquantity_ = pd.read_csv('C:data/순매수량.csv')\nquantity_ = quantity_.iloc[:-1,1:5]\nquantity_.columns = ['Date','individual','foreign','institutional']\nquantity_.index = quantity_['Date'].apply(lambda x: pd.to_datetime(str(x), format='%Y%m%d'))\nquantity_.drop(columns='Date',inplace=True)\n\n\nwindows_ma=[5,20]\nfor i in quantity_.columns:\n    for j in windows_ma:\n        quantity_['{} sma_{}'.format(i,j)] = quantity_[i].rolling(j).mean()\n        quantity_.dropna(inplace=True)\nquantity = quantity_.iloc[:,3:]\n\n\nspans = [20,60]\ntrnd_back = pd.DataFrame()\nfor span in spans:\n    trnd_back['trend_back_scan_{}'.format(span)] = trnd_scan.trend_backward_scanning(df.close, span).t_value\n\n\ndollar_volume = df.volume*df.close\n\n\nclose=df.close\nkyle = microstructure_features.get_bar_based_kyle_lambda(close, df.volume, 20).rename('kyle_lambda')\namihud = microstructure_features.get_bar_based_amihud_lambda(close, dollar_volume, 20).rename('amihud_lambda')\nhasbrouk = microstructure_features.get_bar_based_hasbrouck_lambda(close, dollar_volume,20).rename('hasbrouck_lambda')\nbp_vol = microstructure_features.get_bekker_parkinson_vol(df.high,df.low,20).rename('bekker_parkinson_vol')\ncorsch = microstructure_features.get_corwin_schultz_estimator(df.high,df.low,20).rename('corwin_schultz_estimator')\n\n\nmicrostructure = pd.concat([kyle,amihud,hasbrouk,bp_vol,corsch],axis=1)\n\n\nfeatures = TA.join([moms,quantity,trnd_back,microstructure]).dropna()\nfeatures.to_csv('C:data/features_samsung.csv')\n\n\nfeatures = features['2010':'2020']\nf, axs = plt.subplots(len(features.T),figsize=(10,70),gridspec_kw={'hspace': 1})\nfor i in range(len(features.T)):\n    axs[i].title.set_text(features.columns[i])\n    axs[i].plot(features.iloc[:,i])\nf.savefig('C:image/features.png')"
  },
  {
    "objectID": "posts/ml_trading/1_Get_Market_Data.html",
    "href": "posts/ml_trading/1_Get_Market_Data.html",
    "title": "머신러닝을 이용한 트레이딩: (1) 시장 데이터 수집",
    "section": "",
    "text": "기간: 2005년 7월 27일부터 2022년 1월 17일 까지\n삼성전자의 OHLCV(시가, 고가, 저가, 종가, 거래량)\n투자주체별 거래량: 개인, 외국인 기관\n\n파이썬 라이브러리인 FinanceDataReader와 yfinance을 이용한다. 순매수량 데이터는 대신증권 API로 받은 데이터를 이용한다.\n모든 시장 데이터는 일별 데이터이다.\n\n# lib\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns;sns.set()\n\nimport FinanceDataReader as fdr\nimport yfinance as yf\ndf_ohlc = fdr.DataReader('005930','2005-7-27','2022-1-17').iloc[:,0:4] #가격 수정되어 있음\nvolume_yf = yf.download('005930.KS','2005-7-27','2022-1-17',auto_adjust=True).Volume # 수정거래량\n\ndf_ohlcv = df_ohlc.join(volume_yf).dropna()\ndf = tautil.ohlcv(df_ohlcv)\n\nquantity_ = pd.read_csv('C:data/순매수량.csv')\nquantity_ = quantity_.iloc[:-1,1:5]\nquantity_.columns = ['Date','individual','foreign','institutional']\nquantity_.index = quantity_['Date'].apply(lambda x: pd.to_datetime(str(x), format='%Y%m%d'))\nquantity_.drop(columns='Date',inplace=True)\n\ndf = df.join(quantity_).dropna()\n\n얻은 데이터는 뒤에서 시각화하기로 한다."
  },
  {
    "objectID": "posts/machine_learning/GRL_02_2.html",
    "href": "posts/machine_learning/GRL_02_2.html",
    "title": "그래프 머신러닝: (2-2) 이웃 중복 감지",
    "section": "",
    "text": "Hamilton,W.L. Graph Representation Learning. 2020"
  },
  {
    "objectID": "posts/machine_learning/GRL_02_2.html#local-overlap-measures",
    "href": "posts/machine_learning/GRL_02_2.html#local-overlap-measures",
    "title": "그래프 머신러닝: (2-2) 이웃 중복 감지",
    "section": "Local Overlap Measures",
    "text": "Local Overlap Measures"
  },
  {
    "objectID": "posts/machine_learning/GRL_02_2.html#global-overlap-measures",
    "href": "posts/machine_learning/GRL_02_2.html#global-overlap-measures",
    "title": "그래프 머신러닝: (2-2) 이웃 중복 감지",
    "section": "Global Overlap Measures",
    "text": "Global Overlap Measures"
  },
  {
    "objectID": "posts/machine_learning/GRL_01.html",
    "href": "posts/machine_learning/GRL_01.html",
    "title": "그래프 머신러닝: (1) 들어가며",
    "section": "",
    "text": "Hamilton,W.L. Graph Representation Learning. 2020\n그래프(graph)는 복잡한 시스템을 나타내는 보편적인 수단이다."
  },
  {
    "objectID": "posts/machine_learning/GRL_01.html#그래프란",
    "href": "posts/machine_learning/GRL_01.html#그래프란",
    "title": "그래프 머신러닝: (1) 들어가며",
    "section": "그래프란?",
    "text": "그래프란?\n그래프: \\(\\mathcal{G} = (\\mathcal{V},\\mathcal{E})\\)\n\n그래프는 노드 집합 \\(V\\)와 엣지 집합 \\(\\mathcal{E}\\)로 정의된다. 노드 \\(u \\in \\mathcal{V}\\)에서 노드 \\(v \\in \\mathcal{V}\\)로 가는 엣지를 \\((u,v) \\in \\mathcal{E}\\)로 나타낸다.\n\n단순그래프(simple graph): 두 노드 간 최대 하나의 엣지를 가지고, 노드 스스로 연결하는 엣지가 없으며, 방향이 없는(undirected) 그래프\n인접행렬(adjacency matrix): \\(A \\in \\mathbb{R}^{|\\mathcal{V}| \\times |\\mathcal{V}|}\\)\n\n\\(A[u,v] = \\begin{cases}  1 & \\quad \\text{if } (u,v) \\in \\mathcal{E} \\\\  0 & \\quad \\text{otherwise}  \\end{cases}\\)\n만약, 그래프가 방향이 없는 엣지로 이루어져 있다면 \\(A\\)는 symmetric matrix가 된다.\nweighted 엣지를 가지는 경우 \\(0, 1\\)이 아닌 실수 원소를 가지는 인접행렬이 된다.\n\n\nMulti-relational 그래프\n\\((u,\\tau,v) \\in \\mathcal{E}\\) - 두 종류 이상의 관계에 대한 연결성을 나타낼 수 있는 그래프 - 관계 \\(\\tau \\in R\\)가 있으며, 각 종류의 관계 \\(\\tau\\)마다 인접행렬 \\(A_\\tau\\)가 있다. - 이때 인접행렬은 다음과 같은 3차원이다: \\(A \\in \\mathbb{R}^{|\\mathcal{V}| \\times |\\mathcal{R}| \\times |\\mathcal{V}|}\\) - multi-relational 그래프에는 heterogeneous와 multiplex 그래프가 있다.\n\nHeterogeneous 그래프:\n\n\\(\\mathcal{V}=\\mathcal{V_1} \\cup \\mathcal{V_2} \\cup \\cdots \\cup \\mathcal{V_3}\\) where \\(\\mathcal{V_i} \\cap \\mathcal{V_j} = \\emptyset\\) for all \\(i \\neq j\\)\n즉, disjoint한 노드 부분집합으로 구성된 노드 집합\n\n예시: 노드(약 노드집합, 질병 노드집합), 엣지(약-약:다약부작용, 약-질병:치료)\n\n관계 \\(\\tau_i\\)는 두 노드 부분집합 사이 연결: \\((u,\\tau_i, v) \\in \\mathcal{E} \\rightarrow u \\in V_j, v \\in V_k\\)\n이 때 \\(j \\neq k\\)이면 multipartite 그래프라고 한다. (즉, 다른 부분집합 간의 엣지만 존재)\n\n\n\nMultiplex 그래프\n\n\\(k\\)개의 층(layers)의 집합으로 이루어진 그래프\nintra-layer 엣지와 inter-layer 엣지가 있다.\n\n\n출처: Hammoud & Kramer (2020)\n\n\n\nFeature information\n\n노드의 특징(feature or attribute) (예: 소셜 네트워크에서 한 사람(노드)의 프로필 사진 등 특징) 정보\nfeature는 실수 행렬로 나타냄: \\(X \\in \\mathbb{R}^{|V| \\times m}\\)\nheterogeneous 그래프에서 각 노드는 고유의 특징을 가진다."
  },
  {
    "objectID": "posts/machine_learning/GRL_01.html#머신러닝과-그래프",
    "href": "posts/machine_learning/GRL_01.html#머신러닝과-그래프",
    "title": "그래프 머신러닝: (1) 들어가며",
    "section": "머신러닝과 그래프",
    "text": "머신러닝과 그래프\n\n노드 분류(node classification)\n\n레이블된 노드를 학습하여, 레이블되지 않은 노드를 분류\n기존의 i.i.d.(independent and identically distributed)를 가정하는 지도학습 모형과 달리 i.i.d.를 가정하지 않는다. 대신, 노드 간 연결 집합을 모델링 한다.\nhomophily: 그래프의 이웃 노드 간에 유사한 feature를 공유한다는 개념\nstructural equivalence: 유사한 이웃 구조를 가진 노드가 유사한 레이블을 가진다는 개념\nheterophily: 한 노드가 다른 레이블의 노드와 우선적으로 연결될 것을 가정\n\n\n\n관계 예측(relation prediction)\n\n주어진 노드 집합과 부분적인 엣지 집합을 활용하여 누락된 엣지를 예측하는 작업\n작업의 복잡성은 그래프 유형에 따라 다르며, 단순한 그래프에서는 이웃을 공유하는 노드 쌍을 기반으로 하는 간단한 방법으로 성능을 높일 수 있지만, 복잡한 다중 관계 그래프에서는 복잡한 추론이 필요하다.\n관계 예측은 지도 및 비지도 학습 개념을 혼합하며 그래프 도메인에 특화된 inductive bias가 필요하다.\n또한 단일 그래프 내에서 예측하는 경우와 여러 겹치지 않는 그래프 간 예측을 하는 경우 등 다양한 변형이 있다.\n\n\n\n커뮤니티 감지(community detection)\n\n위의 노드 분류와 관계 예측은 지도학습에 대응되는 작업이었다면, community detection은 비지도학습/클러스터링에 대응된다.\ncommunity detection은 주어진 \\(\\mathcal{G}=(\\mathcal{V},\\mathcal{E})\\)로 잠재적인 커뮤니티 구조를 추론한다.\n\n\n\n그래프 분류, 회귀, 클러스터링 (graph classification, regression, clustering)\n\n그래프 하나하나를 대상으로 분류하거나, 그래프 특성을 예측하거나, 클러스터링한다.\n여러 그래프로 구성된 데이터를 가지고 특정 그래프에 대해 작업한다."
  },
  {
    "objectID": "listing.html",
    "href": "listing.html",
    "title": "",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n그래프 머신러닝: (1) 들어가며\n\n\n\ngraph\n\n\nnetwork\n\n\nmachine learning\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n그래프 머신러닝: (2-1) 그래프 통계량과 커널 방법\n\n\n\ngraph\n\n\nnetwork\n\n\nmachine learning\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n그래프 머신러닝: (2-2) 이웃 중복 감지\n\n\n\ngraph\n\n\nnetwork\n\n\nmachine learning\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n머신러닝을 이용한 트레이딩: (0) 트레이딩 개요\n\n\n\ntrading\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n머신러닝을 이용한 트레이딩: (1) 시장 데이터 수집\n\n\n\ntrading\n\n\ndata\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n머신러닝을 이용한 트레이딩: (2) 가격 모멘텀 라벨링\n\n\n\ntrading\n\n\nlabeling\n\n\nmomentum\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n머신러닝을 이용한 트레이딩: (3) 피쳐 생성\n\n\n\ntrading\n\n\nfeature\n\n\nmomentum\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n머신러닝을 이용한 트레이딩: (4) 피쳐 선정\n\n\n\ntrading\n\n\nfeature selection\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n머신러닝을 이용한 트레이딩: (5) 매매 시그널 분류\n\n\n\ntrading\n\n\nsignals\n\n\nclassification\n\n\nmachine learning\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n머신러닝을 이용한 트레이딩: (6) 매매 규칙\n\n\n\ntrading\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n머신러닝을 이용한 트레이딩: (7) 매매 신뢰도 측정과 전략 강화\n\n\n\ntrading\n\n\nenhancing\n\n\nmachine learning\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n분수 차분 (fractional differentiation)\n\n\n\nfractional differentiation\n\n\ndata\n\n\nstationarity\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n시스템 리스크 분석: (1) 시스템 리스크 개념\n\n\n\nsystemic risk\n\n\nrisk\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n시스템 리스크 분석: (2) 시스템 리스크 측정 방법 - 선행연구 요약\n\n\n\nsystemic risk\n\n\nmeasures\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n시스템 리스크 분석: (2-1) 거시경제지표를 활용한 경제전체 위험도 측정 방법\n\n\n\nsystemic risk\n\n\ncycle\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n시스템 리스크 분석: (2-2) 시장위험 측정 방법\n\n\n\nsystemic risk\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n시스템 리스크 분석: (2-3) 금융기관 단면적 위험 측정방법\n\n\n\nsystemic risk\n\n\nfinancial institution\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n시스템 리스크 분석: (2-4) 금융기관 부도확률 위험 측정방법\n\n\n\nsystemic risk\n\n\nfinancial institution\n\n\ndefault\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n시스템 리스크 분석: (2-5) 네트워크 측정방법\n\n\n\nsystemic risk\n\n\nfinancial institution\n\n\nnetwork\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n시스템 리스크 분석: (3-1) 시스템 리스크와 머신러닝 - 네트워크 기반 및 빅데이터 분석\n\n\n\nsystemic risk\n\n\nmeasures\n\n\nnetwork\n\n\nmachine learning\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n시스템 리스크 분석: (3-2) 시스템 리스크와 머신러닝 - 계량경제학, 미시구조, 금융규제\n\n\n\nsystemic risk\n\n\nmeasures\n\n\nnetwork\n\n\nmachine learning\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n시스템 리스크 분석: (3-3) 시스템 리스크와 머신러닝 - 향후 연구 방향\n\n\n\nsystemic risk\n\n\nnetwork\n\n\nbig data\n\n\nmachine learning\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "E-mail: jhcho1016@naver.com"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Cho’s FinBlog!",
    "section": "",
    "text": "이 블로그는 빅데이터, AI를 활용한 금융 및 경제 분야를 공부하며 정리한 짧은 글이나 마저 끝내지 못한 연구 등을 올리는 개인 블로그입니다.\n\n\n모든 블로그 글 보기\n\n\n주요 관심주제: 금융 머신러닝(machine learning)/딥러닝(deep learning), 자산가격 모멘텀 예측, 금융 시스템 리스크(systemic risk), 네트워크 분석 등\n\n이 블로그의 모든 글은 오직 교육용 목적입니다.\nAbout Me"
  },
  {
    "objectID": "posts/fin_data/Fractional_difference.html",
    "href": "posts/fin_data/Fractional_difference.html",
    "title": "분수 차분 (fractional differentiation)",
    "section": "",
    "text": "가격과 같은 비정상성의(non-stationary) 시계열 자료는 통계모형 등의 분석에 적합하지 않다. 따라서 보통 시계열 자료를 (정수 단위로; 1차, 2차 …) 차분하여 정상적(stationary) 상태로 만들어 분석에 사용한다.\n시계열이 비정상성이라는 것은 그만큼 메모리(memory)를 보존하고 있다는 것이다. 따라서 과도하게 차분을 하여 시계열을 정상적으로 만들면 기존의 시계열 자료가 가지고 있는 정보는 사라지게 된다.\n이를 방지하기 위한 방법이 분수 차분(fractional differentiation)이다. “fractional differentiation”은 M.DePrado의 저서 “Advances in Financial Machine Learning”에 소개된 방법으로 시계열의 메모리를 어느정도 보존하면서 정상성을 만족하도록 차분해주는 것을 의미한다.\n\n\n\n다음과 같은 Backshift 연산자 \\(B\\)가 있다고 하자\n실수 행렬 \\({Xt}\\)에 대해\n\n\\(B^k X_t = X_{t−k}\\) (\\(k \\geq 0\\))\n\\((1-B)^{d} = \\sum^{\\infty}_{k=0}{\\left( \\begin{array}{c}  n \\\\  r  \\end{array}\\right)} (-B)^k\\) (formal binomial series expansion)\n\\((1-B)^{d}X_t\\): d차 차분 시계열 (d는 실수)\n\n따라서 d차 차분에 대해 다음과 같이 시계열이 생성된다.\n\n\\(\\tilde{X}_t = \\sum^{\\infty}_{k=0}w_kX_{t-k}\\)\n\\(w = \\{ 1, -d, \\frac{d(d-1)}{2!}, -\\frac{d(d-1)(d-2)}{3!}, \\cdots, (-1)^k \\prod_{i=1}^{k-1}(\\frac{d-i}{k!}) , \\cdots \\}\\)\n\n하지만 실제로 차분을 구할 때 위와 같은 계산은 무한급수가 되므로 이를 처리해야 한다.\n또한 위의 식에서의 계수(혹은 weight)를 \\(w\\)(예: \\(0.2, 0.08 \\cdots\\))라고 했을 때, 정수가 아닌 \\(d\\)에 대해 \\(k \\geq d+1\\)가 되면 \\(\\text{int}[d]\\)가 짝수일 때 \\(w\\)가 음수가 된다.\n즉, \\(\\text{int}[d]\\)가 짝수일 때, \\(\\lim\\limits_{k\\to \\infty} w_k = 0^-\\)이고, \\(\\text{int}[d]\\)가 홀수일 때, \\(\\lim\\limits_{k\\to \\infty} w_k = 0^+\\)이다.\n따라서 \\(w_k\\)의 \\(k\\)에 대한 window를 정해주거나, 상대적 weight-loss (\\(\\frac{\\sum_{j=T-1}^{T}|w_j|}{\\sum_{i=0}^{T-1}|w_i|}\\))에 대해 threshold를 설정하여 계산을 제한할 수 있다.\n예를 들어 다음과 같이 생각할 수 있다.\n\n\\((1-B)^{0.2} X_t = X_t - 0.2 X_{t-1} - 0.08 X_{t-2} - 0.048 X_{t-3} \\cdots\\)\n\\((1-B)^{0.4} X_t = X_t - 0.4 X_{t-1} - 0.12 X_{t-2} - 0.064 X_{t-3} \\cdots\\)\n\\((1-B)^{0.8} X_t = X_t - 0.8 X_{t-1} - 0.08 X_{t-2} - 0.032 X_{t-3} \\cdots\\)\n\\((1-B)^{1} X_t = X_t - 1.0 X_{t-1} - 0 X_{t-2} - 0 X_{t-3} \\cdots\\)\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport yfinance as yf\n\nimport fracdiff # https://github.com/fracdiff/fracdiff를 사용\n\n\n# S&P500 지수를 가져와본다\n\nsp500 = yf.download('^GSPC', start='2001-1-1', end='2023-1-1').Close\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\nsp500\n\nDate\n2001-01-02    1283.270020\n2001-01-03    1347.560059\n2001-01-04    1333.339966\n2001-01-05    1298.349976\n2001-01-08    1295.859985\n                 ...     \n2022-12-23    3844.820068\n2022-12-27    3829.250000\n2022-12-28    3783.219971\n2022-12-29    3849.280029\n2022-12-30    3839.500000\nName: Close, Length: 5535, dtype: float64\n\n\n\nplt.figure(figsize=(10,3))\nplt.title(\"S&P500\")\nplt.plot(sp500)\nplt.show()\n\n\n\n\n\n# fraction difference (n=1)은 np.diff()와 같다.\n\ndiff = sp500.diff()[1:]\nf_diff_1 = fracdiff.fdiff(sp500, n=1)\nnp.all(diff == f_diff_1)\n\nTrue\n\n\n\ndiff = sp500.diff()\nplt.figure(figsize=(10,3))\nplt.title(\"S&P500 difference\")\nplt.plot(diff)\nplt.show()\n\n\n\n\n\nn_s=[.2,.4,.6,.8,1.]\nw=10\nplt.figure(figsize=(16,8))\nplt.title(f\"S&P500 nth fractional differences\")\nplt.plot(sp500)\nfor n in n_s:\n    diff = fracdiff.fdiff(sp500, n=n, window=w)[w:]\n    diff = pd.Series(diff,index=sp500.index[int(n)+w:])\n    plt.plot(diff)\nplt.legend(['0 (original)']+n_s, title='nth')\nplt.savefig(\"fdiff_thumbnail.jpg\")\nplt.show()\n\n\n\n\n\n\n\n\nDe Prado, M. L. (2018). Advances in financial machine learning. John Wiley & Sons.\nhttps://fracdiff.github.io/fracdiff/"
  },
  {
    "objectID": "posts/fin_data/Fractional_difference.html#방법",
    "href": "posts/fin_data/Fractional_difference.html#방법",
    "title": "분수 차분 (fractional differentiation)",
    "section": "",
    "text": "다음과 같은 Backshift 연산자 \\(B\\)가 있다고 하자\n실수 행렬 \\({Xt}\\)에 대해\n\n\\(B^k X_t = X_{t−k}\\) (\\(k \\geq 0\\))\n\\((1-B)^{d} = \\sum^{\\infty}_{k=0}{\\left( \\begin{array}{c}  n \\\\  r  \\end{array}\\right)} (-B)^k\\) (formal binomial series expansion)\n\\((1-B)^{d}X_t\\): d차 차분 시계열 (d는 실수)\n\n따라서 d차 차분에 대해 다음과 같이 시계열이 생성된다.\n\n\\(\\tilde{X}_t = \\sum^{\\infty}_{k=0}w_kX_{t-k}\\)\n\\(w = \\{ 1, -d, \\frac{d(d-1)}{2!}, -\\frac{d(d-1)(d-2)}{3!}, \\cdots, (-1)^k \\prod_{i=1}^{k-1}(\\frac{d-i}{k!}) , \\cdots \\}\\)\n\n하지만 실제로 차분을 구할 때 위와 같은 계산은 무한급수가 되므로 이를 처리해야 한다.\n또한 위의 식에서의 계수(혹은 weight)를 \\(w\\)(예: \\(0.2, 0.08 \\cdots\\))라고 했을 때, 정수가 아닌 \\(d\\)에 대해 \\(k \\geq d+1\\)가 되면 \\(\\text{int}[d]\\)가 짝수일 때 \\(w\\)가 음수가 된다.\n즉, \\(\\text{int}[d]\\)가 짝수일 때, \\(\\lim\\limits_{k\\to \\infty} w_k = 0^-\\)이고, \\(\\text{int}[d]\\)가 홀수일 때, \\(\\lim\\limits_{k\\to \\infty} w_k = 0^+\\)이다.\n따라서 \\(w_k\\)의 \\(k\\)에 대한 window를 정해주거나, 상대적 weight-loss (\\(\\frac{\\sum_{j=T-1}^{T}|w_j|}{\\sum_{i=0}^{T-1}|w_i|}\\))에 대해 threshold를 설정하여 계산을 제한할 수 있다.\n예를 들어 다음과 같이 생각할 수 있다.\n\n\\((1-B)^{0.2} X_t = X_t - 0.2 X_{t-1} - 0.08 X_{t-2} - 0.048 X_{t-3} \\cdots\\)\n\\((1-B)^{0.4} X_t = X_t - 0.4 X_{t-1} - 0.12 X_{t-2} - 0.064 X_{t-3} \\cdots\\)\n\\((1-B)^{0.8} X_t = X_t - 0.8 X_{t-1} - 0.08 X_{t-2} - 0.032 X_{t-3} \\cdots\\)\n\\((1-B)^{1} X_t = X_t - 1.0 X_{t-1} - 0 X_{t-2} - 0 X_{t-3} \\cdots\\)"
  },
  {
    "objectID": "posts/fin_data/Fractional_difference.html#예시",
    "href": "posts/fin_data/Fractional_difference.html#예시",
    "title": "분수 차분 (fractional differentiation)",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport yfinance as yf\n\nimport fracdiff # https://github.com/fracdiff/fracdiff를 사용\n\n\n# S&P500 지수를 가져와본다\n\nsp500 = yf.download('^GSPC', start='2001-1-1', end='2023-1-1').Close\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\nsp500\n\nDate\n2001-01-02    1283.270020\n2001-01-03    1347.560059\n2001-01-04    1333.339966\n2001-01-05    1298.349976\n2001-01-08    1295.859985\n                 ...     \n2022-12-23    3844.820068\n2022-12-27    3829.250000\n2022-12-28    3783.219971\n2022-12-29    3849.280029\n2022-12-30    3839.500000\nName: Close, Length: 5535, dtype: float64\n\n\n\nplt.figure(figsize=(10,3))\nplt.title(\"S&P500\")\nplt.plot(sp500)\nplt.show()\n\n\n\n\n\n# fraction difference (n=1)은 np.diff()와 같다.\n\ndiff = sp500.diff()[1:]\nf_diff_1 = fracdiff.fdiff(sp500, n=1)\nnp.all(diff == f_diff_1)\n\nTrue\n\n\n\ndiff = sp500.diff()\nplt.figure(figsize=(10,3))\nplt.title(\"S&P500 difference\")\nplt.plot(diff)\nplt.show()\n\n\n\n\n\nn_s=[.2,.4,.6,.8,1.]\nw=10\nplt.figure(figsize=(16,8))\nplt.title(f\"S&P500 nth fractional differences\")\nplt.plot(sp500)\nfor n in n_s:\n    diff = fracdiff.fdiff(sp500, n=n, window=w)[w:]\n    diff = pd.Series(diff,index=sp500.index[int(n)+w:])\n    plt.plot(diff)\nplt.legend(['0 (original)']+n_s, title='nth')\nplt.savefig(\"fdiff_thumbnail.jpg\")\nplt.show()"
  },
  {
    "objectID": "posts/fin_data/Fractional_difference.html#참고문헌",
    "href": "posts/fin_data/Fractional_difference.html#참고문헌",
    "title": "분수 차분 (fractional differentiation)",
    "section": "",
    "text": "De Prado, M. L. (2018). Advances in financial machine learning. John Wiley & Sons.\nhttps://fracdiff.github.io/fracdiff/"
  },
  {
    "objectID": "posts/machine_learning/GRL_02_1.html",
    "href": "posts/machine_learning/GRL_02_1.html",
    "title": "그래프 머신러닝: (2-1) 그래프 통계량과 커널 방법",
    "section": "",
    "text": "Hamilton,W.L. Graph Representation Learning. 2020"
  },
  {
    "objectID": "posts/machine_learning/GRL_02_1.html#node-level-통계량과-특징",
    "href": "posts/machine_learning/GRL_02_1.html#node-level-통계량과-특징",
    "title": "그래프 머신러닝: (2-1) 그래프 통계량과 커널 방법",
    "section": "Node-level 통계량과 특징",
    "text": "Node-level 통계량과 특징\n\nNode degree\n한 노드에 연결된 엣지 수\n\\[d_u = \\sum_{v \\in V}A[u,v] \\quad \\text{for } u \\in \\mathcal{V}\\]\n\nnode degree는 고려해야 할 필수적 요소이며 전통적 기계학습 모델에서 가장 유익한 정보 중 하나이다.\n하지만, 한 노드와 연결된 다른 노드들의 중요성은 노드의 degree에 반영되지 않는다. 즉, 두 노드가 같은 degree를 갖더라도 이웃의 중요도에 따라 중요도가 다를 수 있다.\n\n\n\nNode centrality\neigenvector centrality\n\nnode degree와 달리, 노드의 이웃들의 중요도까지 고려하는 방법이다.\n\n\\[e_u = \\frac{1}{\\lambda}\\sum_{v \\in V}A[u,v] e_v, \\quad \\forall u \\in \\mathcal{V}\\]\n\n식처럼 recursive한 성질을 가진다.\n단, 위의 식을 다시 쓰면 벡터 \\(\\mathbf{e}\\)에 대해 \\(\\lambda \\mathbf{e} = A \\mathbf{e}\\)와 같다. 즉, centrality는 인접행렬의 eigenvector이다. 양의 centrality 값을 가정하면, Perron-Frobenius Theorem을 적용하여 \\(\\mathbf{e}\\)를 구할 수 있다.\n또한, eigenvector centrality느 그래프에서 무한 랜덤워킹할 때 해당 노드에 도착할 확률이라고도 볼 수 있다.\n\n\\(\\mathbf{e}^{(t+1)} = A \\mathbf{e}^{(t)}\\) : power iteration\n\n\n다른 centrality\n\nbetweenness centrality: 한 노드가 다른 두 노드 사이의 최단 경로에 있을 빈도\ncloseness centrality: 한 노드와 다른 모든 노드 사이 최단거리들의 평균\n\n\n\nClustering Coefficient\n\n한 노드의 로컬(local) 이웃에서의 closed triangle의 비율\nclustering coefficient의 변형 중 ‘local’ variant를 보자. \\[c_u = \\frac{|(v_1,v_2) \\in \\mathcal{E}: v_1, v_2 \\in \\mathcal{N}(u)|}{\\binom{d_u}{2}}\\]\n\n분자는 노드 \\(u\\)의 이웃 간 엣지 수이고, 분모는 \\(u\\)의 이웃에 있는 노드 쌍의 수\n\n노드의 이웃이 얼마나 tight하게 클러스터되어 있는지를 측정한다.\nclustering coefficient가 1이면, \\(u\\)의 모든 이웃은 서로의 이웃이기도 하게 된다.\n\n\n\nClosed Triangles, Ego Graphs, and Motif\n\n(Closed Triangles) clustering coefficient는 노드의 로컬 이웃 사이에 있는 closed triangle의 수라고 볼 수도 있다.\n(Ego graphs) clustering coefficient는 실제 triangle의 수와 노드의 ’ego graph’의 총 가능 triangle 수의 비이기도 하다.\n\nego graph: 노드, 이웃, 이웃 간 모든 엣지를 포함하는 subgraph\n\n노드의 ego graph 내 임의의 motif나 graphlet을 세는 개념으로 일반화될 수 있다. 즉, 단순히 triangle을 세는 것보다는 특정 길이의 순환과 같은 복잡한 구조를 고려할 수 있고, 이러한 서로 다른 motif가 ego graph에서 얼마나 자주 발생하는지 세는 것으로 노드를 특성화할 수 있다.\n이러한 방식으로 노드의 ego graph를 조사하여 노드 레벨의 통계량을 계산하는 작업을 그래프 레벨로 근본적으로 변환할 수 있다."
  },
  {
    "objectID": "posts/machine_learning/GRL_02_1.html#graph-level-특징과-graph-kernels",
    "href": "posts/machine_learning/GRL_02_1.html#graph-level-특징과-graph-kernels",
    "title": "그래프 머신러닝: (2-1) 그래프 통계량과 커널 방법",
    "section": "Graph-level 특징과 Graph Kernels",
    "text": "Graph-level 특징과 Graph Kernels\n\nBag of Nodes\n\n단순히 node-level의 통계량을 집계하여 graph-level 피쳐로 만든다.\n하지만, 로컬(local) node-level의 정보만에 기반하기 때문에 글로벌(global)한 속성을 놓칠 수 있다.\n\n\n\nThe Weisfieler-Lehman Kernel\n\n노드-레벨 피처를 더 풍성하게 만들고나서 집계하는 방법 중 하나이다.\nWL 알고리즘은 다음과 같다.\n\n\n각 노드에 초기 레이블 값을 준다. 보통 degree로 한다.: \\(l^{(0)}(v) = d_v\\)\n\n\n노드의 이웃에서 현재 레이블의 multi-set을 hashing하여 새로운 레이블을 할당하며, 이를 반복한다.: \\[l^{(i)}(v) = \\text{HASH}(\\{ \\{ l^{(i)}(v) \\quad \\forall u \\in \\mathcal{N}(v) \\} \\} )\\]\n\n\n{{ }}는 mult-set을 뜻한다. multi-set은 원소의 중복을 허용한다.\n\n\n이를 \\(K\\)번 반복하면 각 노드마다 \\(l^{(K)}(v)\\)가 생기며, 이는 \\(K\\)-hop 이웃의 구조를 요약한다고 볼 수 있다. 다시 말해, WL kernel은 두 그래프의 결과 레이블 집합 간의 차이를 측정하는 것이다.\n\n\n이러한 WL kernel은 그래프 동형(isomorphism)을 확인하는 데 쓰인다: 두 그래프가 \\(K\\)번 WL알고리즘 반복 뒤 생기는 레이블 집합이 같은지를 확인한다.\n\n\n\nGraphlets, Path-Based Methods\ngraphlets\n\nFigure 2.2 in Hamilton,Graph Representation Learning. 2020: 크기가 3인 graphlet들\n\n그래프에 있는 graphlets (서로 다른 작은 subgraph 구조) 수\ngraphlet kernel은 전체 그래프에서 특정한 크기의 그래프 구조가 얼마나 나타나는지 세는 방식이다. 하지만 모든 graphlet을 세는 것은 번거로운 일이다.\n\npath-based 방법\n\ngraphlet을 세는 방법 대신 그래프에서 서로 다른 경로(path)의 종류가 얼마나 있는지를 본다.\n예를 들어 그래프에서 랜덤워크를 가정할때 발생하는 서로 다른 degree 시퀀스를 카운팅하는 random walk kernel(Kashima et al.(2003)), 또는 가장짧은 경로만을 카운팅하는 shortest-path kernel(Borgwardt and Kriegel (2005)) 등이 있다."
  },
  {
    "objectID": "posts/ml_trading/0_Introduction.html",
    "href": "posts/ml_trading/0_Introduction.html",
    "title": "머신러닝을 이용한 트레이딩: (0) 트레이딩 개요",
    "section": "",
    "text": "머신러닝을 이용한 트레이딩\n\n머신러닝을 이용한 모멘텀 예측과 전략 강화\n\n아래는 구체적인 트레이딩에 머신러닝을 활용하는 구조를 보여준다. 다만, 블로그에서는 이 중 일부만 다루기로 한다.\n\n1. 금융 데이터와 바(bar)\n\n틱 데이터로부터 시간 기준 혹은 금액 기준 바 형성\n\n\n\n2. 매수, 매도 시그널 포착\n\n기술적분석을 이용한 기존 모멘텀 전략 (MA crossover, RSI..)\n추가적인 ML 분류기를 이용한 모멘텀 감지\n\n\n\n3. 매매 규칙\n\n위의 매매 시그널로 진입 규칙 설정\n이익실현, 손절, 최대보유기간 등을 고려해 청산 규칙 설정\n과거 시나리오를 바탕으로 각 매매 결과 기록 (뒤의 강화 전략을 위해)\n\n\n\n4. 전략 강화 ML 모형\n\n4.1. 피쳐 생성 (\\(X\\))\n\n시장 데이터(가격, 거래량 등)와 기술적 지표\n시장 미시구조적(microstructure) 특징\n거시경제 변수\n자산 펀더멘털\nSNS/뉴스 센티멘트, 분석 컨센서스 등\n\n\n\n4.2. 머신러닝 모형 최적화\n\n피쳐 선정\nCross-validation (Purged k-fold)\n하이퍼파라미터(hyperparameter) 튜닝\nAutoML 스태킹 (혹은 각각의 Random forest, Adaboost, SVM, GBM, XGBoost, LSTM 등)\n성능 (accuracy, f1 score, roc-auc)\n\n\n\n4.3. 결과\n\n매매 신뢰도 (각각의 매매 시그널에 따라 실제 매매했을 때의 성공 예측 확률)\n\n\n\n\n5. 트레이딩 결정\n\n모멘텀 전략으로부터의 매매 시그널에 따라 베팅할지 혹은 패스할지를 결정\n위의 매매 신뢰도를 바탕으로 매매 금액/비중을 고려\n\n\n\n6. 백테스팅\n\nCumulative returns, Sharpe ratio, max drawdown, win ratio\n\n\n\n\n참고 문헌:\n\nAdvances in Financial Machine Learning, Lopez de Prado (2018)\n\n\n\n플로우차트\nSimple version \n\nDetailed version"
  },
  {
    "objectID": "posts/ml_trading/2_Labeling.html",
    "href": "posts/ml_trading/2_Labeling.html",
    "title": "머신러닝을 이용한 트레이딩: (2) 가격 모멘텀 라벨링",
    "section": "",
    "text": "삼성전자 종가를 기준으로 가격의 트렌드와 트렌드 강도(모멘텀)을 측정하여 이를 라벨(label)로 사용한다.\n\nTrend scanning 기법\n\n향후 N 기간동안의 선형 회귀 시 t통계량의 절댓값의 최대값을 사용\n즉, 일마다 향후 주어진 기간 중 가장 강한 트렌드를 예측하고자 하는 그날의 정답 트렌드로 지정\n\n\n이 실험에서는 측정한 트렌드(\\(|\\hat{t}|\\))를 분위 수로 나누어 이산적인 값으로 지정해주었다. 이는 다음에 분류(classification) 작업을 하기 위함이다.\n아래 그래프에서 기간 N을 달리했을 때를 비교한 뒤, 투자자의 트레이딩 기간을 고려해 이를 정해준다.\n\n# lib\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns;sns.set()\n\n# homemade\nfrom features import tautil\nfrom labeling import labeling\n\n\nimport FinanceDataReader as fdr\ndf_ = fdr.DataReader('005930','2010-1-1','2021-6-1')\ndf = tautil.ohlcv(df_)\n\n\nclose =df.close\n\n\nwindows=[60]\n\n\ntrend_scanning_regime_q3 = []\nfor i in windows:\n    trend_scanning_regime_q3.append(labeling.trend_scanning_label(close,window=i,q=3)[0].abs())\n\n\ntrend_scanning_q3 = []\nfor i in windows:\n    trend_scanning_q3.append(labeling.trend_scanning_label(close,window=i,q=3)[0])\n\n\nfor j in range(len(windows)):\n    y = trend_scanning_q3[j][:'2020']\n    i = windows[j]\n    f, (a0, a1) = plt.subplots(2, gridspec_kw={'height_ratios': [5, 1]}, figsize=(15,5))\n    f.suptitle(\"Trend Scanning Labels {}span 3cut\".format(i))\n    a0.plot(close[:'2020'],alpha=0.4)\n    a0.scatter(close[:'2020'].index,close[:'2020'],c=y, cmap='vlag')\n    a1.plot(y.fillna(0))\n    f.show()\n\n\n\n\n\nfor j in range(len(windows)):\n    y = np.sign(trend_scanning_q3[j]-1)+1\n    i = windows[j]\n    f, (a0, a1) = plt.subplots(2, gridspec_kw={'height_ratios': [5, 1]}, figsize=(15,5))\n    f.suptitle(\"Trend Scanning (long position) Labels {}span 4cut\".format(i))\n    a0.plot(close,alpha=0.4)\n    a0.scatter(close.index,close,c=y, cmap='vlag')\n    a1.plot(y.fillna(0))\n    f.savefig(\"c:image/labeling/trend_scanning_long_pos_{}.png\".format(i))\n    f.show()"
  },
  {
    "objectID": "posts/ml_trading/4_Feature_Selection.html",
    "href": "posts/ml_trading/4_Feature_Selection.html",
    "title": "머신러닝을 이용한 트레이딩: (4) 피쳐 선정",
    "section": "",
    "text": "Input\n\nLabel: 트렌드 스캐닝 기준 up-trend vs. (down- or no-trend)\n기간 : 2005 - 2010\n피쳐: market data features\n\nModel: 랜덤포레스트\n\n5가지 피쳐 선정 기법 비교: original, mda-kmeans, mda-optics, mda-onc, rfecv, sbfs\n\nOutput\n\n최상의 방법으로 선정한 피쳐 사용\n# lib\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns;sns.set()\nplt.style.use('tableau-colorblind10')\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score, accuracy_score, f1_score\n\nfrom sklearn.cluster import OPTICS, KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.feature_selection import RFECV, SequentialFeatureSelector\n\n# homemade\nfrom feature_engineering import cluster\nfrom feature_importance import importance\nfrom labeling import labeling\nfrom mlutil.pkfold import PKFold\nmarket_df = pd.read_csv('C:data/market_samsung.csv')\nmarket_df = market_df.rename(columns={market_df.columns[0]:'Date'})\nmarket_df.index = pd.to_datetime(market_df.Date)\nmarket_df.drop(columns='Date',inplace=True)\nmarket_df.dropna(inplace=True)\n\nfeature_df = pd.read_csv('C:data/features_samsung.csv')\nfeature_df = feature_df.rename(columns={feature_df.columns[0]:'Date'})\nfeature_df.index = pd.to_datetime(feature_df.Date)\nfeature_df.drop(columns='Date',inplace=True)\nfeature_df.dropna(inplace=True)\nX = feature_df.dropna()\nX.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 3873 entries, 2005-11-04 to 2021-10-15\nData columns (total 32 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   momentum_rsi_15           3873 non-null   float64\n 1   momentum_wr_15            3873 non-null   float64\n 2   trend_adx_15              3873 non-null   float64\n 3   trend_aroon_ind_20        3873 non-null   float64\n 4   trend_dpo_20              3873 non-null   float64\n 5   trend_macd_diff_25_10_9   3873 non-null   float64\n 6   trend_mass_index_10_25    3873 non-null   float64\n 7   trend_trix_15             3873 non-null   float64\n 8   volatility_atr_10         3873 non-null   float64\n 9   volatility_ui_15          3873 non-null   float64\n 10  volume_cmf_20             3873 non-null   float64\n 11  volume_fi_15              3873 non-null   float64\n 12  volume_mfi_15             3873 non-null   float64\n 13  volume_sma_em_15          3873 non-null   float64\n 14  volume_vpt                3873 non-null   float64\n 15  ret_10                    3873 non-null   float64\n 16  ret_20                    3873 non-null   float64\n 17  ret_5                     3873 non-null   float64\n 18  std_30                    3873 non-null   float64\n 19  individual sma_5          3873 non-null   float64\n 20  individual sma_20         3873 non-null   float64\n 21  foreign sma_5             3873 non-null   float64\n 22  foreign sma_20            3873 non-null   float64\n 23  institutional sma_5       3873 non-null   float64\n 24  institutional sma_20      3873 non-null   float64\n 25  trend_back_scan_20        3873 non-null   float64\n 26  trend_back_scan_60        3873 non-null   float64\n 27  kyle_lambda               3873 non-null   float64\n 28  amihud_lambda             3873 non-null   float64\n 29  hasbrouck_lambda          3873 non-null   float64\n 30  bekker_parkinson_vol      3873 non-null   float64\n 31  corwin_schultz_estimator  3873 non-null   float64\ndtypes: float64(32)\nmemory usage: 998.5 KB"
  },
  {
    "objectID": "posts/ml_trading/4_Feature_Selection.html#클러스터-기반-방법",
    "href": "posts/ml_trading/4_Feature_Selection.html#클러스터-기반-방법",
    "title": "머신러닝을 이용한 트레이딩: (4) 피쳐 선정",
    "section": "클러스터 기반 방법",
    "text": "클러스터 기반 방법\n\nclustering\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_sc = sc.fit_transform(X)\nX_sc = pd.DataFrame(X_sc, index=X.index, columns=X.columns)\n\n\nX_sc=X_sc[:'2020']\n\n\nsilhouette_coefficients = []\nkmeans_kwargs = {\n    \"init\": \"random\",\n    \"n_init\": 10,\n    \"max_iter\": 300,\n    \"random_state\": 42,\n}\n\nfor k in range(2, 30):\n    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n    kmeans.fit(X.T)\n    score = silhouette_score(X.T, kmeans.labels_)\n    silhouette_coefficients.append(score)\n\n\nn_clusters=np.argmin(silhouette_coefficients)+2\nkmeans = KMeans(\n    init=\"random\",\n    n_clusters=n_clusters,\n    n_init=10,\n    max_iter=300,\n    random_state=42)\nkmeans.fit(X_sc.T)\nclusters_kmeans = {i: X_sc.columns[np.where(kmeans.labels_ == i)[0]].tolist() for i in np.unique(kmeans.labels_)}\n\n\noptics = OPTICS(min_cluster_size=2)\noptics.fit(X.T)\nclusters_optics = {i: X_sc.columns[np.where(optics.labels_ == i)[0]].tolist() for i in np.unique(optics.labels_)}\n\n\n# 오래 걸림.\nclusters_onc_dist = cluster.get_feature_clusters(X_sc, dependence_metric= 'distance_correlation')\n\nNo feature/s found with low silhouette score. All features belongs to its respective clusters\n\n\n\n\nmda - selection\n\n#labeling\ntrend_scanning_window = 60\ntrend_scanning_q = 3\nts_out = labeling.trend_scanning_label(market_df.close, window = trend_scanning_window, q = trend_scanning_q)\nmom_label = ts_out[0]\ny = np.sign(mom_label-1)+1 # up-trend vs others\n\n\nraw_X = X_sc.copy()\n\ntmp = raw_X.join(y).dropna()\nX=tmp.iloc[:,:-1]\ny=tmp.iloc[:,-1]\n\n# train & test split\n# use previous data for feature selection\nX = X.loc['2005':'2010']\ny = y.loc['2005':'2010']\n\n\n# CV\nn_cv=4\nt1 = ts_out[1].loc[X.index]\ncv = PKFold(n_cv,t1,0.01)\n\n\nclusters = [clusters_kmeans[i] for i in range(n_clusters)]\nclusters2 = [clusters_optics[i] for i in clusters_optics.keys()]\nclusters3 = clusters_onc_dist\n\n\nclf = RandomForestClassifier(n_estimators=1000,class_weight='balanced')\nmda_cluster = importance.mean_decrease_accuracy(clf,X,y,cv,clustered_subsets=clusters)\nmda_cluster2 = importance.mean_decrease_accuracy(clf,X,y,cv,clustered_subsets=clusters2)\nmda_cluster3 = importance.mean_decrease_accuracy(clf,X,y,cv,clustered_subsets=clusters3)\n\n\nfeatures_mda_kmeans = mda_cluster.loc[mda_cluster['mean'] == mda_cluster['mean'].max()].index\nfeatures_mda_optics = mda_cluster2.loc[mda_cluster2['mean'] == mda_cluster2['mean'].max()].index\nfeatures_mda_onc_dist = mda_cluster3.loc[mda_cluster3['mean'] == mda_cluster3['mean'].max()].index\n\n# 0에서 min 값으로 변경함.\n\n\nnew_X1 = X[features_mda_kmeans]\nnew_X2 = X[features_mda_optics]\nnew_X3 = X[features_mda_onc_dist]"
  },
  {
    "objectID": "posts/ml_trading/4_Feature_Selection.html#비-클러스터링-방법",
    "href": "posts/ml_trading/4_Feature_Selection.html#비-클러스터링-방법",
    "title": "머신러닝을 이용한 트레이딩: (4) 피쳐 선정",
    "section": "비-클러스터링 방법",
    "text": "비-클러스터링 방법\n\nRFECV(Rercursive Feature Elimination with CV)\n\n\n# 오래걸림\n\nrf = RandomForestClassifier(class_weight='balanced')\n\nmin_features_to_select = 2  # Minimum number of features to consider\nrfecv = RFECV(\n    estimator=rf,\n    step=1,\n    cv=cv,\n    scoring=\"accuracy\",\n    min_features_to_select=min_features_to_select,\n)\nnew_X5_ = rfecv.fit_transform(X,y)\n\n\nnew_X5 = pd.DataFrame(new_X5_, index=X.index, columns=rfecv.get_feature_names_out())\n\n\nX_list = [X,new_X1,new_X2,new_X3,new_X5]"
  },
  {
    "objectID": "posts/ml_trading/6_Trading_Rules.html",
    "href": "posts/ml_trading/6_Trading_Rules.html",
    "title": "머신러닝을 이용한 트레이딩: (6) 매매 규칙",
    "section": "",
    "text": "Trading rules: 매수 진입만 허용\n# lib\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns;sns.set()\nplt.style.use('tableau-colorblind10')\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, normalize\n\n# homemade\nfrom features import trnd_scan, tautil\nfrom labeling import labeling\nfrom triple_barrier import get_barrier, make_rt\nfrom backtest import print_round_trip_stats, round_trip\nimport warnings\nwarnings.filterwarnings(action='ignore')\nmarket_df = pd.read_csv('C:data/market_samsung.csv')\nmarket_df = market_df.rename(columns={market_df.columns[0]:'Date'})\nmarket_df.index = pd.to_datetime(market_df.Date)\nmarket_df.drop(columns='Date',inplace=True)\nmarket_df.dropna(inplace=True)\n\nclose = market_df.close['2010':'2020']"
  },
  {
    "objectID": "posts/ml_trading/6_Trading_Rules.html#진입-규칙",
    "href": "posts/ml_trading/6_Trading_Rules.html#진입-규칙",
    "title": "머신러닝을 이용한 트레이딩: (6) 매매 규칙",
    "section": "진입 규칙",
    "text": "진입 규칙\n\nMomentum Prediction\n\nsignals = pd.read_csv('C:data/momentum_signals.csv')\nsignals.index = pd.to_datetime(signals['Date'])\nsignals.drop(columns='Date',inplace=True)\n\n\nsignals = signals['signals'].loc['2010':'2020']\n\n\nscaler = normalize\nscaler2 = MinMaxScaler()\nsignals = pd.Series(scaler2.fit_transform(normalize(signals.values.reshape(-1,1),axis=0)).reshape((-1,)), \n                           index=signals.index).rename('signals')\n\n\nplt.hist(signals,bins=50)[2]\nplt.title('Distribution of momentum signals')\nplt.show()\n\n\n\n\n\nthresholds = [0, 0.3]\n\n\nenter_ml_list=[]\nfor h in thresholds:\n    enter_ml_list.append(signals.loc[signals&gt;h].index)\n\n\nfor i in range(len(thresholds)):\n    plt.figure(figsize=(10,3))\n    plt.plot(close, alpha=0.5)\n    plt.title('Enter points ML Prediction (Option {})'.format(i+1))\n    plt.plot(close.loc[enter_ml_list[i]],marker='^',linewidth=0,alpha=0.3)\n    plt.legend(['price','Long signals from momentum classifier'])\n    plt.show()\n\n\n\n\n\n\n\n\n\nTech. Analysis Long/short decision\n\nopen = market_df.open['2010':'2020']\nrsi = tautil.RSIIndicator(open,14).rsi().dropna()\nlong = (rsi&gt;=50) & (rsi&lt;70)\nenter_ta = rsi.loc[long].index\n\n\nplt.figure(figsize=(10,3))\nplt.plot(close, alpha=0.5)\nplt.title('Enter points TA')\nplt.plot(close.loc[enter_ta],marker='^',linewidth=0,alpha=0.3)\nplt.legend(['price','Long signals from rsi'])\nplt.show()\n\n\n\n\n\nenter_list = [enter_ta]\nenter_list.append((enter_ml_list[1]& enter_ta).sort_values().drop_duplicates())\n\n\nfor i in range(len(thresholds)):\n    plt.figure(figsize=(10,3))\n    plt.plot(close, alpha=0.5)\n    plt.title('Enter points (Option {})'.format(i+1))\n    plt.plot(close.loc[enter_list[i]],marker='^',linewidth=0,alpha=0.3)\n    plt.legend(['price','Enter points'])\n    plt.show()\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10,3))\nplt.plot(close, alpha=0.5)\nplt.title('Position Enter points'.format(i+1))\nplt.plot(close.loc[enter_list[1]],marker='^',linewidth=0,alpha=0.3)\nplt.legend(['price','Enter points'])\nplt.show()"
  },
  {
    "objectID": "posts/ml_trading/6_Trading_Rules.html#청산-규칙",
    "href": "posts/ml_trading/6_Trading_Rules.html#청산-규칙",
    "title": "머신러닝을 이용한 트레이딩: (6) 매매 규칙",
    "section": "청산 규칙",
    "text": "청산 규칙\n\n# no Rule (benchmark)\npt_sl_bm = [1000,1000]\nmax_holding_bm = [1, 0]\nno_exit_rule = [pt_sl_bm,max_holding_bm]\n\n\n#dynamic target rule\nmax_holding = [60, 0]\nclose_ = market_df.close['2009':'2020']\nchanges = close_.pct_change(1).to_frame()\nfor i in range(2,max_holding[0]+1):\n    changes = changes.join(close_.pct_change(i).rename('close {}'.format(i)))\ndynamic_target = changes.abs().dropna().mean(axis=1)['2010':]\n\n\nbarrier_exit_list=[]\nbarrier_exit_list.append(get_barrier(close, enter_list[1], [1,1], max_holding, target = dynamic_target))  #dynamic  \n\nrts_exit_list=[]\nfor i in range(len(barrier_exit_list)):\n    rts_exit_list.append(make_rt(close,barrier_exit_list[i].dropna()))\n\n\nplt.figure(figsize=(10,2))\nplt.title('Dynamic exit rule returns')\nplt.plot(barrier_exit_list[0].ret)\nplt.legend(['Returns of dynamic exit target rate'])\n\n&lt;matplotlib.legend.Legend at 0x1fe01fa7148&gt;"
  },
  {
    "objectID": "posts/ml_trading/6_Trading_Rules.html#벤치마크",
    "href": "posts/ml_trading/6_Trading_Rules.html#벤치마크",
    "title": "머신러닝을 이용한 트레이딩: (6) 매매 규칙",
    "section": "벤치마크",
    "text": "벤치마크\n\n매번 매매시\n\n\nbarrier_bm = get_barrier(close, close.index, no_exit_rule[0], no_exit_rule[1]) #no rule\n\n\nrts_bm = make_rt(close,barrier_bm.dropna())\n\n\nround_trip.get_df_ann_sr(rts_bm,'Benchmark',years=11)\n\n\n\n\n\n\n\n\nBenchmark\n\n\n\n\navg_n_bets_per_year\n243.272727\n\n\nwin_ratio\n0.518835\n\n\nannualized_sharpe_ratio\n0.537080\n\n\n\n\n\n\n\n\nresult_df = pd.concat([round_trip.get_df_ann_sr(rts_bm,'No Rule')], axis=1)\nfor i in range(len(rts_exit_list)):\n    result_df = result_df.join(round_trip.get_df_ann_sr(rts_exit_list[i],'Enter & Exit Rule'))\n    \nresult_df\n\n\n\n\n\n\n\n\nNo Rule\nEnter & Exit Rule\n\n\n\n\navg_n_bets_per_year\n243.272727\n105.000000\n\n\nwin_ratio\n0.518835\n0.590988\n\n\nannualized_sharpe_ratio\n0.537080\n1.800316"
  },
  {
    "objectID": "posts/ml_trading/6_Trading_Rules.html#최적-청산-규칙-파라미터",
    "href": "posts/ml_trading/6_Trading_Rules.html#최적-청산-규칙-파라미터",
    "title": "머신러닝을 이용한 트레이딩: (6) 매매 규칙",
    "section": "최적 청산 규칙 파라미터",
    "text": "최적 청산 규칙 파라미터\n\n#dynamic target rule\n# different maximum holding\nclose_ = market_df.close['2009':'2020']\nrolling = np.arange(20,260,10)\nmhs = [20,60,120,260]\nwin_ratios = pd.DataFrame()\n\nfor mh in mhs:\n    max_holding = [mh, 0]\n    dynamic_targets = []\n    for j in rolling:\n        for i in range(2,j+1):\n            changes = close_.pct_change(1).to_frame()\n            changes = changes.join(close_.pct_change(i).rename('close {}'.format(i)))\n        dynamic_target = changes.abs().dropna().mean(axis=1)['2010':]\n        dynamic_targets.append(dynamic_target)\n\n    barrier_exit_list_rolling=[]\n    for i in range(len(dynamic_targets)):\n        barrier_exit_list_rolling.append(get_barrier(close, enter_list[1], [1,1], max_holding, target = dynamic_targets[i]))  #dynamic  \n\n    rts_exit_list=[]\n    for i in range(len(barrier_exit_list_rolling)):\n        rts_exit_list.append(make_rt(close,barrier_exit_list_rolling[i].dropna()))\n\n    result_df = pd.concat([round_trip.get_df_ann_sr(rts_bm,'No Rule')], axis=1)\n    for i in range(len(rts_exit_list)):\n        result_df = result_df.join(round_trip.get_df_ann_sr(rts_exit_list[i],'{}'.format(rolling[i])))\n\n    win_ratios['Max. holding {} days'.format(mh)] = result_df.T.win_ratio\nwin_ratios\n\n\n\n\n\n\n\n\nMax. holding 20 days\nMax. holding 60 days\nMax. holding 120 days\nMax. holding 260 days\n\n\n\n\nNo Rule\n0.518835\n0.518835\n0.518835\n0.518835\n\n\n20\n0.541054\n0.576857\n0.572169\n0.575130\n\n\n30\n0.564014\n0.573898\n0.570069\n0.570441\n\n\n40\n0.532872\n0.559689\n0.553155\n0.550562\n\n\n50\n0.555363\n0.588591\n0.589455\n0.587727\n\n\n60\n0.564991\n0.605195\n0.605195\n0.603463\n\n\n70\n0.562392\n0.605195\n0.600866\n0.595671\n\n\n80\n0.557192\n0.600000\n0.584416\n0.586147\n\n\n90\n0.555459\n0.599134\n0.598787\n0.601732\n\n\n100\n0.563258\n0.587522\n0.600520\n0.599653\n\n\n110\n0.555459\n0.575022\n0.595486\n0.604510\n\n\n120\n0.561525\n0.585281\n0.611785\n0.620451\n\n\n130\n0.560659\n0.593560\n0.608014\n0.620209\n\n\n140\n0.559792\n0.601739\n0.607485\n0.623151\n\n\n150\n0.555844\n0.579496\n0.578261\n0.597391\n\n\n160\n0.545927\n0.574413\n0.581010\n0.600174\n\n\n170\n0.558925\n0.598432\n0.598082\n0.625981\n\n\n180\n0.551127\n0.586297\n0.594266\n0.615451\n\n\n190\n0.548918\n0.594805\n0.601386\n0.629116\n\n\n200\n0.548918\n0.591342\n0.606586\n0.623050\n\n\n210\n0.535065\n0.596886\n0.612987\n0.627706\n\n\n220\n0.555844\n0.614187\n0.628571\n0.643290\n\n\n230\n0.552768\n0.595506\n0.605195\n0.624567\n\n\n240\n0.561525\n0.618387\n0.629887\n0.647569\n\n\n250\n0.559792\n0.609714\n0.625543\n0.644097\n\n\n\n\n\n\n\n\nplt.figure(figsize=(15,6))\nplt.title(\"Exit rules\")\nplt.plot(win_ratios)\nplt.legend(win_ratios)\nplt.ylabel('win ratio')\nplt.xlabel('Rolling days for calculating target rate')\nplt.show()"
  },
  {
    "objectID": "posts/systemic_risk/1_Systemic_Risk.html",
    "href": "posts/systemic_risk/1_Systemic_Risk.html",
    "title": "시스템 리스크 분석: (1) 시스템 리스크 개념",
    "section": "",
    "text": "시스템 리스크\n\n시스템 리스크 (systemic risk): 금융중개기능이 원활히 작동하지 못하여 경제성장과 사회후생에 심각하게 손상을 줄 정도의 심각한 금융불안정을 지칭하는 시스템적 사건이 발생할 위험 - 유럽중앙은행(ECB(2010))\n\n\n\n\nSystemic_Risk\n\n\n금융에서 시스템리스크는 금융시스템 또는 한 지역·국가의 전체 시장, 나아가 글로벌 시장의 붕괴로 이어지는 위기를 말한다. 2008년 글로벌 금융위기의 강력한 파괴력과 거대한 연쇄반응은 금융안정과 관련하여 시스템리스크의 중요성을 인식하게 했다.\n전통적인 금융 리스크와 다른 가장 중요한 특징은 “내부 및 외부 요인의 지속적인 반응으로 인해 전체 금융 시스템에 걸친 위험 전이 및 시스템 장애의 문제”이다. 따라서 시스템 리스크에 따른 결과는 “계단식” 실패로 이어지고 금융 시스템의 개인에게 영향을 미쳐 은행 시스템이 유동성 문제와 지불 불가 상태가 된다. 즉, 시스템 금융 리스크는 글로벌 또는 지역 금융 시스템의 유동성 위험을 유발하는 연쇄적 위기로 설명될 수 있다(Silva, Kimura, & Sobreiro, 2017).\n지난 10년이 넘는 기간 동안 금융 생태계의 연구, 금융 감독, 국경 간 자본 흐름 모니터링 등 시스템적 금융 위험에 초점을 맞춘 많은 양의 연구가 있었다. 그러나 현대 금융은 이미 광범위하고 상호 연결된 네트워크를 갖춘 상호의존적 시스템으로 구성되어 있으며, 이는 점점 더 글로벌해지는 사회의 특성을 따르고 있다.\n따라서 현행 금융의 시스템 리스크에 대응하기 위해서는 비정상적인 위험행위를 자동으로 탐지하고 금융시장의 대규모 금융데이터를 신속하게 처리함으로써 정보기술(IT) 기법을 활용하여 위험 단서와 목적을 신속하게 파악하고 발견할 수 있는 새로운 도구가 개발되어야 한다. 지능적이고 자동화된 기계 학습 방법은 위험 성향과 함께 점점 더 복잡해지는 금융 네트워크, 금융 거래의 빅데이터, 시장 정서 등으로 인한 시스템 위험을 평가하고 탐지하는 도구가 될 수 있다.\n\n\n시스템 리스크의 특징\n시스템 리스크의 특징을 설명한 선행연구\n\nCifuentes(2003): 금융시스템에 집중도(concentration)가 높아질수록 충격이 금융기관들 간에 신속하게 잘 전파되어 시스템 리스크가 높아진다고 주장\nDe Nicolo et al.(2012): 미시적 금융불안정이 시스템리스크로 확산되는 과정에서 부의 외부성(negative externalities)이 중요한 역할을 한다고 강조\nBIS(2010): 시스템리스크를 특정 시점을 기준으로 위험도가 금융시스템에 어떤 모습으로 분포되어 있는지에 주목하는 횡단면 위험과 시스템리스크가 시간에 따라 증가하는지 여부에 주목하는 시계열적 위험으로 구분\n\n소수의 대형 금융기관에 위험이 집중되거나 금융기관들 간에 상호 연계성이 강할수록 시스템리스크는 커짐(횡단면 위험)\n경기호황기의 지나친 신용을 공급은 경기불황기에 나타날 수 있는 금융불안정을 더 심화시킴(시계열적 위험)\n\n\n\n\n시스템 리스크 요인\n시스템 리스크를 축적시키는 요인\n\n투자자, 금융기관의 집단적 협조실패(coordination failure): 은행예금인출 쇄도(bank run), 자산 급매도(asset fire-sale)\n정보 요인: 근시안적 행동(myopia), 정보의 비대칭성, 지나친 위험추구 성향\n유인체계: 금융기관의 도덕적 해이, 단기 위험 추구 성향을 부추기는 보상체계\n규제 및 감독체계 오류: 그림자 금융(shadow banking), 규제자본의 경기순응성\n\n시스템 리스크를 촉발시키는 요인\n\n금융시스템의 외부(exogenous) 또는 내부(endogenous) 충격(shock)\n신용위험이나 유동성 위험과 같은 미시적 성격의 충격 또는 공통요인의 발현과 같은 거시적 충격\n소수의 대형 금융기관의 도산 또는 소규모이지만 다수의 금융기관들이 한꺼번에 도산하는 형태\n\n시스템 리스크의 전염과 확산\n\n금융기관들 간의 다양한 형태의 부내(balance sheet) 또는 부외(off-balance sheet) 직접적 거래관계\n정보 경로: 자산급매도, 쏠림현상(herding), 비이성적 행동 등\n금융기관들의 투자나 위험관리 측면에서의 유사성\n해외은행의 큰 비중 (해외의 외생적 충격)\n금융시스템의 구조적 취약성(structural vulnerabilities)\n\n높은 단기자금조달 의존도\n노출 위험의 높은 유사성\n중앙청산체계가 마련되어 있지 않은 장외거래 시장에 대한 높은 의존도\n큰 비중을 차지하는 그림자 금융\n높은 경기순응성을 보이는 신용공급\n금융기관의 심각한 만기 및 통화의 불일치\n\n\n\n\n참고 문헌\n\nKou, G., Chao, X., Peng, Y., Alsaadi, F. E., & Herrera-Viedma, E. (2019). Machine learning methods for systemic risk analysis in financial sectors. Technological and Economic Development of Economy, 25(5), 716-742.\nSiegmann, A. (2017). Policy Lessons from Systemic Risk Modeling and Measurement. In Systemic Risk Tomography (pp. 239-273). Elsevier.\n서상원. (2018). 시스템리스크의 측정과 관리: 서베이와 제언. 금융안정연구, 19(1), 131-232."
  },
  {
    "objectID": "posts/systemic_risk/2-2_Systemic_Risk_Measures_Market.html",
    "href": "posts/systemic_risk/2-2_Systemic_Risk_Measures_Market.html",
    "title": "시스템 리스크 분석: (2-2) 시장위험 측정 방법",
    "section": "",
    "text": "서상원 (2018)\n시스템리스크 측정 방법 중 시장위험을 측정하는 방법들을 살펴본다."
  },
  {
    "objectID": "posts/systemic_risk/2-2_Systemic_Risk_Measures_Market.html#turb-turbulence-index",
    "href": "posts/systemic_risk/2-2_Systemic_Risk_Measures_Market.html#turb-turbulence-index",
    "title": "시스템 리스크 분석: (2-2) 시장위험 측정 방법",
    "section": "TURB: Turbulence index",
    "text": "TURB: Turbulence index\nKritzman & Li (2010)\n금융시장이나 자산시장에서 형성되는 가격의 변동이 통상적인 패턴에서 벗어나는 경우를 금융혼란(financial turbulence)이라고 정의하고 이를 “Mahalanobis distance”라는 측도를 사용하여 측정\n\n혼란지수(turbulence index)는 여러 자산의 수익률들이 자신의 과거 평균치로부터 얼마나 괴리되어 형성되는지를 변동성을 감안하여 평균적으로 측정\n\\(d_t \\equiv (y_t - \\mu)\\sum^{-1}(y_t - \\mu)'\\)\n\n\\(y_t\\)는 자산 수익률 벡터, \\(\\mu\\)는 자산 수익률 표본평균 벡터, \\(\\sum\\)은 자산 수익률의 표본공분산 행렬\n\n분석 결과:\n\n혼란지수로 판단할 때 글로벌 금융위기기간이 1980년대 이후 혼란도가 가장 높았던 시기\n금융 혼란기와 평온기를 비교하면, 금융자산의 수익률-위험 비율은 평온기에 비해 혼란기에서 훨씬 낮게 나타남\n혼란기가 시작되면 대략 1개월 이상의 기간동안 혼란기가 지속되는 특성 관측.\n\n\n\n\n\nKritzman & Li (2010)_Figure 4"
  },
  {
    "objectID": "posts/systemic_risk/2-2_Systemic_Risk_Measures_Market.html#ciss-composite-indicator-of-systemic-stress",
    "href": "posts/systemic_risk/2-2_Systemic_Risk_Measures_Market.html#ciss-composite-indicator-of-systemic-stress",
    "title": "시스템 리스크 분석: (2-2) 시장위험 측정 방법",
    "section": "CISS: Composite Indicator of Systemic Stress",
    "text": "CISS: Composite Indicator of Systemic Stress\nHollo et al.(2012)\n다수의 금융시장 지표들을 종합하여 금융시스템의 위험 상황을 알려주는 종합지표\n\n금융시스템을 단기금융시장, 채권시장, 주식시장, 금융기관 및 외환 시장 등 5개의 하위부문으로 구분, 각 하위부문별로 3개씩의 정보변수들을 고려하여 총 15개의 변수를 이용하여 종합지수를 작성\n\n\n\n\nHollo_et_al._(2012)_Figure 1\n\n\n\n각 하위부문별로 3개씩의 정보변수 각각을 경험 누적분포함수(empirical cumulative distribution function)를 이용하여 [0, 1] 사이의 값을 가지도록 변환, 변환된 변수들을 단순 평균하여 해당 하위부문의 종합지수를 산출\n\\({CISS}_t = (w \\circ s_t)C_t(w \\circ s_t)'\\)\n\n\\(w\\)는 5개의 하위부문에 대한 가중치 벡터 (주관적)\n\\(s_t\\)는 5개의 하위부문 종합지수 벡터\n= Hadamard product (벡터 원소간 곱)\n\\(C_t\\)는 5개의 하위부문 종합지수간 상관계수 행렬\n\nCISS에 마코프 전환모형(Markov switching model)을 적용하여 정상, 경계, 위험 등 3개의 국면을 식별\n\n\n\n\nHollo_et_al._(2012)_Figure 8"
  },
  {
    "objectID": "posts/systemic_risk/2-2_Systemic_Risk_Measures_Market.html#nois-market-noise",
    "href": "posts/systemic_risk/2-2_Systemic_Risk_Measures_Market.html#nois-market-noise",
    "title": "시스템 리스크 분석: (2-2) 시장위험 측정 방법",
    "section": "NOIS: Market noise",
    "text": "NOIS: Market noise\nHu, Pan, and Wang(2013)\n미국 국채시장에서의 일별 거래정보를 통해 시장 잡음(market noise)을 측정\n\n이자율의 term structure에 대한 Svensson(1994) 모형을 이용하여 순간 선도이자율 (instantaneous forward rate) \\(f\\) 를 다음과 같이 표시\n\n\\(f(m,b) = \\beta_0+\\beta_1 exp(-\\frac{m}{\\tau_1})+\\beta_2\\frac{m}{\\tau_1}exp(-\\frac{m}{\\tau_1})+\\beta_3\\frac{m}{\\tau_2}exp(-\\frac{m}{\\tau_2})\\)\n\\(m\\)은 만기\n\\(b = (\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\tau_1, \\tau_2)\\)는 모수 벡터\n\n실제 거래된 미 국채수익률이 추정된 수익률곡선으로부터 평균적으로 얼마나 괴리되어 있는지를 다음과 같이 측정\n\n\\({Noise}_t = \\sqrt{\\frac{1}{N_t}\\sum^{N_t}_{i=1}[y^i_t-y^i(b_t)]^2}\\)\n\\(N_t\\)는 미국 국채 거래건수, \\(y^i_t\\)는 미국 국채의 실현된 수익률, \\(y^i(b_t)\\)는 수익률 곡선에 의해 추정된 수익률\n\n미국 국채시장이 금융시장의 중심적 위치를 차지하기 때문에 미국 국채시장에서의 시장 잡음은 단지 국채시장의 유동성만을 나타내는 것이 아니라 전반적인 금융시장의 유동성 상황을 나타낸다고 주장\n분석 결과: 시장 잡음 지표는 bid-ask spread나 on-the-run premium과 같은 통상적인 시장 유동성 지표보다 위기시에 더욱 민감한 움직임을 보임으로써 금융시장의 유동성 상황을 보다 적절히 반영하는 것으로 평가\n\n\n\n\nHu,Pan, and_Wang(2013)_Figure 2"
  },
  {
    "objectID": "posts/systemic_risk/2-2_Systemic_Risk_Measures_Market.html#pca-ar-pricipal-component-analysis-absorption-ratio",
    "href": "posts/systemic_risk/2-2_Systemic_Risk_Measures_Market.html#pca-ar-pricipal-component-analysis-absorption-ratio",
    "title": "시스템 리스크 분석: (2-2) 시장위험 측정 방법",
    "section": "PCA-AR: Pricipal component analysis-Absorption Ratio",
    "text": "PCA-AR: Pricipal component analysis-Absorption Ratio\nKritzman et al. (2010)\n금융자산 간에 강한 연관성이 형성되면 위기시에 부정적 영향이 보다 신속하고 광범위하게 파급됨\n\n\\(AR=\\frac{\\sum^{n}_{i=1}\\lambda_i}{\\sum^{N}_{i=1}\\lambda_i}\\)\n\n\\(\\lambda\\)는 \\(i\\)번째 eigenvalue (N개의 금융자산의 과거 수익률을 PCA 기법으로 분석했을 때의 주성분 공분산)\n\\(n\\)개 주성분\n\n실험: 미국의 주식시장에 대해 51개 산업별 지수간 동조성이 변화된 모습을 AR을 기준으로 측정 (N=51, n=10)\nAR이 크게 증가한 이후에 미국 주가가 크게 하락한 현상이 나타났으며, 반대로 AR이 낮은 모습을 보인 기간 이후에 주가가 상승함. 또한, AR은 주식시장 뿐만 아니라 주택시장 등을 포함하는 전반적인 금융 및 자산시장의 불안정과 국제 금융위기에 대해 선행성을 지니는 것으로 나타남.\n\n\n\n\nKritzman_et_al._(2010)_Figure 5"
  },
  {
    "objectID": "posts/systemic_risk/2-4_Systemic_Risk_Measures_Probability_of_Default.html",
    "href": "posts/systemic_risk/2-4_Systemic_Risk_Measures_Probability_of_Default.html",
    "title": "시스템 리스크 분석: (2-4) 금융기관 부도확률 위험 측정방법",
    "section": "",
    "text": "서상원 (2018)\n시스템리스크 측정 방법 중 금융기관의 부도와 같은 위험사건을 명시적으로 정의하는 측정하는 법을 알아본다."
  },
  {
    "objectID": "posts/systemic_risk/2-4_Systemic_Risk_Measures_Probability_of_Default.html#cds-dip-credit-default-swap-distress-insurance-premium",
    "href": "posts/systemic_risk/2-4_Systemic_Risk_Measures_Probability_of_Default.html#cds-dip-credit-default-swap-distress-insurance-premium",
    "title": "시스템 리스크 분석: (2-4) 금융기관 부도확률 위험 측정방법",
    "section": "CDS-DIP : Credit default swap-distress insurance premium",
    "text": "CDS-DIP : Credit default swap-distress insurance premium\nHuang et al.(2009)\n\nDIP(distress insurance premium): CDS 시장가격을 이용하여 금융시스템내에 개별 금융기관들이 발행한 CDS들의 가상 포트폴리오(가중치: 금융기관의 부채규모)에 대해서 금융 위험(financial distress) 사건이 발생할 경우 보호받기 위해서 지불하여야 하는 프리미엄을 산출\nCDS 스프레드:\n\n금융기관 \\(i\\)의 신용사건을 기초자산으로 하는 CDS 스프레드 \\(s_{it}\\)는 해당 금융기관이 위험상황에 처할 경우 발생하는 금융손실의 위험을 보장받을 것을 대가로 ‘보장’ 매입자가 일정기간마다 지불하여야 하는 (원금 대비) 프리미엄 비율\n금융기관의 위험확률이 증가하면 CDS 스프레드도 같이 증가하므로 CDS 스프레드 시장가격은 금융기관의 위험도에 대해 시장에서 평가하는 정보를 가짐\n\n\\(PD_{it} = \\frac{a_{t}s_{it}}{a_t LGD_{it}+b_t s_{it}}\\)\n\\(a_t \\equiv \\int^{t+T}_{t}{e^{-r\\tau}d\\tau}\\)\n\\(b_t \\equiv \\int^{t+T}_{t}{\\tau^{-r\\tau}d\\tau}\\)\n\\(r\\)은 무위험수익률, \\(T\\)는 CDS 계약의 잔존만기, \\(LGD\\)는 부도시손실률(loss given default)을 나타냄\n이렇게 추정된 부도확률은 위험중립 확률(risk-neutral probability)이므로 이를 이용하여 산출한 DIP을 금융상품의 가격 즉, 프리미엄으로 간주\n또한 여러 금융기관간 위험 사건의 종속성을 시변하는 상관계수를 이용하여 측정 (향후 12주 후에 대한 상관계수 예측식)\n\n\\(\\rho_{t,t+12}=c+k_{1}\\rho_{t-12,t}+\\sum^{l}_{i=1}k_{2i}\\rho_{t-i,t-1+1}+\\eta X_t+v_t\\)\n\\(\\rho\\)는 하첨자로 표시된 (일주일 단위의) 두 기간간 평균 자산수익률 상관계수, \\(X_t\\)는 예측변수로 사용되는 금융시장 변수\n\n예측된 상관계수를 이용하여 DIP을 추정\n\n\n\n\nHuang et al. (2009) Figure 1"
  },
  {
    "objectID": "posts/systemic_risk/2-4_Systemic_Risk_Measures_Probability_of_Default.html#cds-corisk-credit-default-swap-corisk",
    "href": "posts/systemic_risk/2-4_Systemic_Risk_Measures_Probability_of_Default.html#cds-corisk-credit-default-swap-corisk",
    "title": "시스템 리스크 분석: (2-4) 금융기관 부도확률 위험 측정방법",
    "section": "CDS-CoRisk: Credit default swap-CoRisk",
    "text": "CDS-CoRisk: Credit default swap-CoRisk\nIMF(2009)\n금융기관간 위험도의 상호연계성을 CDS 스프레드를 이용하여 측정\n\nCoRisk는 quantile regression 기법을 사용하여 한 금융기관의 위험 사건 여부가 다른 금융기관에 미치는 영향을 측정\n\n\\(CDS_{it}=\\alpha_{\\tau}+\\sum^{K}_{k=1}\\beta_{\\tau k}R_{kt}+\\gamma_{\\tau j}CDS_{jt}+\\epsilon_{it}\\)\n\\(\\tau\\)는 백분위수, \\(R\\)은 \\(K\\)개의 위험요인\n위험요인: 일반적인 리스크 프리미엄, 수익률곡선의 기울기, LIBOR 스프레드, 단기자금시장의 유동성 상황, VIX 등\n\nCDS 스프레드가 높을수록 고위험도를 나타내며, 95 percentile을 상회하는 경우를 위험 사건이 발생한 것으로 간주\nCoRisk (금융기관 \\(j\\)에 대한 \\(i\\)의 조건부)\n\n\\(Conditional CoRisk_t(i,j) = 100 \\times (\\frac{CDS(95|j)}{CDS_i(95)}-1 )\\)\n\\(CDS(95|j)=\\alpha_{95}+\\sum^{K}_{k=1}\\beta_{95k}R_{kt}+\\gamma_{95j}CDS_j(95)\\)\nCoRisk는 한 금융기관의 CDS 스프레드의 무조건부 95 percentile과 다른 금융기관에서 위험 사건이 발생한 경우에 대한 조건부 95 percentile간의 상대적 차이를 나타냄\n\n\n\n\n\nIMF(2009) Fig2.5.\n\n\n\n\n\nIMF(2009) Fig2.6."
  },
  {
    "objectID": "posts/systemic_risk/2-4_Systemic_Risk_Measures_Probability_of_Default.html#option-ipod-option-implied-probability-of-default",
    "href": "posts/systemic_risk/2-4_Systemic_Risk_Measures_Probability_of_Default.html#option-ipod-option-implied-probability-of-default",
    "title": "시스템 리스크 분석: (2-4) 금융기관 부도확률 위험 측정방법",
    "section": "Option-iPoD : Option-implied probability of default",
    "text": "Option-iPoD : Option-implied probability of default\nCapuano(2008)\n옵션가격으로부터 부도확률을 추정\n\nMerton(1974)의 구조모형을 이용\n\n금융기관의 (시장가치 기준) 자산을 \\(V\\), 부채를 \\(D\\), 자본을 \\(E\\)라고 하면, 금융기관의 부도사건은 \\(V&lt;D\\)로 정의\n자본은 부채를 먼저 상환하고 남는 부분에 대한 청구권으로서 \\(E=\\max(V-D,0)\\)\n이는 마치 기초자산을 \\(V\\)로, 행사가격을 \\(D\\)로 하는 콜옵션과 동일한 구조\n\n부도확률(PoD):\n\n\\(PoD = \\int^{D}_0 fdV\\)\n\n확률밀도함수 \\(f\\) 는 CIMDO 방법을 이용하여 다음과 같이 추정\n\n\\(\\min \\int^{\\infty}_0 f(V)ln(\\frac{f(V)}{f^0(V)})dV\\)\n사후 (posterior) 밀도함수 \\(f\\)는 사전(prior) 밀도함수 \\(f^0\\) 와의 cross-entropy를 최소화\n\n사후 밀도함수의 조건\n\n자본이 콜옵션 가치로 평가됨\n\n\\(E=\\int^{\\infty}_{D}(V-D)f(V)dV\\)\n\n행사가격이 \\(K\\)인 주식옵션의 가격이 \\(C\\)일 때 이 옵션가격과 일치성의 가짐\n\n\\(C=\\int^{\\infty}_{D+K}(V-D-K)f(V)dV\\)\n\n확률밀도함수의 면적은 1\n\n\\(1=\\int^{\\infty}_{0}f(V)dV\\)\n\n\n사후 밀도함수\n\n\\(f(V) = f^0(V)\\exp [ \\lambda_0-1+\\lambda_1e^{-rT}I_{V&gt;D}(V-D)+\\lambda_2e^{-rT}I_{V&gt;D+K}(V-D-K) ]\\)\n\\(\\lambda_0\\)는 확률밀도함수의 면적이 1이라는 조건에 대한 라그랑지 승수, \\(\\lambda_1\\)는 자본이 콜옵션의 가치로 평가된다는 조건에 대한 라그랑지 승수, \\(\\lambda_2\\)는 주식옵션의 가격이 만족하여야 하는 조건에 대한 라그랑지 승수"
  },
  {
    "objectID": "posts/systemic_risk/2-4_Systemic_Risk_Measures_Probability_of_Default.html#cca-contingent-claim-approach",
    "href": "posts/systemic_risk/2-4_Systemic_Risk_Measures_Probability_of_Default.html#cca-contingent-claim-approach",
    "title": "시스템 리스크 분석: (2-4) 금융기관 부도확률 위험 측정방법",
    "section": "CCA: Contingent claim approach",
    "text": "CCA: Contingent claim approach\nLehar(2005)\n주가에 기업의 부도확률에 대한 정보가 내재되어 있다는 점에 기반하여 주가 정보를 이용하여 내재된 부도확률에 대한 정보를 얻는 방법\n\nMerton(1974)의 콜옵션 구조모형(\\(E=\\max(V-D,0)\\))에서 주식이 일종의 콜옵션과 같은 조건부 증권(contingent claim)으로 봄\n금융기관의 자산 \\(V\\)가 다음과 같이 geometric 브라운 운동으로 따른다고 가정\n\n\\(dV=\\mu Vdt+\\sigma Vdz\\)\n주가 \\(E\\) 는 행사가격이 \\(D_t\\), 기초자산 가격이 \\(V_t\\)인 BSM(Black-Scholes-Merton) 콜옵션가격과 동일\n\n금융기관의 자산에 대한 확률과정의 모수 \\(\\mu, \\sigma\\)를 금융기관의 주가에 대한 정보를 이용하여 추정\n\n다음의 우도함수를 극대화하도록 추정\n\\(L(\\mu,\\sigma)=-\\frac{m-1}{2}\\ln(2\\pi)-\\frac{m-1}{2}\\ln\\sigma^2 - \\sum^{m}_{t=2}\\ln \\hat{V}_t(\\sigma) - \\sum^{m}_{t=2}\\ln N(\\hat{d}_t) - \\frac{1}{2\\sigma^2}\\sum^{m}_{t=2}[\\ln\\frac{\\hat{V}_t(\\sigma)}{\\hat{V}_{t-1}(\\sigma)}]^2\\)\n\\(\\hat{V}_t(\\sigma)\\)는 BSM의 콜옵션가격 공식에서 구해지는 해\n\n금융기관간의 연계성을 다음과 같이 지수가중 이동평균 방법을 이용하여 모형화\n\n금융기관 \\(i, j\\)의 공분산: \\(\\sigma_{ij,t} = \\lambda\\sigma_{ij,t-1}+(1-\\lambda)\\ln(\\frac{V^i_t}{V^i_{t-1}})\\ln(\\frac{V^j_t}{V^j_{t-1}})\\)\n\nLehar(2005)는 시스템리스크 측정지표로서 전체 금융시스템 내에 금융기관중 부실금융기관의 수가 일정 비율 (예를 들어, 10%) 이상일 확률을 나타내는 SIN과 자산기준 부실금융기관의 비중이 일정 비율 이상일 확률을 나타내는 SIV 등을 제시"
  },
  {
    "objectID": "posts/systemic_risk/2_Systemic_Risk_Measures.html",
    "href": "posts/systemic_risk/2_Systemic_Risk_Measures.html",
    "title": "시스템 리스크 분석: (2) 시스템 리스크 측정 방법 - 선행연구 요약",
    "section": "",
    "text": "시스템 리스크를 측정하는 기존의 방법에 대해 알아본다. 각 서베이 논문에서 정리한 표를 살펴보자."
  },
  {
    "objectID": "posts/systemic_risk/2_Systemic_Risk_Measures.html#서상원-2018의-서베이",
    "href": "posts/systemic_risk/2_Systemic_Risk_Measures.html#서상원-2018의-서베이",
    "title": "시스템 리스크 분석: (2) 시스템 리스크 측정 방법 - 선행연구 요약",
    "section": "서상원 (2018)의 서베이",
    "text": "서상원 (2018)의 서베이\n\n\n\n카테고리\n이름\n내용\n관련 논문\n\n\n\n\n거시경제\nFCYCL(금융사이클)\n금융부문의 불균형을 나타내는 집계변수를 고려\nDrehmann, Borio and Tsatsaronis(2012)\n\n\n거시경제\nCCYCL(신용사이클)\nGDP대비 신용 비율 중심 분석\nJuselius and Drehmann(2015)\n\n\n거시경제\nEWS-SE(조기경보모형-신호접근법)\n여러 정보변수들을 비모수적 방법으로 활용하여 금융위기의 발생가능성을 예측\nKaminsky, Lizondo and Reinhart(1998)\n\n\n거시경제\nEWS-LR(조기경보모형-로짓모형)\n여러 거시 및 금융부문의 정보변수를 활용하여 미리 예측\n\n\n\n거시경제\nEWS-Hybrid(조기경보모형-혼합모형)\n여러 거시 및 금융부문의 정보변수를 활용하여 미리 예측\nSuh(2017)\n\n\n시장위험\nTURB(시장혼란지수)\n금융시장 에서의 자산가격형성이 통상적인 패턴에서 벗어나는지를 판단\nKritzman & Li (2010)\n\n\n시장위험\nCISS(Composite Indicator of Systemic Stress)\n여러 금융시장에서 형성되는 중요한 정보변수들을 종합하여 현재의 전반적인 금융시장 상황이 극단적인 모습을 보이는지를 요약\nHollo et al.(2012)\n\n\n시장위험\nNOIS(시장 잡음)\n금융시장에서 차익거래 기회가 얼마나 존재하는지를 개별 금융거래 자료를 이용하여 측정\nHu, Pan, and Wang(2013)\n\n\n시장위험\nPCA-AR(absorbtion ratio)\n금융자산 수익률간의 상관성 정도 측정\nKritzman et al. (2010)\n\n\n금융기관 Distress\nPCAS\n금융자산 수익률간의 상관성 정도와 금융시스템에 대한 개별 금융기관의 기여도 측정\nBillio et al.(2012)\n\n\n금융기관 Distress\nCoVaR\n금융시스템의 주가수익률이 일정 임계치 이상으로 크게 하락하는 것을 시스템리스크라고 하고 한 금융기관에서 심각한 금융위험 사건이 발생하였다는 조건의 부과가 시스템리스크 수준을 얼마나 증가시키는지를 측정\nAdrian & Brunnermeier (2008)\n\n\n금융기관 Distress\nSES(Systemic expected shortfall)\n시스템리스크가 발생한 경우 개별 금융기관의 적정 자본수준 대비 자본의 부족규모 예상 치\nAcharya et al. (2010)\n\n\n금융기관 Distress\nCES(Component expected shortfall)\n금융기관별 MES(시스템리스크가 발생하였을 때 특정 금융기관의 주가 수익률 하락폭의 기댓값)에 해당 금융기관의 비중을 곱한 값\nBanulescu and Dumitrescu(2015)\n\n\n금융기관 Distress\nSRISK\n금융 시스템 위기가 발생한 경우에 개별 금융기관의 자본 부족규모 예상치를 측정 / 개별 금융기관 및 금융시스템의 주가 수익률이 GARCH-DCC 모형을 따른다고 가정\nBrownlees & Engle (2010)\n\n\n금융기관 Distress\nStructural GARCH\n통상적인 GARCH 모형을 수정한 Structural GARCH 모형을 제안하고 이를 SRISK 지표에 적용 - ‘레버리지’ 효과를 반영\nEngle and Siriwardane(2014)\n\n\n금융기관 Distress\nJPod(Joint probability of default)*\n금융시스템내의 모든 금융기관들에서 동시에 위험 사건이 발생할 확률\nSegoviano & Goodhart (2009)\n\n\n금융기관 Distress\nBSI(banking stability index)*\n적어도 하나의 금융기관이 위험할 때 위험 금융기관 수의 기댓값\nSegoviano & Goodhart (2009)\n\n\n금융기관 Distress\nPAO(probability that at least one bank becomes distressed)*\n특정 금융기관이 위험한 상황일 때 다른 금융기관 중 적어도 하나의 금융기관에서 위험 사건이 발생할 확률\nSegoviano & Goodhart (2009)\n\n\n금융기관 Distress\nDDM(distress dependence matrix)*\ni번째 금융기관이 위험할 때 j번째 금융기관이 위험에 처할 조건부확률\nSegoviano & Goodhart (2009)\n\n\n금융기관 부도위험\nCDS**-DIP(Distress insurance premium)\n금융시스템내에 개별 금융기관들이 발행한 CDS들의 (부채규모를 가중치로 하는) 가상적인 포트폴리오에 대해서 신용위험 사건 발생시 보호받기 위해서 지불하여야 하는 프리미엄\nHuang et al.(2009)\n\n\n금융기관 부도위험\nCDS-CoRisk\nCDS 스프레드에 대해 CoVaR와 유사한 개념을 적용\nIMF(2009)\n\n\n금융기관 부도위험\nOption-iPoD(Option-implied probability of default)\nCIMDO 기법을 적용하여 개별 금융기관의 부도확률을 추정하는 방법을 제시\nCapuano(2008)\n\n\n금융기관 부도위험\nCCA*** 기반 시스템리스크\nCCA 방법에 기반하여 금융기관의 부도확률에 대해 추정 - 전체 금융시스템 내에 금융기관중 부실금융기관의 비율이 일정 임계치 이상일 확률\nLehar(2005)\n\n\n금융기관 네트워크\nNETW\n금융기관들간의 직접적인 연계성이 존재하는 경우 네트워크 측정\n\n\n\nAgent-based\nABM\n다수의 독립적인 이질적이고 경제주체들(agents)이 실제 상황에서 행하는 것과 유사하게 상호작용을 하고 새로운 환경에 적응(adapt)하면서 경제적 행동을 하도록 모형화하고 이러한 경제행위들의 상호작용의 결과를 분석\nThurner(2011), Bookstaber et al.(2014), Bookstaber and Paddrik(2015)\n\n\n스트레스 테스트\nSTEST\n스트레스 시나리오를 구성하고, 스트레스 시나리오 하에서 금융기관이 어떤 영향을 받을지 추정하고, 그러한 금융부문의 영향으로 인해 거시경제가 어떤 영향을 받을지 추정하는 과정으로 이루어짐 - CCAR(Comprehensive Capital Analysis and Review), DFAST(Dodd-Frank Act Stress Testing)\n\n\n\n스트레스 테스트\nR(reverse)-STEST\n금융기관을 위험에 처하게 하는 시나리오가 어떤 것인지를 찾는 방법\n\n\n\n\n*개별 금융기관들의 위험 확률(PoD)들로부터 CIMDO(consistent multivariate density optimizing) 기법을 활용하여 개별 금융기관들간의 위험 사건에 대한 결합분포를 구하고 이를 이용한 금융안정 지수들 (Segoviano and Goodhart(2009))\n**CDS 스프레드는 해당 금융기관이 위험사건 발생시 금융손실의 위험을 보장받을 것을 대가로 ‘보장’ 매입자가 일정기간마다 ‘보장’ 매도자에게 지불하여야 하는 (원금 대비) 프리미엄 비율 / CDS 스프레드 시장 가격은 금융기관의 위험도에 대해 시장에서 평가하는 정보를 가지고 있으며, 이 CDS 스프레드를 통해 위험사건의 발생확률을 역산할 수 있음\n***CCA(contingent claim approach)는 만기 시점에서 기업의 (시장가치 기준) 자산이 부채를 하회하는 것을 기업의 부도사건으로 정의하면, 자본은 부채를 먼저 상환하고 남는 부분에 대한 청구권으로서 기초자산을 자산가치로 하고 행사가격을 부채가치로 하는 콜옵션과 동일하다는 Merton(1974)의 구조모형에 기반한 부도확률 측정법 (Merton(1974))\n각 측정 방법의 자세한 설명은 아래 글에 나와있다.\n시스템 리스크 분석: (2-1) 거시경제지표를 활용한 경제전체 위험도 측정방법\n시스템 리스크 분석: (2-2) 시장위험 측정방법\n시스템 리스크 분석: (2-3) 금융기관 단면적 위험 측정방법\n시스템 리스크 분석: (2-4) 금융기관 부도확률 위험 측정방법\n시스템 리스크 분석: (2-5) 금융기관 네트워크 측정방법"
  },
  {
    "objectID": "posts/systemic_risk/2_Systemic_Risk_Measures.html#tommasobelluzzosystemicriskgithub-repo의-서베이",
    "href": "posts/systemic_risk/2_Systemic_Risk_Measures.html#tommasobelluzzosystemicriskgithub-repo의-서베이",
    "title": "시스템 리스크 분석: (2) 시스템 리스크 측정 방법 - 선행연구 요약",
    "section": "TommasoBelluzzo/SystemicRisk(Github Repo)의 서베이",
    "text": "TommasoBelluzzo/SystemicRisk(Github Repo)의 서베이\nhttps://github.com/TommasoBelluzzo/SystemicRisk\n\n\n\n카테고리\n이름\n내용\n관련 논문\n\n\n\n\nBUBBLES DETECTION\nBUB (Bubbles Flag), BMPH (Boom Phases Flag), BRPH (Burst Phases Flag)\n\nPhillips et al. (2015), Phillips & Shi (2018), Phillips & Shi (2019)\n\n\nBUBBLES DETECTION\nBC (Bubbling Capitalization), BCP (Bubbling Capitalization Percentage)\n\nBrunnermeier et al. (2020)\n\n\nCOMPONENT MEASURES\nAR (Absorption Ratio)\n금융자산 수익률간의 상관성 정도 측정\nKritzman et al. (2010)\n\n\nCOMPONENT MEASURES\nCATFIN\n\nAllen et al. (2012)\n\n\nCOMPONENT MEASURES\nCS (Correlation Surprise)\n\nKinlaw & Turkington (2012)\n\n\nCOMPONENT MEASURES\nTI (Turbulence Index)\n금융시장 에서의 자산가격형성이 통상적인 패턴에서 벗어나는지를 판단\nKritzman & Li (2010)\n\n\nCOMPONENT MEASURES\nPCA\n금융자산 수익률간의 상관성 정도와 금융시스템에 대한 개별 금융기관의 기여도 측정\n\n\n\nCONNECTEDNESS MEASURES\nDCI (Dynamic Causality Index), CIO (“In & Out” Connections), CIOO (“In & Out - Other” Connections), Network Centralities\n\nBillio et al. (2011)\n\n\nCROSS-ENTROPY MEASURES\nJPod(Joint probability of default)\n금융시스템내의 모든 금융기관들에서 동시에 위험 사건이 발생할 확률\nSegoviano & Goodhart (2009)\n\n\nCROSS-ENTROPY MEASURES\nFSI (Financial Stability Index)\n적어도 하나의 금융기관이 위험할 때 위험 금융기관 수의 기댓값\nSegoviano & Goodhart (2009)\n\n\nCROSS-ENTROPY MEASURES\nPCE(Probability of Cascade Effects)\n특정 금융기관이 위험한 상황일 때 다른 금융기관 중 적어도 하나의 금융기관에서 위험 사건이 발생할 확률\nSegoviano & Goodhart (2009)\n\n\nCROSS-ENTROPY MEASURES\nDiDE(distress dependency)\ni번째 금융기관이 위험할 때 j번째 금융기관이 위험에 처할 조건부확률\nSegoviano & Goodhart (2009)\n\n\nCROSS-ENTROPY MEASURES\nSI (Systemic Importance)\n\n\n\n\nCROSS-ENTROPY MEASURES\nSV (Systemic Vulnerability)\n\n\n\n\nCROSS-ENTROPY MEASURES\nCoJPoDs (Conditional Joint Probabilities of Default)\n\n\n\n\nCROSS-QUANTILOGRAM MEASURES\nFull Cross-Quantilograms, Partial Cross-Quantilograms\n\nHan et al. (2016)\n\n\nCROSS-SECTIONAL MEASURES\nIdiosyncratic Metrics: Beta, Value-at-Risk & Expected Shortfall\n\n\n\n\nCROSS-SECTIONAL MEASURES\nCAViaR (Conditional Autoregressive Value-at-Risk)\n\nWhite et al. (2015)\n\n\nCROSS-SECTIONAL MEASURES\nCoVaR & Delta CoVaR (Conditional Value-at-Risk)\n금융시스템의 주가수익률이 일정 임계치 이상으로 크게 하락하는 것을 시스템리스크라고 하고 한 금융기관에서 심각한 금융위험 사건이 발생하였다는 조건의 부과가 시스템리스크 수준을 얼마나 증가시키는지를 측정\nAdrian & Brunnermeier (2008)\n\n\nCROSS-SECTIONAL MEASURES\nMES (Marginal Expected Shortfall), SES (Systemic Expected Shortfall)\n시스템리스크가 발생한 경우 개별 금융기관의 적정 자본수준 대비 자본의 부족규모 예상 치\nAcharya et al. (2010)\n\n\nCROSS-SECTIONAL MEASURES\nSRISK (Conditional Capital Shortfall Index)\n금융 시스템 위기가 발생한 경우에 개별 금융기관의 자본 부족규모 예상치를 측정 / 개별 금융기관 및 금융시스템의 주가 수익률이 GARCH-DCC 모형을 따른다고 가정\nBrownlees & Engle (2010)\n\n\nDEFAULT MEASURES\nD2C (Distance To Capital)\n\nChan-Lau & Sy (2007)\n\n\nDEFAULT MEASURES\nD2D (Distance To Default)\n\nVassalou & Xing (2004)\n\n\nDEFAULT MEASURES\nDIP (Distress Insurance Premium)\n\nBlack et al. (2012)\n\n\nDEFAULT MEASURES\nSCCA (Systemic Contingent Claims Analysis)\n\nJobst & Gray (2013)\n\n\nLIQUIDITY MEASURES\nILLIQ (Illiquidity Measure)\n\nAmihud (2002)\n\n\nLIQUIDITY MEASURES\nRIS (Roll Implicit Spread)\n\nHasbrouck (2009)\n\n\nLIQUIDITY MEASURES\nClassic Indicators: Hui-Heubel Liquidity Ratio, Turnover Ratio & Variance Ratio\n\nChan-Lau & Sy (2007)\n\n\nREGIME-SWITCHING MEASURES\n2-States Model: High & Low Volatility\n\nBillio et al. (2010)\n\n\nREGIME-SWITCHING MEASURES\n3-States Model: High, Medium & Low Volatility\n\nBillio et al. (2010)\n\n\nREGIME-SWITCHING MEASURES\n4-States Model: High & Low Volatility With Corrections\n\nBillio et al. (2010)\n\n\nREGIME-SWITCHING MEASURES\nAP (Average Probability of High Volatility)\n\nAbdymomunov (2011)\n\n\nREGIME-SWITCHING MEASURES\nJP (Joint Probability of High Volatility)\n\nAbdymomunov (2011)\n\n\nSPILLOVER MEASURES\nSI (Spillover Index), Spillovers From & To, Net Spillovers\n\nDiebold & Yilmaz (2008), Diebold & Yilmaz (2012), Diebold & Yilmaz (2014)\n\n\nTAIL DEPENDENCE MEASURES\nACHI (Average Chi)\n\nBalla et al. (2014)\n\n\nTAIL DEPENDENCE MEASURES\nADR (Asymptotic Dependence Rate)\n\nBalla et al. (2014)\n\n\nTAIL DEPENDENCE MEASURES\nFRM (Financial Risk Meter)\n\nMihoci et al. (2020)"
  },
  {
    "objectID": "posts/systemic_risk/3_Systemic_Risk_And_Machine_Learning_1_2.html",
    "href": "posts/systemic_risk/3_Systemic_Risk_And_Machine_Learning_1_2.html",
    "title": "시스템 리스크 분석: (3-2) 시스템 리스크와 머신러닝 - 계량경제학, 미시구조, 금융규제",
    "section": "",
    "text": "G. Kou et al. (2019)"
  },
  {
    "objectID": "posts/systemic_risk/3_Systemic_Risk_And_Machine_Learning_1_2.html#계량경제학과-통계학",
    "href": "posts/systemic_risk/3_Systemic_Risk_And_Machine_Learning_1_2.html#계량경제학과-통계학",
    "title": "시스템 리스크 분석: (3-2) 시스템 리스크와 머신러닝 - 계량경제학, 미시구조, 금융규제",
    "section": "계량경제학과 통계학",
    "text": "계량경제학과 통계학\n시스템 리스크 측정의 전통적인 모델로 Conditional Value as Risk, Conditional-Risk 및 Systemic Expected Loss(SES) 등이 있다.\n\nAcharya(2009)는 은행의 유한 책임과 한 은행의 실패로 인한 부정적인 외부 효과의 존재가 시스템적 위험으로 이어진다고 제안했다.\nAcharya, Pedersen, Philippon and Richardson(2010)은 금융 위기로 충격을 받은 개별 은행의 자본 격차를 측정하는 데 SES를 사용할 수 있다고 제안했다.\nAcharya et al.(2012)는 레버리지와 예상 손실의 함수인 자본 부족에 기반한 시스템 위험의 새로운 측정을 설명했다.\nBrownlees and Engle(2011, 2017)은 금융기관에서 위험의 경고 신호로 작용할 수 있는 GARCH 및 SRISK 방법을 사용하여 자본 부족 측정을 제안했다.\n\n특별한 금융기관 간 위험상태의 상호의존성을 연구하기 위해서는 단면적(cross-sectional) 방법을 사용한다.\n\nCarmassi and Herring(2016)은 금융기관의 구조적 복잡성 관점에서 체계적으로 중요한 은행의 복잡성과 시스템적 위험 간의 관계를 연구하였다. 패널 데이터의 결과로 대규모 인수합병이 여전히 복잡성의 주요 원인임을 보여주었다.\nBattaglia and Gallo(2017)는 이사회의 독립성 외에도 이사회의 특성이 은행의 시스템적 위험에 상당한 영향을 미친다고 보았다. 자본 관리, 소유권 구조 및 은행 사업 통제는 모두 이 관계에 어느 정도 영향을 미친다.\n\n유동성 부족은 항상 뱅크런(bank run)의 위험을 야기하며, 유동성 감소는 은행시스템의 연쇄반응으로 시스템 리스크를 초래한다.\n\nCox와 Wang(2014)은 비유동 대출 비율과 은행 간 자금 조달 시장에 대한 노출을 포함하여 은행 실패의 예측 변수를 연구하고 관리자가 위험을 예측하는 데 도움이 될 수 있는 징후를 식별했다.\nLaeven et al.(2016)은 시스템 리스크의 증가는 은행의 규모에 비례하고 은행의 자본금에 반비례한다는 사실을 발견했다. 따라서 추가자본요구가 시스템리스크 예방에 크게 기여하지 않는다고 제안하였다.\nCalmès and Théoret(2013)은 시스템 은행 위험 수준을 포착하는 데 더 효과적인 새로운 유형의 위험 가중 자산 관리 지수를 개발했다.\n\n고전적인 회귀 방법은 금융의 시스템 위험을 분석하는 데 널리 사용되는 방법이기도 하다.\n\nBalogh(2012)는 회귀분석을 통해 유럽 은행시스템의 부실자산 비율과 자본적정성 비율에 영향을 미치는 요인을 연구한 후 거시건전성 규제체계의 주요 내용을 분석하고 시스템 리스크에 대한 일련의 모니터링 및 조기경보지표를 개발하였다.\nCalabrese and Giudici(2015)는 실제 부도 상황과 M&A 상황에서 관련된 거시경제적 요인을 기반으로 이진 데이터에 대한 로지스틱 회귀 방법을 적용했다.\n\n예를 들어 유명한 DSGE(Dynamic Stochastic General Equilibrium) 모델과 같은 거시 경제 방법은 금융 위험과 관련하여 실물 경제에 미치는 영향을 분석하는 데에도 사용된다.\n\nBrunnermeier and Sannikov(2014)는 성장률을 연구하기 위해 DSGE를 구축했으며 자본 가격의 급격한 변동으로 인해 불안정할 것이라는 사실을 발견했다.\nDuca and Peltonen(2013)은 시스템적 위험에 대해 M2, 주택 가격 및 GDP를 결합한 거시 지수를 제안했다.\nHuang, Zhou, and Zhu(2012)는 “역사적 상관관계” 및 시뮬레이션 관점에서 체계적 위험 측정 방법을 구축했다.\nCalmès and Théoret(2014)은 거시경제적 위험이 체계적 금융 위험에 미치는 영향을 연구하고 경제 침체기 및 상승기의 은행 위험 간의 차이를 확인했다.\n\n또한 다른 계량경제학 및 통계 방법으로 미래예측(Kritzman & Li, 2010; Segoviano & Goodhart, 2009), 불확정 클레임 분석(Merton, 1974; Jobst & Gray, 2013) 및 스트레스 테스트(Allen, Goldstein, Jagtiani & Lang, 2016), 신용위험분포(Bernardi & Romagnoli, 2016) 등이 있다.\n기존 문헌에서 사용되는 계량경제적 방법은 주로 단변량 및 다변량 회귀를 기반으로 한다.\n\nLi, Wang, He(2013)와 같이 SVM(Support Vector Machine)을 사용하여 시스템 금융 위험 예측을 연구하는 다른 방법도 있다. L. Zhang, Hu and D. Zhang(2015)은 SVM을 사용하여 공급망 금융에서 중소기업을 위한 신용 위험 평가 모델을 연구했다.\nCalabrese, Elkink, and Giudici(2017)는 유럽 국가 부채 위기 동안 전염 효과를 측정하기 위해 이진 공간 자동 회귀 모델(binary spatial autoregressive model)을 사용했다.\n\n시스템 리스크를 측정하기 위해 사용된 계량경제적 방법은 오랫동안 사용되어 왔지만 현대 금융 시스템은 은행에서 더욱 복잡한 상관 관계를 가진다. 따라서 복잡한 은행 네트워크로 인해 계량경제적 방법을 연구하기는 더욱 어려워졌다."
  },
  {
    "objectID": "posts/systemic_risk/3_Systemic_Risk_And_Machine_Learning_1_2.html#금융시장-위험과-안정성-분석",
    "href": "posts/systemic_risk/3_Systemic_Risk_And_Machine_Learning_1_2.html#금융시장-위험과-안정성-분석",
    "title": "시스템 리스크 분석: (3-2) 시스템 리스크와 머신러닝 - 계량경제학, 미시구조, 금융규제",
    "section": "금융시장 위험과 안정성 분석",
    "text": "금융시장 위험과 안정성 분석\n미시 구조에 대한 연구는 장외(OTC) 파생상품 시장(Arora & Rathinam, 2011), 투자 펀드 위험(Bengtsson, 2014; Jin & Nadal De Simone, 2014), 유럽 증권 시장 (Wymeersch, 2010), 내생적 자산 시장(Bluhm & Krahnen, 2014), 시장 지향 은행(Calmès and Théoret, 2013), 교차 시장 금융 위험(Xiong et al., 2011) 및 체계적으로 중요한 기관(Walter, 2012) 등을 포함하는 금융 시스템 리스크의 중요한 부분이다.\n신용 파생 상품 시장, 헤지 펀드, 자본 네트워크, 섀도우 뱅킹 등과 같은 다양한 금융 도구와 시스템 위험 간의 관계를 탐구하려고 시도한 연구도 다수 있다.\n\nAbedifar, Giudici and Hashem(2017)은 이슬람 창구가 있는 기존 은행이 이중 은행 시스템에서 시스템 리스크 공격에 대해 가장 탄력성이 떨어지는 부문을 가지고 있음을 발견했다.\nCalistru(2012)는 신용파생상품의 위험관리 방안을 연구하여 신용파생상품 시장의 운영 효율성이 시스템 리스크의 발생을 예방할 수 있다고 제안하였다.\nKing and Maier(2009)는 헤지펀드 연계가 시스템적 금융 위험에 미치는 영향을 연구했다.\nLiang(2016)은 그림자 금융이 중국 금융 시스템에 미치는 영향을 연구했다. 그들은 그림자 금융이 금융 네트워크를 통해 금융 기관에 위험 전염을 조장했다고 주장했다. 반면 실물경제에 기여하는 그림자금융의 역할은 약화되고 있다고 보았다.\nGaffeo and Molinari(2016)의 연구에 따르면 중요한 경고 요인은 은행 간 시장의 규모와 은행 자본화 수준이다. 거대하고 밀접하게 연결된 중앙 시장은 고도로 발달된 은행 간 시장의 자본 네트워크에 유리하지만, 자본화된 기본 시장의 경우 시스템의 유연성을 크게 약화시킬 수 있다.\n\n또한 최근의 대표적인 연구에는 다음이 있다.\n\nBongini et al.(2017)은 보험 산업의 시스템 리스크를 다루었다. 이 연구는 투자자들이 새로운 규제 프레임워크가 보험 산업의 시스템 리스크를 줄일 수 있는지와 “대마불사(too big to fail)” 정책이 도덕적 해이의 확산을 통제할 수 있는지에 대해 회의적이라고 제안했다.\nKuzubas, Saltoglu and Sever(2016)는 서로 다른 수준의 레버리지가 채무 불이행의 시스템 효과와 은행 간 시장 전염의 특성을 어떻게 크게 바꿀 수 있는지 보여주었다.\nLi, Liu, Siganos, and Zhou(2016)의 연구에 따르면 적절한 규제는 은행 주식 발행 증가로 인한 주식 가격 거품의 가능성을 줄였다.\nJ. P. Li, Feng, Sun and M. L. Li(2012)는 은행 산업의 위험 통합을 검토하고 추가 연구를 위해 몇 가지 주제를 제안했다.\nHamdi et al. (2017)은 비이자 소득 분석을 통해 튀니지 은행의 새로운 비즈니스 모델 하에서 은행 성과 및 위험을 연구했다."
  },
  {
    "objectID": "posts/systemic_risk/3_Systemic_Risk_And_Machine_Learning_1_2.html#금융규제의-정량분석",
    "href": "posts/systemic_risk/3_Systemic_Risk_And_Machine_Learning_1_2.html#금융규제의-정량분석",
    "title": "시스템 리스크 분석: (3-2) 시스템 리스크와 머신러닝 - 계량경제학, 미시구조, 금융규제",
    "section": "금융규제의 정량분석",
    "text": "금융규제의 정량분석\n지난 몇 년 동안 시스템 리스크를 다루는 금융 규제에 대한 많은 연구가 상당한 진전을 이루었다(Galat & Moessner, 2013; Adrian, Covitz, & Liang, 2015; Alexander, 2011; Borio, 2003; Kara, 2016).\n금융위기에 대한 대응으로 금융연구자, 중앙은행, 국제금융기관의 공통된 의견은 금융감독 미흡이 시스템적 리스크 확산으로 이어졌다는 것이다. 광범위한 금융 규제 연구는 글로벌 금융 위기의 교훈에서 비롯되었다. 시스템 리스크를 측정하기 위한 기본 기술로 요약되는 네트워크, 빅데이터 분석, 계량경제학 등 세 가지 주요 방법론을 제안하는 것 외에도 규제 설계는 이론 연구 및 금융 규제 실무에서 제안된 분석 결과를 바탕으로 또 다른 중요한 분야이다. 금융 규제 비용을 평가하기 위해서는 몇 가지 편익-비용 분석 경로를 고려해야 한다(Posner & Weyl, 2013).\n금융 규제 연구에서 두드러진 주제인 시스템 리스크 규제는 네트워크 분석과도 관련이 있다.\n\n정량적 분석 결과로부터 지식을 발굴하고 경영의사를 결정하는 것은 정책간 상호의존성(Bosma, 2016), 최적개입정책규칙의 관계(Clark & Jokung, 2015) 등 미래 금융규제의 주요 방향 중 하나가 된다.\nCao and Illing(2010)은 서로 다른 규제 메커니즘을 비교하고 시스템 유동성 위험에 대처하는 방법을 도출했다.\nNucera et al. (2016)은 시스템 리스크 분류를 연구한 결과 가격 기반 등급과 자본 기반 등급의 결과가 서로 크게 벗어남을 발견했다.\n\n계량적 및 통계적 관점은 이 분야에서 가장 전통적인 연구 방향이다.\n\nAshraf et al. (2016)은 회귀분석을 통해 이슬람 국가의 은행시스템에 대한 표준 금융안정성 측정시스템을 구축하여 순안정자금조달비율(NSFR)이 이슬람 국가 은행의 재무안정성에 미치는 효과를 검증하였다.\n다른 인사이트로는 금융 위험과 경제 발전 사이의 관계가 있다(Calmès & Theoret, 2014; Lupu, 2015; Fazio et al., 2015; Fernández et al., 2016)."
  }
]