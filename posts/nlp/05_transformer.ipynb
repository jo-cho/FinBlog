{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b45240c0-525b-43b7-a1b3-1e8a8bcb9092",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"자연어처리: 트랜스포머\"\n",
    "subtitle: \"Transformer\"\n",
    "author: \"Cheonghyo Cho\"\n",
    "categories: [natural language processing, transformer, attention]\n",
    "image: \"img/nlp_05_7.png\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f8ace7-fee9-4f23-bde9-b063f1cb342f",
   "metadata": {},
   "source": [
    "트랜스포머(Transformer)는 2017년 구글이 발표한 논문인 \"Attention is all you need\"에서 나온 모델로 기존의 seq2seq의 구조인 인코더-디코더를 따르면서도, 어텐션(Attention)만으로 구현한 모델이다.\r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a073fc5a-b691-46cb-98f0-6bc74816db64",
   "metadata": {},
   "source": [
    "트랜스포머 모델은 자연어 처리와 인공지능 분야에서 혁신을 일으킨 최신 딥러닝 아키텍처로, 번역, 요약, 질의응답 등 다양한 언어 관련 작업에서 탁월한 성능을 보여주며 기존 모델들의 한계를 극복하는 데 큰 기여를 하고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffebbb5-3373-47e9-a888-fee99d11dc32",
   "metadata": {},
   "source": [
    "# 트랜스포머(Transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48017430-52ca-4b8c-9013-30cf8261a9d8",
   "metadata": {},
   "source": [
    "**주요 하이퍼파라미터 (옆의 숫자는 논문에 제시한 값)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da05aca-1b7c-45fc-a93b-d35bb143cd6e",
   "metadata": {},
   "source": [
    "$d_{model} = 512$: 트랜스포머의 인코더와 디코더에서의 정해진 입력과 출력의 크기 & 임베딩 벡터의 차원\n",
    "\n",
    "$\\text{num\\_layers} = 6$: 트랜스포머에서 하나의 인코더와 디코더를 층으로 생각하였을 때, 인코더와 디코더가 총 몇 층으로 구성되었는지를 의미\n",
    "\n",
    "$\\text{num\\_heads} = 8$: 트랜스포머는 여러 개로 분할해 병렬로 어텐션을 수행하고 결과값을 다시 하나로 합치는 방식. 이때 이 병렬의 개수\n",
    "\n",
    "$d_{ff} = 2048$: 트랜스포머 내부에는 피드 포워드 신경망이 존재. 해당 신경망의 은닉층의 크기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f58eb52-94b6-494a-b166-1c314630b883",
   "metadata": {},
   "source": [
    "`small_transformer = transformer(vocab_size=9000, num_layers=4, dff=512, d_model=128, num_heads=4, dropout=0.3, name=\"small_transformer\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a798e262-94b1-4f05-b184-65f8ae15bfdf",
   "metadata": {},
   "source": [
    "<img src='img/nlp_05_1.png' width='200'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c60127a-a063-4352-b0b4-b6d0edf4eee0",
   "metadata": {},
   "source": [
    "seq2seq 구조에서는 인코더와 디코더에서 각각 하나의 RNN이 t개의 시점(time step)을 가지는 구조였다면,\n",
    "\r\n",
    "트랜스포머는 인코더와 디코더 단위가 N개(ex.6)로 구성되는 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228482e8-184f-4d68-b65a-b27ee252d2ea",
   "metadata": {},
   "source": [
    "<img src='img/nlp_05_2.png' width='400'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03ea406-0e52-441d-be3a-b4ad1eeec9b5",
   "metadata": {},
   "source": [
    "RNN은 사용되지 않지만 여전히 인코더-디코더의 구조는 유지되고 있음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ddc23102-72c7-4097-aa12-20014378a3fb",
   "metadata": {},
   "source": [
    "# 포지셔널 인코딩(Positional Encoding)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13d12d3b-9a03-41d5-990f-efabecc3f2fd",
   "metadata": {},
   "source": [
    "트랜스포머는 RNN과 다르게 단어 입력을 순차적으로 받는 방식이 아님사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b017b18-a238-41b3-b863-7a2edbc825d6",
   "metadata": {},
   "source": [
    "- 포지셔널 인코딩: 단어의 위치 정보를 얻기 위해서 각 단어의 임베딩 벡터에 위치 정보들을 더하여 모델의 입력으로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecb3631-3491-46b6-b0ca-04661e1f6553",
   "metadata": {},
   "source": [
    "<img src='img/nlp_05_3.png' width='400'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b54ee73-34f0-4e05-a88e-613be8679dbc",
   "metadata": {},
   "source": [
    "<img src='img/nlp_05_4.png' width='400'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9592ab76-ebee-4ddb-a442-eea836570147",
   "metadata": {},
   "source": [
    "<img src='img/nlp_05_5.png' width='250'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34da1a70-f6f0-4625-b853-ba25ab2e2d14",
   "metadata": {},
   "source": [
    "# 어텐션"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bde8017-1723-407c-afa9-a7e19a7cc2f1",
   "metadata": {},
   "source": [
    "트랜스포머에 쓰이는 어텐션(attention)을 미리 살펴보면 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27af0575-d1c1-49d7-8f16-42b727d07d4f",
   "metadata": {},
   "source": [
    "<img src='img/nlp_05_6.png' width='200'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665d12ed-2bad-44a9-8c1b-33a0ccd90947",
   "metadata": {},
   "source": [
    "<img src='img/nlp_05_8.png' width='400'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb18d5ca-4204-44dd-a89e-1f1841cc9b8c",
   "metadata": {},
   "source": [
    "- 여기서 Query, Key 등이 같다는 것은 벡터의 값이 같다는 것이 아니라 벡터의 출처가 같다는 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1a3919-6cf8-4635-ba88-738990895479",
   "metadata": {},
   "source": [
    "<img src='img/nlp_05_7.png' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334ca639-6f79-4a1f-938d-2bacbb74e55a",
   "metadata": {},
   "source": [
    "- 트랜스포머의 아키텍처에서 세 가지 어텐션이 각각 어디에서 이루어지는지를 보여줌\n",
    "- ‘Multi-head’: 트랜스포머가 어텐션을 병렬적으로 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502c6993-3db9-43fd-ae9c-ec8a83d2eca4",
   "metadata": {},
   "source": [
    "# 인코더(Encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a64333-18b1-4ba4-adc1-b29406ca8005",
   "metadata": {},
   "source": [
    "<img src='img/nlp_05_9.png' width='250'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eca434-c66e-4d87-ba98-013e886499c9",
   "metadata": {},
   "source": [
    "- 트랜스포머는 하이퍼파라미터인 num_layers 개수의 인코더 층을 쌓음\n",
    "- 인코더를 하나의 층이라는 개념으로 생각한다면, 하나의 인코더 층은 크게 총 2개의 서브층(sublayer)으로 나뉘어짐\n",
    "  - (멀티 헤드) 셀프 어텐션 \n",
    "  - (포지션-와이즈) 피드 포워드 신경망(FFFN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10a8433f-5950-4f09-8ccd-4455dc539ec9",
   "metadata": {},
   "source": [
    "## 셀프-어텐션 (Self-attention)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd11560-07d3-40b5-952e-5dd1fc791afd",
   "metadata": {},
   "source": [
    "셀프 어텐션(self-attention)은 어텐션을 자기 자신에게 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcee1bf-9c04-475c-98af-e047e4b0dca5",
   "metadata": {},
   "source": [
    "**Q, K, V 벡터**\n",
    "\n",
    "- 우선 각 단어 벡터들로부터 Q벡터, K벡터, V벡터를 얻음.\n",
    "- Q, K, V는 $𝑑_{𝑚𝑜𝑑𝑒𝑙}$(각 단어 벡터 차원)을 𝑛𝑢𝑚_ℎ𝑒𝑎𝑑𝑠로 나눈 값을 차원으로 가짐.\n",
    "- 각 단어 벡터에 가중치 행렬을 곱하여 완성됨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ffea40-c2e1-4a73-8948-c8d3bed1dae8",
   "metadata": {},
   "source": [
    "**스케일드 닷-프로덕트 어텐션(Scaled dot-product Attention)**\n",
    "\n",
    "- 각 Q벡터는 모든 K벡터에 대해서 어텐션 스코어를 구하고, 어텐션 분포를 구함\n",
    "- 이를 사용하여 모든 V벡터를 가중합하여 어텐션 값 또는 컨텍스트 벡터를 구함\n",
    "- 그리고 이를 모든 Q벡터에 대해서 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5cf7fa-bffd-42d9-832b-139211330656",
   "metadata": {},
   "source": [
    "<img src='img/nlp_05_11.png' width='800'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d350680b-4f87-49a1-8eec-2da4f20f7e34",
   "metadata": {},
   "source": [
    "- 하지만 굳이 이렇게 각 Q벡터마다 일일히 따로 연산할 필요가 있을까? => 행렬 연산으로 일괄 처리\n",
    "\n",
    "**행렬 연산**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40777fda-20c3-4b59-a90d-06a6ee332720",
   "metadata": {},
   "source": [
    "<img src='img/nlp_05_12.png' width='300'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ef7c18-0af4-4947-863a-f3d3105fdebc",
   "metadata": {},
   "source": [
    "<img src='img/nlp_05_13.png' width='400'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45154a2d-04ec-47f7-9bb9-602145795521",
   "metadata": {},
   "source": [
    "- Q 행렬을 K 행렬을 전치한 행렬과 곱해주면, 각각 단어의 Q 벡터와 K 벡터의 내적이 각 행렬의 원소가 되는 행렬이 결과로 나옴"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1e4750-084e-4bf9-b105-ff829605806f",
   "metadata": {},
   "source": [
    "<img src='img/nlp_05_14.png' width='400'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd65175-cfad-41ed-a707-b6c3b9050c1a",
   "metadata": {},
   "source": [
    "- 결과 행렬 값에 $\\sqrt{𝑑_𝑘}$를 나눠주면 각 행 열은 어텐션 스코어 값을 가짐. 여기에 어텐션 값을 구할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0d5050-0a1d-4099-9cb2-c1a43e508fbe",
   "metadata": {},
   "source": [
    "- 이 때 패딩을 실행함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21137583-368b-4371-a847-44f48bbc29e0",
   "metadata": {},
   "source": [
    "<img src='img/nlp_05_15.png' width='400'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f540e25-eee8-4ff9-a198-e368b7b36c6d",
   "metadata": {},
   "source": [
    "- `<PAD>`는 실질적인 의미를 가진 단어가 아님\n",
    "\n",
    "- 이에 대해 유사도를 구하지 않도록 마스킹(Masking)함\n",
    "  - 매우 작은 음수값을 채움\n",
    "  - 소프트맥스 함수를 지나면 0이 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81550e76-ffb2-4528-b635-be1f3b4b8b3a",
   "metadata": {},
   "source": [
    "## 멀티 헤드 어텐션(Multi-head attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97944a41-f4be-473d-9cca-18bf85fb42e6",
   "metadata": {},
   "source": [
    "- 앞서 배운 어텐션에서는 $𝑑_{𝑚𝑜𝑑𝑒𝑙}$의 차원을 가진 단어 벡터를 $Q, K, V$로 바꾸어(𝑛𝑢𝑚_ℎ𝑒𝑎𝑑𝑠로 나누어 축소) 어텐션을 수행함. \n",
    "- 그 이유는 여러번의 어텐션을 병렬로 사용하기 위함. ==> 멀티 헤드(multi-head)\n",
    "- $𝑑_{𝑚𝑜𝑑𝑒𝑙}$의 차원을 $𝑛𝑢𝑚\\_ℎ𝑒𝑎𝑑𝑠$ 개로 나누어 $𝑑_{𝑚𝑜𝑑𝑒𝑙}  / 𝑛𝑢𝑚\\_ℎ𝑒𝑎𝑑𝑠$ 의 차원을 가지는 $Q, K, V$에 대해서 𝒏𝒖𝒎_𝒉𝒆𝒂𝒅𝒔 개의 병렬 어텐션을 수행\n",
    "- 각각의 어텐션 값 행렬을 어텐션 헤드라고 부르며, 가중치 행렬 $𝑊^𝑄$, $𝑊^𝐾$, $𝑊^𝑉$의 값은 어텐션 헤드마다 전부 다름"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b94b57-6935-424a-aaab-2a0bff096066",
   "metadata": {},
   "source": [
    "<img src='img/nlp_05_16.png' width='600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21960a72-4b21-460d-ad0f-90afcc4cd347",
   "metadata": {},
   "source": [
    "- 어텐션을 병렬로 수행하여 다른 시각으로 정보들을 수집"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1923ba-2d04-4a7f-abfb-35b0f07b7128",
   "metadata": {},
   "source": [
    "<img src='img/nlp_05_17.png' width='300'>\n",
    "\n",
    "- 병렬 어텐션 수행 후 모든 어텐션 헤드를 연결(concatenate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8b5e79-4f64-4d1b-bb07-8a3c4dff0870",
   "metadata": {},
   "source": [
    "<img src='img/nlp_05_18.png' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64324506-2fe6-4615-b052-5e1874390926",
   "metadata": {},
   "source": [
    "- 어텐션 헤드를 모두 연결한 행렬에 가중치 행렬 $𝑊^𝑜$을 곱함 ==>  멀티-헤드 어텐션 행렬\n",
    "\n",
    "- 인코더에서의 입력의 크기는 출력에서도 동일 크기로 계속 유지됨"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2faa10d1-2c17-4b8a-a652-c312345503a9",
   "metadata": {},
   "source": [
    "# 포지션-와이즈 피드 포워드 신경망(Position-wise FFNN)\r\n",
    "\r\n",
    "포지션 와이즈 FFNN은 인코더와 디코더에서 공통적으로 가지고 있는 서브층으로, 완전 연결 FFNN임."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75ad649-82fc-4fd9-bd7b-27dee68ee1bf",
   "metadata": {},
   "source": [
    "$FFNN(x) = MAX(0, xW_1 + b_1)W_2 +b_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8118c339-9646-4f9c-a3ee-fbcd797c598a",
   "metadata": {},
   "source": [
    "<img src='img/nlp_05_19.png' width='250'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8182fe88-ca06-4b86-8d01-1d31f7ac2910",
   "metadata": {},
   "source": [
    "- $x$는 멀티-헤드 어텐션 결과로 나온 행렬\n",
    "- 가중치 행렬 $𝑊_1$ 은 $(𝑑_{𝑚𝑜𝑑𝑒𝑙}, 𝑑_{𝑓𝑓})$ 크기, $𝑊_2$은 $(𝑑_{𝑓𝑓}, 𝑑_{𝑚𝑜𝑑𝑒𝑙})$ 크기를 가짐\n",
    "- $𝑑_{𝑓𝑓}$는 은닉층의 크기(ex. 2048)\n",
    "- 매개변수 $𝑊_1,𝑏_1,𝑊_2, 𝑏_2$는 하나의 인코더 층 내에서는 다른 문장, 다른 단어들마다 정확하게 동일하게 사용됨. 하지만 인코더 층마다는 다른 값을 가짐."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ce757c-6d7c-493b-87d7-4351c3e4e52e",
   "metadata": {},
   "source": [
    "<img src='img/nlp_05_20.png' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7a8ada-d3f0-49ed-b6dc-3f6543f98422",
   "metadata": {},
   "source": [
    "- 두번째 서브층을 지난 인코더의 최종 출력은 여전히 인코더의 입력의 크기였던 $(seq\\_len ,𝑑\\_{𝑚𝑜𝑑𝑒𝑙})$의 크기가 보존\n",
    "\n",
    "- 하나의 인코더 층을 지난 이 행렬은 다음 인코더 층으로 전달되고, 다음 층에서도 동일한 인코더 연산이 반복"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0be2e9a-1e27-40b3-8d56-1f0a90704d1d",
   "metadata": {},
   "source": [
    "# 잔차 연결(Residual connection)과 층 정규화(Layer Normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8f7179-dda3-40cc-8b5e-d2cc174b5fb6",
   "metadata": {},
   "source": [
    "<img src='img/nlp_05_21.png' width='400'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6bb0ba-dc2f-43db-b311-b8ab26c9337d",
   "metadata": {},
   "source": [
    "여기서 Add & Norm은 잔차 연결(residual connection)과 층 정규화(layer normalization)를 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1066439-d2ac-48ab-ab7f-8492ce9f797a",
   "metadata": {},
   "source": [
    "### 잔차 연결\n",
    "\n",
    "$x+Sublayer(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1db3b8-ec4d-4dc2-90f8-c67a10159a2b",
   "metadata": {},
   "source": [
    "<img src='img/nlp_05_22.png' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc9b638-13b6-4ecf-9f03-a395a1a38acc",
   "metadata": {},
   "source": [
    "- 잔차 연결은 서브층의 입력과 출력을 더하는 것을 의미\n",
    "- 컴퓨터 비전 분야에서 주로 사용되는 모델의 학습을 돕는 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252a190c-f52d-4e6a-9376-b8c8b5575710",
   "metadata": {},
   "source": [
    "### 층 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728d1ec4-cfdd-4106-b416-9b6ffd534672",
   "metadata": {},
   "source": [
    "$\\hat{x}_{i,k} = \\frac{x_{i,k}-\\mu_i}{\\sqrt{\\sigma^2_i+\\epsilon}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e9f87d-cac1-4c04-a841-71a63643ff89",
   "metadata": {},
   "source": [
    "<img src='img/nlp_05_23.png' width='200'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709dbd99-425e-489b-8700-58ecb90feeec",
   "metadata": {},
   "source": [
    "층 정규화는 텐서의 마지막 차원에 대해서 평균과 분산을 구하고, 이를 가지고 어떤 수식을 통해 값을 정규화하여 학습을 돕는 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3285ea5b-fa77-46f7-b914-93d5d5628441",
   "metadata": {},
   "source": [
    "# 디코더(Decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32b8598-90ba-4ea8-8296-97a088f94e88",
   "metadata": {},
   "source": [
    "## 인코더에서 디코더로"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972b2b25-9394-4249-bc87-e5ea3dc17027",
   "metadata": {},
   "source": [
    "<img src='img/nlp_05_24.png' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b4aa26-84f5-4f2f-9f8b-10394e3474cc",
   "metadata": {},
   "source": [
    "- 인코더 연산이 끝났으면 디코더 연산이 시작되어 디코더 또한 num_layers 만큼의 연산을 하는데, 이때마다 인코더가 보낸 출력을 각 디코더 층 연산에 사용함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb14c8a-2b6f-42e8-8a54-2077289725a5",
   "metadata": {},
   "source": [
    "## 디코더"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9321eb92-cf21-4935-b410-46739ba56e0d",
   "metadata": {},
   "source": [
    "<img src='img/nlp_05_25.png' width='250'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ad66b1-31ae-4049-bf47-8dc7383b02cd",
   "metadata": {},
   "source": [
    "- 디코더도 인코더와 동일하게 임베딩 층과 포지셔널 인코딩을 거친 후의 문장 행렬이 입력됨. \n",
    "- 교사 강요(Teacher Forcing)을 사용하여 훈련되므로 학습 과정에서 디코더는 번역할 문장에 해당되는 <sos> je suis étudiant의 문장 행렬을 한 번에 입력받음. \n",
    "- 그리고 디코더는 이 문장 행렬로부터 각 시점의 단어를 예측하도록 훈련됨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279f2850-abfd-4050-8496-7d30915a9601",
   "metadata": {},
   "source": [
    "- 단, 트랜스포머는 문장 행렬로 입력을 한 번에 받으므로 현재 시점의 단어를 예측하고자 할 때, 입력 문장 행렬로부터 미래 시점의 단어까지도 참고할 수 있는 현상이 발생 => 룩-어헤드 마스크(look-ahead mask)를 도입 => 첫번째 층인 멀티헤드 셀프어텐션 서브층에 적용 (masked multi-head self-attention)\n",
    "\n",
    "<img src='img/nlp_05_26.png' width='200'> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facbb792-cd3a-401e-b1c0-4b11e5aa6cfe",
   "metadata": {},
   "source": [
    "- 두번째 서브층인 멀티헤드 어텐션은 다른 경우와 달리 셀프 어텐션이 아님.\n",
    "\n",
    "- 이유는 셀프 어텐션은 Query, Key, Value가 같은 경우를 말하는데, 인코더-디코더 어텐션은 Query가 디코더인 행렬인 반면, Key와 Value는 인코더 행렬이기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22d18eb-040d-4536-ab66-37bab1d66658",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 참고자료\n",
    "\n",
    "- 딥 러닝을 이용한 자연어 처리 입문(https://wikidocs.net/book/2155)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
