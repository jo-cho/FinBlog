{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d85d3ce1-edca-4b75-a113-75ce79885195",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"자연어처리: 워드임베딩 - Word2vec\"\n",
    "subtitle: \"Word Embedding - Word2vec: CBOW, Skip-gram and SGNS\"\n",
    "author: \"Cheonghyo Cho\"\n",
    "categories: [nlp, word embedding, word2vec]\n",
    "image: \"img/nlp_02_1.png\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104b3133-bb58-4501-94c1-4483537c1d08",
   "metadata": {},
   "source": [
    "워드 임베딩(Word Embedding)은 자연어 처리(NLP)에서 단어를 고정된 크기의 실수 벡터로 변환하는 기술이다. 이러한 벡터는 단어의 의미를 수치적으로 표현하며, 비슷한 의미를 가진 단어들은 유사한 벡터로 표현된다. 이 중 가장 대표적인 방법인 Word2Vec에 대해 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a8e99d-e264-4355-a000-2aa4031a90ca",
   "metadata": {},
   "source": [
    "### 희소 표현(sparse representation) vs 밀집 표현(dense representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacc5710-916d-4d36-aa6c-9abaa2f4698c",
   "metadata": {},
   "source": [
    "- 희소 표현\n",
    "    - 원-핫 벡터 (0과 1) 등\n",
    "    - Ex) 강아지 = [ 0 0 0 0 1 0 0 0 0 0 0 0 ... 중략 ... 0] (이때 1 뒤의 0의 수는 9995개)\n",
    "    - 공간적 낭비, 단어의 의미를 표현하지 못함\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf5b941-99c3-4fda-9545-f1f6383fce59",
   "metadata": {},
   "source": [
    "- 밀집 표현\n",
    "    - LSA, Word2Vec, FastText, Glove 등\n",
    "    - 사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞춤\n",
    "    - Ex) 강아지 = [0.2 1.8 1.1 -2.1 1.1 2.8 ... 중략 ...] (이 벡터의 차원은 128)\n",
    "    - 단어를 밀집 벡터(dense vector)의 형태로 표현하는 방법을 워드 임베딩(word embedding)이라고 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61bbc44a-36fb-423c-bb21-4eedccdedb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW:\n",
      "[[0 0 0 1 1]\n",
      " [1 0 0 1 0]\n",
      " [0 1 1 0 1]]\n",
      "Vocabulary:\n",
      "{'love': 3, 'programming': 4, 'coding': 0, 'is': 2, 'fun': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 예제 문서\n",
    "documents = [\n",
    "    \"I love programming.\",\n",
    "    \"I love coding.\",\n",
    "    \"Programming is fun.\"\n",
    "]\n",
    "\n",
    "# CountVectorizer를 사용하여 BoW 생성\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# BoW 결과 출력\n",
    "print(\"BoW:\")\n",
    "print(X.toarray())\n",
    "print(\"Vocabulary:\")\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30777897-577d-4ad3-bf13-be06760476c9",
   "metadata": {},
   "source": [
    "| 원-핫 벡터        | 임베딩 벡터       |\n",
    "|-------------------|--------------------|\n",
    "| 차원              | 고차원(단어 집합의 크기) | 저차원             |\n",
    "| 다른 표현         | 희소 벡터의 일종   | 밀집 벡터의 일종    |\n",
    "| 표현 방법         | 수동              | 훈련 데이터로부터 학습함 |\n",
    "| 값의 타입         | 1과 0             | 실수               |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8552248f-2426-4a5e-9131-a07d8fad6dfc",
   "metadata": {},
   "source": [
    "## 워드투벡터(Word2Vec)\n",
    "\n",
    "분산 표현(Distributed Represetation)\n",
    "- 희소 표현은 단어 벡터간 유사성 표현 불가 → 의미를 다차원 공간에 벡터화\n",
    "- ‘분포 가설’ 가정 ('비슷한 문맥에서 등장하는 단어들은 비슷한 의미를 가진다’)\n",
    "- 저차원에 단어의 의미를 여러 차원에다가 분산하여 표현\n",
    "- 단어 벡터 간 유의미한 유사도 계산 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80823452-276b-4495-8b0a-5bd9149f3b55",
   "metadata": {},
   "source": [
    "분산 표현의 대표적인 학습 방법인 Word2Vec(2013) 부터 알아보자.\n",
    "\n",
    "Word2Vec의 학습 방식에는 CBOW(Continuous Bag of Words)와 Skip-Gram 두 가지 방식이 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4555cc1b-d3d9-48e6-b51d-3d02789e8289",
   "metadata": {},
   "source": [
    "<img src='img/nlp_02_1.png' width='600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c85b4e7-8798-4376-8b3e-906b531b40ff",
   "metadata": {},
   "source": [
    "### CBOW(Continuous Bag of Words) \n",
    "\n",
    "주변에 있는 단어들을 입력으로 중간에 있는 단어들을 예측하는 방법\n",
    "\n",
    "아래 예는 윈도우(window) 크기가 2 인 경우 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77c8ab4-11d2-4134-b7c6-df1f419a8731",
   "metadata": {},
   "source": [
    "<img src='img/nlp_02_2.png' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fe06ad-3045-4256-93ea-611f09a34a76",
   "metadata": {},
   "source": [
    "- 슬라이딩 윈도우(sliding window): 윈도우 크기가 정해지면 윈도우를 옆으로 움직여서 주변 단어와 중심 단어의 선택을 변경해가며 학습을 위한 데이터 셋을 만듦"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3681cc-074c-43f2-9db4-1cd1fe14532d",
   "metadata": {},
   "source": [
    "<img src='img/nlp_02_3.png' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507b8643-c9cc-43f6-9ade-5d4bf0a669ec",
   "metadata": {},
   "source": [
    "- 입력층(Input layer)의 입력으로서 앞, 뒤로 사용자가 정한 윈도우 크기 범위 안에 있는 주변 단어들의 원-핫 벡터가 들어감 \n",
    "\n",
    "- 출력층(Output layer)에서 예측하고자 하는 중간 단어의 원-핫 벡터가 레이블로서 필요\n",
    "\n",
    "- Word2Vec은 은닉층이 다수인 딥러닝 모델이 아니라 은닉층이 1개인 ‘shallow neural network’\n",
    "\n",
    "- Word2Vec의 은닉층은 활성화 함수가 존재하지 않으며, 룩업 테이블이라는 연산은 담당, 투사층(projection layer)이라고 부름"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947fce10-a749-4e84-adfd-ba1164d4bc1b",
   "metadata": {},
   "source": [
    "<img src='img/nlp_02_4.png' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff1275a-b584-4d28-a109-857cee3671f1",
   "metadata": {},
   "source": [
    "CBOW에서 투사층의 크기($M$)는 임베딩하고 난 벡터의 차원이 됨. \n",
    "- 그림에서 $M=5$, CBOW를 수행하고나서 얻는 각 단어의 임베딩 벡터의 차원은 5가 됨.\n",
    "\n",
    "입력층과 투사층 사이의 가중치($W$)는 $V × M$ 행렬이며, 투사층에서 출력층 사이의 가중치($W'$)는 $M \\times V$ 행렬임(여기서 $V$는 단어 집합 크기)\n",
    "- 그림에서 $V=7$, $W$는 $7 × 5$ 행렬, $W'$는 $5 × 7$ 행렬\n",
    "\n",
    "인공 신경망 훈련 전에 가중치 행렬 $W$와 $W'$는 랜덤 값\n",
    "- CBOW는 주변 단어로 중심 단어를 더 정확히 맞추기 위해 계속해서 **이 $W$와 $W'$를 학습해가는 구조**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511589a9-390f-4cf0-8b83-2899132f0f0e",
   "metadata": {},
   "source": [
    "<img src='img/nlp_02_5.png' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dfdcd2-fd53-4d76-ad57-31855a9fb2fc",
   "metadata": {},
   "source": [
    "입력 벡터 $x$와 가중치 $W$ 행렬의 곱은 사실 $W$행렬의 $i$번째 행을 그대로 읽어오는 것과(lookup) 동일\n",
    "\n",
    "**lookup**해온 W의 각 행벡터가 Word2Vec 학습 후에는 각 단어의 M차원의 **임베딩 벡터**로 간주됨 = $V_i$  (단어집합 크기 $V$와 다름!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeb78e2-9cd2-42e1-a8b6-d337439f56e3",
   "metadata": {},
   "source": [
    "<img src='img/nlp_02_6.png' width='500'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59966154-ed4c-4264-b1da-ef7a18252f88",
   "metadata": {},
   "source": [
    "주변 단어의 원-핫 벡터($x$)에 대해 가중치 $W$가 곱해서 생겨진 ‘결과 벡터들’($V_i$)은 투사층에서 만나 평균인 벡터를 구하게 됨\r\n",
    "\r\n",
    "투사층에서 벡터의 평균을 구하는 부분은 CBOW가 Skip-Gram과 다른 차이점이기도함\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207ce868-dad2-4a5c-b9c2-a4cea3804b3e",
   "metadata": {},
   "source": [
    "<img src='img/nlp_02_7.png' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fe66b1-ce16-430b-8e7e-bda78903a374",
   "metadata": {},
   "source": [
    "구해진 평균 벡터($v$)는 두번째 가중치 행렬 $W'$와 곱해지며, 곱셈의 결과($z$)로는 원-핫 벡터들과 차원이 $V$로 동일한 벡터가 나옴\n",
    "\n",
    "이 벡터(z)에 CBOW는 **소프트맥스(softmax) 함수**를 지나고, 벡터의 각 원소들의 값은 **0과 1사이의 실수**를 가짐(총 합 1)\n",
    "\n",
    "- 이는 다중 클래스 분류 문제를 위한 일종의 스코어 벡터(score vector)\n",
    "- 스코어 벡터(𝒚 ̂)의 j번째 인덱스가 가진 값은 j번째 단어가 중심 단어일 확률임\n",
    "- 이 스코어 벡터의 값은 레이블(y)에 해당하는 벡터인 중심 단어 원-핫 벡터의 값에 가까워져야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b7ebd4-6146-4e99-9234-645dcd7c384b",
   "metadata": {},
   "source": [
    "<img src='img/nlp_02_8.png' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4250af4b-cb57-44c6-8016-0d0bbd4faa96",
   "metadata": {},
   "source": [
    "이 두 벡터값 $(y, \\hat{y})$의 오차를 줄이기위해 CBOW는 **손실 함수(loss function)로 크로스 엔트로피(cross-entropy) 함수 사용**\n",
    "\n",
    "$$\\text{cost}(\\hat{y},y) = - \\sum^V_{j=1}{y_j \\log (\\hat{y}_j})$$\n",
    "\n",
    "**역전파(Back Propagation)를 수행하면 $W$와 $W'$가 학습됨**\n",
    "\n",
    "학습이 다 되었다면 $M$차원의 크기를 갖는 $W$의 행렬의 행을 각 단어의 임베딩 벡터로 사용하거나 $W$와 $W'$ 행렬 두 가지 모두를 가지고 임베딩 벡터를 사용하기도 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "618aa9f6-4c88-4147-8a63-73a1f97ec548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'machine':\n",
      "[ 0.1478254  -0.03021165 -0.09053031  0.13081697 -0.09672265]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 예제 문서\n",
    "documents = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"I love natural language processing and machine learning.\",\n",
    "    \"Word embeddings are a type of word representation that allows words to be represented as vectors.\",\n",
    "    \"Gensim is a useful library for text processing in Python.\",\n",
    "    \"Machine learning models can be used for a variety of tasks, including classification and regression.\",\n",
    "    \"Deep learning is a subset of machine learning that uses neural networks.\",\n",
    "    \"Text data requires preprocessing before it can be used in machine learning models.\",\n",
    "    \"Natural language processing involves the interaction between computers and humans using natural language.\",\n",
    "    \"The field of artificial intelligence includes machine learning and deep learning.\",\n",
    "    \"Python is a popular programming language for data science and machine learning.\"\n",
    "]\n",
    "\n",
    "# 토큰화\n",
    "tokenized_docs = [word_tokenize(doc.lower()) for doc in documents]\n",
    "\n",
    "# Word2Vec 모델 초기화 및 학습 (CBOW)\n",
    "model = Word2Vec(sentences=tokenized_docs, vector_size=5, window=2, sg=0, min_count=1)\n",
    "\n",
    "# \"machine\" 단어의 벡터 출력\n",
    "print(\"Vector for 'machine':\")\n",
    "print(model.wv['machine'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4d07681-f1da-443c-a651-73de1b707480",
   "metadata": {},
   "source": [
    "### Skip-gram\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672c9c74-3e88-4a65-ad80-725712936e87",
   "metadata": {},
   "source": [
    "중심에 있는 단어를 입력으로 주변에 있는 단어들을 예측하는 방법\n",
    "\n",
    "나머지 방식은 CBOW와 동일"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ce2c20-6315-4efa-8122-4dba2376eee4",
   "metadata": {},
   "source": [
    "<img src='img/nlp_02_9.png' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32c3863-efc6-4a76-94ac-f247b7858377",
   "metadata": {},
   "source": [
    "<img src='img/nlp_02_10.png' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b103f351-7831-4391-b668-92e05e430b89",
   "metadata": {},
   "source": [
    "보통, 한 단어에 대해 여러 학습 기회를 가지는 Skip-gram의 성능이 더 좋다고 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a366b3-c8f8-432b-9bf3-621fedc50927",
   "metadata": {},
   "source": [
    "<img src='img/nlp_02_11.png' width='500'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca7b2789-c5ae-4fe8-b229-73b5413ebcbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'machine':\n",
      "[ 0.14802615 -0.03004972 -0.08976299  0.13072683 -0.0961291 ]\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec 모델 초기화 및 학습 (Skip-Gram)\n",
    "model = Word2Vec(sentences=tokenized_docs, vector_size=5, window=2, sg=1, min_count=1)\n",
    "\n",
    "# \"machine\" 단어의 벡터 출력\n",
    "print(\"Vector for 'machine':\")\n",
    "print(model.wv['machine'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b25c25-41f6-47f7-8c8b-deabc05c3fdc",
   "metadata": {},
   "source": [
    "### 네거티브 샘플링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7346695-595d-439c-8300-5d3f86c1a9c7",
   "metadata": {},
   "source": [
    "만약 단어 집합의 크기가 수만 이상에 달한다면 Word2Vec은 학습하기에 무거운 모델이 됨.\n",
    "- Word2Vec은 역전파 과정에서 **모든 단어의 임베딩 벡터값의 업데이트를 수행**\n",
    "- 현재 집중하고 있는 중심 단어와 주변 단어가 '강아지'와 '고양이', '귀여운'과 같은 단어라면, 사실 이 단어들과 별 연관 관계가 없는 '돈가스'나 '컴퓨터'와 같은 수많은 단어의 임베딩 벡터값까지 업데이트하는 것은 비효율적임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ab61ce-69a9-415b-a0ec-9148b5c7f2e6",
   "metadata": {},
   "source": [
    "네거티브 샘플링(negative sampling)은 Word2Vec이 학습 과정에서 전체 단어 집합이 아니라 일부 단어 집합에만 집중할 수 있도록 하는 방법\n",
    "- 하나의 중심 단어에 대해서 전체 단어 집합보다 훨씬 작은 단어 집합을 만들어 놓고 마지막 단계를 이진 분류 문제로 변환\n",
    "- 주변 단어들을 긍정(positive), 랜덤으로 샘플링 된 단어들을 부정(negative)으로 레이블링한다면 이진 분류 문제를 위한 데이터셋이 됨\n",
    "- 이는 기존의 단어 집합의 크기만큼의 선택지를 두고 다중 클래스 분류 문제를 풀던 Word2Vec보다 훨씬 연산량에서 효율적임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1ab8d3-b3c8-4602-b6c8-248ff6f3306c",
   "metadata": {},
   "source": [
    "**Skip-gram with negative sampling(SGNS)**\n",
    "\n",
    "SGNS는 중심 단어와 주변 단어가 모두 입력이 되고, 두 단어가 실제 윈도우 크기 내에 존재하는 이웃 관계인지 그 확률을 예측\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc21e9ab-18b7-477f-a446-858ef37e75a0",
   "metadata": {},
   "source": [
    "<img src='img/nlp_02_12.png' width='1000'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe11a1e-b134-48de-b47d-b20052ca69b2",
   "metadata": {},
   "source": [
    "<img src='img/nlp_02_13.png' width='400'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a923ec-62ce-48a8-b123-c6ce48385ff5",
   "metadata": {},
   "source": [
    "두 테이블 중 \n",
    "- 하나는 입력 1인 중심 단어의 테이블 룩업을 위한 임베딩 테이블\n",
    "- 하나는 입력 2인 주변 단어의 테이블 룩업을 위한 임베딩 테이블 \n",
    "\n",
    "각 단어는 각 임베딩 테이블을 테이블 룩업하여 임베딩 벡터로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c33f064-b958-4c90-8a15-f34e7639d1a6",
   "metadata": {},
   "source": [
    "<img src='img/nlp_02_14.png' width='200'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d066fe-bd15-4901-b0d1-226edcfb961f",
   "metadata": {},
   "source": [
    "<img src='img/nlp_02_15.png' width='400'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d180d2-16fe-4d6b-8042-de7e617f0ee5",
   "metadata": {},
   "source": [
    "중심 단어와 주변 단어의 **내적값**을 이 모델의 **예측값**으로 하고, **레이블과의 오차로부터 역전파**하여 중심 단어와 주변 단어의 **임베딩 벡터값을 업데이트**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9756bca7-6a07-4b07-9e06-7148385beed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'machine':\n",
      "[ 0.14802615 -0.03004972 -0.08976299  0.13072683 -0.0961291 ]\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec 모델 초기화 및 학습 (Skip-Gram with Negative Sampling)\n",
    "model = Word2Vec(sentences=tokenized_docs, vector_size=5, window=2, sg=1, hs=0, negative=5, min_count=1)\n",
    "\n",
    "# \"machine\" 단어의 벡터 출력\n",
    "print(\"Vector for 'machine':\")\n",
    "print(model.wv['machine'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad69ee4-0b8b-422b-88b8-33c4cb9d029a",
   "metadata": {},
   "source": [
    "## 참고자료\n",
    "\n",
    "- 딥 러닝을 이용한 자연어 처리 입문(https://wikidocs.net/book/2155)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
