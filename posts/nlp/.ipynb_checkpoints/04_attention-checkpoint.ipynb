{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b45240c0-525b-43b7-a1b3-1e8a8bcb9092",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"자연어처리: 어텐션 메커니즘\"\n",
    "subtitle: \"Seq2seq and Attention mechanism\"\n",
    "author: \"Cheonghyo Cho\"\n",
    "categories: [natural language processing, attention, seq2seq]\n",
    "image: \"img/nlp_04_10.png\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda637c0-8689-4a50-83a4-a24865fd8889",
   "metadata": {},
   "source": [
    "Attention 메커니즘은 모델이 입력 시퀀스의 관련 부분에 선택적으로 집중할 수 있게 하여 자연어 처리부터 컴퓨터 비전까지 다양한 작업에서 성능을 향상시킨다. Seq2seq과 어텐션 메커니즘에 대해 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffebbb5-3373-47e9-a888-fee99d11dc32",
   "metadata": {},
   "source": [
    "# Seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bfbd9d-b498-4340-baeb-1b89fef9409a",
   "metadata": {},
   "source": [
    "<img src='img/nlp_04_1.png' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1c7b5c-bb2c-4047-a4a9-9b0c7d3bc1f1",
   "metadata": {},
   "source": [
    "seq2seq는 입력된 시퀀스로부터 다른 도메인의 시퀀스를 출력하는 모델\n",
    "- 예) 챗봇, 기계 번역, 내용 요약, STT(Speech to Text) 등"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c87be43-dd79-46f4-9278-00388760f557",
   "metadata": {},
   "source": [
    "seq2seq는 크게 인코더와 디코더 두 개의 모듈로 구성\n",
    "- 인코더는 입력 문장의 모든 단어들을 순차적으로 입력 받은 뒤\n",
    "- 모든 단어 정보들을 압축해 하나의 벡터로 만듦 => 컨텍스트 벡터(context vector)\n",
    "- 컨텍스트 벡터를 디코더로 전송\n",
    "- 디코더는 컨텍스트 벡터를 받아서 번역된 단어를 한 개씩 순차적으로 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96c9deb-4e10-4559-bc8c-e0c56a2ac032",
   "metadata": {},
   "source": [
    "<img src='img/nlp_04_2.png' width='600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0c5b7b-18b8-4574-a48b-0bc63718c9f7",
   "metadata": {},
   "source": [
    "- 입력 문장의 단어 토큰 각각은 RNN 셀의 각 시점의 입력\n",
    "- 인코더 RNN 셀의 마지막 시점 은닉 상태(컨텍스트 벡터)를 디코더 RNN 셀로 전송\n",
    "- 컨텍스트 벡터는 디코더 RNN 셀의 첫번째 은닉 상태에 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acc4eb1-77bb-4f63-9935-502c62151421",
   "metadata": {},
   "source": [
    "<img src='img/nlp_04_3.png' width='400'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "098d7803-05a4-4fc9-8725-49620839e609",
   "metadata": {},
   "source": [
    "- 현재 시점 t에서의 은닉 상태는 과거 시점의 동일한 RNN 셀에서의 모든 은닉 상태의 값들의 영향을 누적해서 받아온 값\r",
    "- \n",
    "컨텍스트 벡터는 입력 문장의 모든 단어 토큰들의 정보를 요약해서 담고 있음\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a798e262-94b1-4f05-b184-65f8ae15bfdf",
   "metadata": {},
   "source": [
    "<img src='img/nlp_04_4.png' width='200'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c60127a-a063-4352-b0b4-b6d0edf4eee0",
   "metadata": {},
   "source": [
    "디코더에서 각 시점의 RNN 셀에서 출력 벡터가 나오면, 해당 벡터는 소프트맥스 함수를 통해 출력 시퀀스의 각 단어별 확률값을 반환, 디코더는 출력 단어를 결정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f908430e-46c9-4745-920a-2777ed1bbfd4",
   "metadata": {},
   "source": [
    "컨텍스트 벡터는 디코더의 초기 은닉 상태로만 사용할 수도 있고, 컨텍스트 벡터를 디코더가 단어를 예측하는 매 시점마다 하나의 입력으로 사용할 수도 있으며, \n",
    "\n",
    "더 나아가 어텐션 메커니즘이라는 방법을 통해 지금 알고있는 컨텍스트 벡터보다 더욱 문맥을 반영할 수 있는 컨텍스트 벡터를 구하여 매 시점마다 하나의 입력으로 사용할 수도 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc23102-72c7-4097-aa12-20014378a3fb",
   "metadata": {},
   "source": [
    "# 어텐션 메커니즘\n",
    "\n",
    "Attention Mechanism"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13d12d3b-9a03-41d5-990f-efabecc3f2fd",
   "metadata": {},
   "source": [
    "**RNN에 기반한 seq2seq 모델의 문제**\n",
    "1. 하나의 고정된 크기의 벡터에 모든 정보를 압축 => 정보 손실이 발생\n",
    "2. RNN의 고질적인 문제인 기울기 소실(vanishing gradient) 문제\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc043b7-ad2f-4cab-992f-45d20ba3cf45",
   "metadata": {},
   "source": [
    "**어텐션의 아이디어**\n",
    "- 디코더에서 출력 단어를 예측하는 매 시점마다, 인코더에서의 전체 입력 문장을 다시 한 번 참고 \n",
    "- 단, 전체 입력 문장을 전부 다 동일한 비율로 참고하는 것이 아니라, 해당 시점에서 예측해야 할 단어와 연관이 있는 입력 단어 부분을 좀 더 집중(attention)해서 봄"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c9d34b8-a6bb-46ac-8961-98653aa7c413",
   "metadata": {},
   "source": [
    "## 어텐션 함수(attention function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecb3631-3491-46b6-b0ca-04661e1f6553",
   "metadata": {},
   "source": [
    "<img src='img/nlp_04_5.png' width='300'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221e8191-c7a8-4ac8-8579-88af3be23018",
   "metadata": {},
   "source": [
    "**Attention(Q, K, V) = Attention Value**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef49345-540d-48c1-8d49-aacf0dda17e9",
   "metadata": {},
   "source": [
    "주어진 '쿼리(Query)'에 대해서 모든 '키(Key)'와의 유사도를 각각 구함 \n",
    "- 이 유사도를 키와 맵핑되어있는 각각의 '값(Value)'에 반영\n",
    "- 유사도가 반영된 '값(Value)'을 모두 더해서 리턴\n",
    "- 어텐션 값(Attention Value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34da1a70-f6f0-4625-b853-ba25ab2e2d14",
   "metadata": {},
   "source": [
    "## 닷-프로덕트 어텐션(Dot-Product Attention)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27af0575-d1c1-49d7-8f16-42b727d07d4f",
   "metadata": {},
   "source": [
    "<img src='img/nlp_04_6.png' width='400'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6ba9f5-1b8a-4d94-a203-fe3203feee2c",
   "metadata": {},
   "source": [
    "예시)\n",
    "- 디코더의 세번째 LSTM 셀에서 출력 단어를  예측. \n",
    "- 디코더의 첫번째, 두번째 LSTM 셀은 이미 어텐션 메커니즘을 통해 \"je\"와 \"suis\"를 예측하는 과정을 거쳤다고 가정\n",
    "- 세번째 LSTM 셀은 출력 단어를 예측하기 위해서 인코더의 모든 입력 단어들의 정보를 다시 한번 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d2454b-545c-4335-ad01-f1c1c3e8d5cd",
   "metadata": {},
   "source": [
    "소프트맥스 함수를 통해 나온 결과값은 I, am, a, student 단어 각각이 출력 단어를 예측할 때 얼마나 도움이 되는 지의 정도를 수치화한 값\n",
    "\n",
    "- ==>  이를 하나의 정보로 담아서 디코더로 전송 (녹색 삼각형)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565ec612-9d6a-4dea-9488-22459fa42279",
   "metadata": {},
   "source": [
    "<img src='img/nlp_04_7.png' width='400'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83266433-171b-43cd-a628-5c9fd629b94c",
   "metadata": {},
   "source": [
    "### 어텐션 스코어(attention score)\n",
    "\n",
    "현재 디코더의 시점 $t$에서 단어를 예측하기 위해, 인코더의 모든 은닉 상태 각각이 디코더의 현 시점의 은닉 상태 $s_t$와 얼마나 유사한지를 판단하는 스코어값"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a67d1f7-cc94-468c-a22a-9e5dd3d59145",
   "metadata": {},
   "source": [
    "<img src='img/nlp_04_8.png' width='300'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b469dbf-bb10-443b-8122-3100363eed9b",
   "metadata": {},
   "source": [
    "- 닷-프로덕트 어텐션에서는 스코어 값을 구하기 위해 $s_t$를 전치하고 각 은닉 상태와 내적(dot product)을 수행 \n",
    "- 즉, 모든 어텐션 스코어 값은 스칼라가 됨\n",
    "\n",
    "인코더의 모든 은닉 상태의 어텐션 스코어의 모음값을 $e^t$라고 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502c6993-3db9-43fd-ae9c-ec8a83d2eca4",
   "metadata": {},
   "source": [
    "### 어텐션 분포(attention distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743c8be9-6143-4b36-8cca-1e50eff9d5ce",
   "metadata": {},
   "source": [
    "<img src='img/nlp_04_9.png' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca27467-d047-45ef-ac0b-a080560a1d29",
   "metadata": {},
   "source": [
    "- $e^t$에 소프트맥스 함수를 적용하여, 모든 값을 합하면 1이 되는 확률 분포를 얻음\n",
    "- 각각의 값는 어텐션 가중치임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09f9949-8fca-4ee4-918b-d9e609495ac0",
   "metadata": {},
   "source": [
    "$\\alpha^t = softmax(e^t)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10a8433f-5950-4f09-8ccd-4455dc539ec9",
   "metadata": {},
   "source": [
    "### 어텐션 값(attention value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad328fb-99c6-49e2-bc3a-f360ed427529",
   "metadata": {},
   "source": [
    "<img src='img/nlp_04_10.png' width='450'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3db20c-33b8-4379-854a-eaec05aecef6",
   "metadata": {},
   "source": [
    "어텐션의 최종 결과값을 얻기 위해서 각 인코더의 은닉 상태와 어텐션 가중치값들을 곱하고, 최종적으로 모두 더함\n",
    "- 즉, 가중합(Weighted Sum)을 진행\n",
    "\n",
    "이러한 어텐션 값 $a_t$는 종종 인코더의 문맥을 포함하고 있다고 하여, 컨텍스트 벡터(context vector)라고도 불림"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b740a89-312e-4783-894e-03f45001c23d",
   "metadata": {},
   "source": [
    "$a_t = \\sum^{N}_{i=1} \\alpha_i^{t}h_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf50c86-bda5-447e-830e-5348dd02f61a",
   "metadata": {},
   "source": [
    "### 어텐션 값과 디코더 은닉상태 연결"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5cf7fa-bffd-42d9-832b-139211330656",
   "metadata": {},
   "source": [
    "<img src='img/nlp_04_11.png' width='450'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d350680b-4f87-49a1-8eec-2da4f20f7e34",
   "metadata": {},
   "source": [
    "어텐션 값 $a_t$를 $s_t$와 결합(concatenate)하여 하나의 벡터로 만듦 => $v_t$\n",
    "\n",
    "- 이 $v_t$를 예측 연산의 입력으로 사용하므로서 인코더로부터 얻은 정보를 활용하여 더 잘 예측할 수 있게 됨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40777fda-20c3-4b59-a90d-06a6ee332720",
   "metadata": {},
   "source": [
    "<img src='img/nlp_04_12.png' width='200'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c25926f-5645-4ef8-a7bc-e266e3bc3d47",
   "metadata": {},
   "source": [
    "$\\tilde{s}_t=\\tanh(\\mathbf{W}_c[a_t;s_t]+b_c)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797bbf4c-f72e-4b69-b614-164f1f2cb27d",
   "metadata": {},
   "source": [
    "- 논문에서는 $v_t$를 바로 출력층으로 보내기 전에 신경망 연산을 한 번 더 추가 (가중치 행렬과 곱하고, 편향 추가, 하이퍼볼릭탄젠트 함수)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bf7792-b812-4f8f-addd-eebfd70f19d8",
   "metadata": {},
   "source": [
    "$\\hat{y}_t = Softmax(\\mathbf{W}_y \\tilde{s}_t + b_y)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69bdee34-e77a-402e-909f-0ee60cc1eff8",
   "metadata": {},
   "source": [
    "- $\\tilde{s}_t$를 출력층의 입력으로 사용하여 예측 벡터 ($\\hat{𝑦}_𝑡$)를 얻음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ef7c18-0af4-4947-863a-f3d3105fdebc",
   "metadata": {},
   "source": [
    "<img src='img/nlp_04_13.png' width='600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81550e76-ffb2-4528-b635-be1f3b4b8b3a",
   "metadata": {},
   "source": [
    "## 바다나우 어텐션(Bahdanau Attention)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32bf01e2-133e-4b9b-99fa-43db8b6014c3",
   "metadata": {},
   "source": [
    "어텐션 스코어\n",
    "\n",
    "- 바다나우 어텐션에서는 $t-1$ 시점의 은닉 상태 $𝑠_{𝑡−1}$를 사용\r",
    "- $score(𝑠_{𝑡−1},h_i) = W^T_a tanh(W_bs_{t-1}+W_ch_i)$, $score(𝑠_{𝑡−1},H) = W^T_a tanh(W_bs_{t-1}+W_cH)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f421e313-6c32-41f6-b71e-b6fde256a0b9",
   "metadata": {},
   "source": [
    "어텐션 분포\n",
    "- 소프트맥스 사용\n",
    "\n",
    "어텐션 값\n",
    "- 가중합(Weighted Sum) 진행 a.k.a. 컨텍스트 벡터\n",
    "\n",
    "어텐션 값(attention value)\n",
    "- 컨텍스트 벡터($a_t$)와 현재 시점의 입력인 단어의 임베딩 벡터를 연결하고, 현재 시점의 새로운 입력으로 사용\n",
    "- 이전 시점의 셀로부터 전달받은 은닉 상태 $𝑠_{𝑡−1}$와 현재 시점의 새로운 입력으로부터 $𝑠_𝑡$를 구함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de890ce-a023-40e0-aed4-f9cea0255424",
   "metadata": {},
   "source": [
    "## 실습 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b83b025-d137-4fc0-9c8c-d2af4dfbfbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vector shape: (64, 256)\n",
      "Attention weights shape: (64, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "import numpy as np\n",
    "\n",
    "class BahdanauAttention(Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query: Decoder hidden state (batch_size, hidden size)\n",
    "        # values: Encoder output (batch_size, seq_len, hidden size)\n",
    "        \n",
    "        # Expand query to (batch_size, seq_len, hidden size)\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        \n",
    "        # Score shape == (batch_size, seq_len, 1)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "        \n",
    "        # Attention weights shape == (batch_size, seq_len, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # Context vector shape after sum == (batch_size, hidden size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        return context_vector, attention_weights\n",
    "\n",
    "# 예시 입력 데이터\n",
    "encoder_output = np.random.rand(64, 10, 256).astype(np.float32)  # (batch_size, seq_len, hidden_size)\n",
    "decoder_hidden = np.random.rand(64, 256).astype(np.float32)      # (batch_size, hidden_size)\n",
    "\n",
    "# Bahdanau 어텐션 레이어 생성\n",
    "attention_layer = BahdanauAttention(units=10)\n",
    "context_vector, attention_weights = attention_layer(decoder_hidden, encoder_output)\n",
    "\n",
    "print(\"Context vector shape:\", context_vector.shape)  # (batch_size, hidden_size)\n",
    "print(\"Attention weights shape:\", attention_weights.shape)  # (batch_size, seq_len, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2437b800-a543-409e-be53-2d4ed402db09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vector shape: torch.Size([64, 256])\n",
      "Attention weights shape: torch.Size([64, 10, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, attn_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = nn.Linear(hidden_size, attn_size)\n",
    "        self.W2 = nn.Linear(hidden_size, attn_size)\n",
    "        self.V = nn.Linear(attn_size, 1)\n",
    "\n",
    "    def forward(self, query, values):\n",
    "        # query: (batch_size, hidden_size)\n",
    "        # values: (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        query_with_time_axis = query.unsqueeze(1)  # (batch_size, 1, hidden_size)\n",
    "        score = self.V(torch.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))  # (batch_size, seq_len, 1)\n",
    "        \n",
    "        attention_weights = F.softmax(score, dim=1)  # (batch_size, seq_len, 1)\n",
    "        \n",
    "        context_vector = attention_weights * values  # (batch_size, seq_len, hidden_size)\n",
    "        context_vector = torch.sum(context_vector, dim=1)  # (batch_size, hidden_size)\n",
    "        \n",
    "        return context_vector, attention_weights\n",
    "\n",
    "# 예시 입력 데이터\n",
    "batch_size = 64\n",
    "seq_len = 10\n",
    "hidden_size = 256\n",
    "attn_size = 10\n",
    "\n",
    "encoder_output = torch.randn(batch_size, seq_len, hidden_size)  # (batch_size, seq_len, hidden_size)\n",
    "decoder_hidden = torch.randn(batch_size, hidden_size)  # (batch_size, hidden_size)\n",
    "\n",
    "# Bahdanau 어텐션 레이어 생성\n",
    "attention_layer = BahdanauAttention(hidden_size, attn_size)\n",
    "context_vector, attention_weights = attention_layer(decoder_hidden, encoder_output)\n",
    "\n",
    "print(\"Context vector shape:\", context_vector.shape)  # (batch_size, hidden_size)\n",
    "print(\"Attention weights shape:\", attention_weights.shape)  # (batch_size, seq_len, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22d18eb-040d-4536-ab66-37bab1d66658",
   "metadata": {},
   "source": [
    "# 참고자료\n",
    "\n",
    "- 딥 러닝을 이용한 자연어 처리 입문(https://wikidocs.net/book/2155)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
