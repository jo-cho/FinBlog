{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f28461fd",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"그래프 머신러닝: (4-1) 다중 관계 데이터와 지식 그래프\"\n",
    "subtitle: \"(GRL) Multi-Relational Data and Knowledge Graphs\"\n",
    "author: \"Cheonghyo Cho\"\n",
    "categories: [graph, network, multi-relational, knowledge graph]\n",
    "image: \"grl_04_1.PNG\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c9204a",
   "metadata": {},
   "source": [
    "Hamilton,W.L. *Graph Representation Learning*. 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ab4d04",
   "metadata": {},
   "source": [
    "앞에서는 저차원 임베딩에 대해 다루었으며, 특히 shallow embedding 방법을 위주로 다루었다. 이 포스트에서는 shallow embedding 방법으로 **다중관계**(**multi-relational**)의 그래프에서의 기술에 대해 알아본다.\n",
    "\n",
    "- 이 포스트에서 다룰 대부분의 방법들은 원래 지식 그래프(knowledge graph)를 위해 디자인되었다.\n",
    "- 지식 그래프란, $\\mathcal{G}=(\\mathcal{V},\\mathcal{E},\\mathcal{R})$의 엣지는 튜플 $(u, \\tau,v) \\in \\mathcal{V} \\times \\mathcal{R} \\times \\mathcal{V}$로 정의되며 노드 $u$와 노드 $v$가 관계 $\\tau$로 연결되어 있다는 것을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b321c487",
   "metadata": {},
   "source": [
    "# 다중관계 데이터의 재구성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79e031a",
   "metadata": {},
   "source": [
    "이제 재구성(reconstruction) 작업을 다중관계에서 다루게 된다. 즉 여러 종류의 엣지를 다루어야 한다.\n",
    "\n",
    "- 우선, 디코더의 인풋으로 노드 임베딩 쌍만을 넣지 않고, 관계의 종류까지 넣는다. $\\text{DEC}: \\mathbb{R}^d \\times \\mathcal{R} \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}^+$\n",
    "- 디코더의 아웃풋은 엣지 $(u,\\tau, v)$가 그래프에 존재할 가능도(likelihood)가 된다.\n",
    "\n",
    "- 구체적인 예시로, 가장 간단하고 초창기의 다중관계 방법(RESCAL; Nickel et al.,2011)을 살펴보자:\n",
    "    - $$\\text{DEC}(u,\\tau,v) = \\mathbf{z}_u^{\\top}\\mathbf{R}_{\\tau}\\mathbf{z}_v$$\n",
    "        - $\\mathbf{R} \\in \\mathbb{R}^{d\\times d}$는 $\\tau \\in \\mathcal{R}$에 대한 learnable 행렬임.\n",
    "    - 이 디코더를 간단히 하기 위해서 임베딩 행렬 $Z$와 관계 행렬 $R_{\\tau}$를 훈련시키며, 아래와 같은 기본 재구성 손실을 사용한다:\n",
    "    - $$\\mathcal{L} = \\sum_{u \\in \\mathcal{V}}\\sum_{v \\in \\mathcal{V}}\\sum_{\\tau \\in \\mathcal{R}} {\\|\\text{DEC}(u,\\tau,v)-\\mathcal{A}[u,\\tau,v] \\|}^2\n",
    "    \\\\ = \\sum_{u \\in \\mathcal{V}}\\sum_{v \\in \\mathcal{V}}\\sum_{\\tau \\in \\mathcal{R}} {\\|\\mathbf{z}_u^{\\top}\\mathbf{R}_{\\tau}\\mathbf{z}_v-\\mathcal{A}[u,\\tau,v] \\|}^2$$\n",
    "        - $\\mathcal{A} \\in \\mathbb{R}^{|\\mathcal{V}|\\times|\\mathcal{R}|\\times|\\mathcal{V}|}$은 다중관계 그래프에 대한 인접 텐서(adjacency tensor)임."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d210105d-e824-4124-832c-6aa056e4ba88",
   "metadata": {},
   "source": [
    "앞서, 서로다른 디코더와 유사도 측정, 손실 함수에 따라 노드 임베딩 방법이 다양한 것을 확인할 수 있었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f86bc83",
   "metadata": {},
   "source": [
    "# 참고자료\n",
    "\n",
    "[1] Hamilton, W. L. (2020). *Graph Representation Learning.* Morgan & Claypool Publishers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba4234d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
